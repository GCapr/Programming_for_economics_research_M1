<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 2c: Web Scraping | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content { -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; }
    .output-simulation { background: #1e1e1e; border-radius: 8px; margin: 1rem 0; overflow: hidden; font-family: 'Fira Code', monospace; font-size: 0.8rem; display: none; }
    .output-simulation.visible { display: block; }
    .output-header { background: #333; padding: 0.5rem 1rem; display: flex; justify-content: space-between; align-items: center; color: #ccc; font-size: 0.75rem; }
    .output-body { padding: 1rem; color: #d4d4d4; white-space: pre-wrap; overflow-x: auto; max-height: 400px; overflow-y: auto; }
    .output-body .out-green { color: #4ec9b0; }
    .output-body .out-yellow { color: #dcdcaa; }
    .output-body .out-blue { color: #569cd6; }
    .output-body .out-num { color: #b5cea8; }
    .output-body .out-str { color: #ce9178; }
    .output-body .out-red { color: #f14c4c; }
    .research-banner { background: #4a5568; color: white; padding: 1.5rem; border-radius: 12px; margin: 2rem 0; border-left: 5px solid #ed8936; }
    .research-banner h3 { color: white; margin-top: 0; }
    .concept-box { background: #f0f9ff; border-left: 4px solid #2563eb; padding: 1.5rem; margin: 1.5rem 0; border-radius: 0 8px 8px 0; }
    .concept-box h4 { color: #1e40af; margin-top: 0; }
    .legal-box { background: #fef3c7; border-left: 4px solid #d97706; padding: 1.5rem; margin: 1.5rem 0; border-radius: 0 8px 8px 0; }
    .legal-box h4 { color: #92400e; margin-top: 0; }
    .danger-box { background: #fee2e2; border-left: 4px solid #dc2626; padding: 1.5rem; margin: 1.5rem 0; border-radius: 0 8px 8px 0; }
    .danger-box h4 { color: #991b1b; margin-top: 0; }
    .run-btn { background: #22c55e; color: white; border: none; padding: 0.5rem 1rem; border-radius: 6px; cursor: pointer; font-size: 0.85rem; font-weight: 600; margin-top: 0.5rem; display: inline-flex; align-items: center; gap: 0.5rem; }
    .run-btn:hover { background: #16a34a; }
    .run-btn::before { content: '‚ñ∂'; font-size: 0.7rem; }
    .close-output { background: transparent; border: none; color: #9ca3af; cursor: pointer; font-size: 1.2rem; }
    .close-output:hover { color: white; }
    /* Code tooltips - hover explanations (JavaScript-powered for boundary detection) */
    .code-tooltip {
      position: relative;
      cursor: help;
      border-bottom: 1px dotted #888;
      text-decoration: none;
    }

    /* Tooltip element created by JavaScript */
    .tooltip-popup {
      position: fixed;
      background: #1f2937;
      color: white;
      padding: 0.5rem 0.75rem;
      border-radius: 6px;
      font-size: 0.75rem;
      white-space: normal;
      max-width: 300px;
      opacity: 0;
      pointer-events: none;
      transition: opacity 0.15s ease-in-out;
      z-index: 10000;
      line-height: 1.4;
      text-align: left;
      font-family: var(--font-body);
      font-style: normal;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    .tooltip-popup.visible {
      opacity: 1;
    }

    /* Small arrow pointer */
    .tooltip-popup::after {
      content: '';
      position: absolute;
      border: 6px solid transparent;
    }
    .tooltip-popup.arrow-bottom::after {
      top: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-top-color: #1f2937;
    }
    .tooltip-popup.arrow-top::after {
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-bottom-color: #1f2937;
    }
    .tooltip-popup.arrow-left::after {
      top: 50%;
      right: 100%;
      transform: translateY(-50%);
      border-right-color: #1f2937;
    }
    .tooltip-popup.arrow-right::after {
      top: 50%;
      left: 100%;
      transform: translateY(-50%);
      border-left-color: #1f2937;
    }
    .html-diagram { background: #1e1e1e; border-radius: 8px; padding: 1.5rem; margin: 1.5rem 0; font-family: 'Fira Code', monospace; font-size: 0.85rem; color: #d4d4d4; overflow-x: auto; }
    .html-diagram .tag { color: #569cd6; }
    .html-diagram .attr { color: #9cdcfe; }
    .html-diagram .value { color: #ce9178; }
    .html-diagram .content { color: #d4d4d4; }
    .html-diagram .comment { color: #6a9955; }
    .checklist { background: #f0fdf4; border: 1px solid #86efac; border-radius: 8px; padding: 1.5rem; margin: 1.5rem 0; }
    .checklist h4 { color: #166534; margin-top: 0; }
    .checklist ul { margin-bottom: 0; }
    .checklist li { margin-bottom: 0.5rem; }

    /* ========================================
       CONSISTENT TYPOGRAPHY HIERARCHY
       (Matches site-wide conventions)
       ======================================== */

    /* Section numbers in headings */
    .section-num {
      color: var(--color-accent);
      font-weight: 700;
    }

    /* Section divider between major sections */
    .section-divider {
      height: 2px;
      background: linear-gradient(90deg, transparent 0%, var(--color-border) 15%, var(--color-border) 85%, transparent 100%);
      margin: 3rem 0 2rem 0;
    }

    /* ========================================
       COLLAPSIBLE/ACCORDION SECTIONS
       ======================================== */

    .accordion-section {
      border: 1px solid var(--color-border);
      border-radius: 8px;
      margin: 1rem 0;
      overflow: hidden;
      background: white;
    }

    .accordion-header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 1rem 1.25rem;
      background: var(--color-bg-alt);
      cursor: pointer;
      transition: background 0.2s ease;
      border: none;
      width: 100%;
      text-align: left;
      font-family: var(--font-heading);
      font-size: 1.1rem;
      font-weight: 600;
      color: var(--color-primary);
    }

    .accordion-header:hover {
      background: #e2e8f0;
    }

    .accordion-header::after {
      content: '+';
      font-size: 1.4rem;
      font-weight: 400;
      color: var(--color-accent);
      transition: transform 0.3s ease;
    }

    .accordion-section.open .accordion-header::after {
      content: '‚àí';
    }

    .accordion-content {
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.3s ease-out;
      padding: 0 1.25rem;
    }

    .accordion-section.open .accordion-content {
      max-height: 2000px;
      padding: 1rem 1.25rem 1.25rem;
    }

    .accordion-content p:last-child,
    .accordion-content ul:last-child {
      margin-bottom: 0;
    }

    /* ========================================
       TABLE OF CONTENTS (Simpler, site-consistent)
       ======================================== */

    .toc {
      background: var(--color-bg-alt);
      border: 1px solid var(--color-border);
      border-radius: 8px;
      padding: 1.25rem 1.5rem;
      margin: 1.5rem 0 2rem 0;
    }

    .toc h3 {
      font-size: 1rem;
      color: var(--color-primary);
      margin: 0 0 0.75rem 0;
      padding: 0;
    }

    .toc ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }

    .toc li {
      margin: 0.4rem 0;
    }

    .toc a {
      color: var(--color-text);
      font-size: 0.95rem;
      padding: 0.25rem 0;
      display: inline-block;
    }

    .toc a:hover {
      color: var(--color-accent);
    }

    .toc .toc-num {
      color: var(--color-accent);
      font-weight: 600;
      margin-right: 0.25rem;
    }
  </style>
</head>
<body>
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <p style="font-size: 0.9rem; color: #666; margin-bottom: 1.5rem;">Please enter the course password to access the materials.</p>
      <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
      <button id="password-submit">Access Course</button>
      <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav active">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li class="active"><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li><a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a></li>
          <li><a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a></li>
          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <div class="module-header">
          <h1>2c &nbsp;Web Scraping</h1>
          <div class="module-meta">
            <span>~5 hours</span>
            <span>HTML, BeautifulSoup, Legal Considerations</span>
            <span>Intermediate</span>
          </div>
        </div>

        <div class="learning-objectives">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Understand when web scraping is appropriate vs. using APIs</li>
            <li>Navigate the legal and ethical landscape of web scraping</li>
            <li>Parse HTML structure and extract data using BeautifulSoup/rvest</li>
            <li>Handle pagination, rate limiting, and dynamic content</li>
            <li>Build robust, respectful scrapers for research data collection</li>
          </ul>
        </div>

        <!-- Critical Legal Warning -->
        <div class="danger-box">
          <h4>Before You Scrape: Legal and Ethical Obligations</h4>
          <p><strong>Web scraping exists in a legal gray area.</strong> What's technically possible isn't always legal or ethical. Before scraping any website, you must understand the legal framework and respect website owners' rights.</p>
          <p style="margin-bottom: 0;"><strong>This module teaches responsible scraping for legitimate research purposes only.</strong></p>
        </div>

        <!-- Research Project Scenario -->
        <div class="research-banner">
          <h3>üìä Course Research Project: Climate Vulnerability & Economic Growth</h3>
          <p style="margin-bottom: 1rem;">Throughout this course, we're building a research project on <strong>climate vulnerability and economic growth</strong>. In <a href="02a-file-import.html" style="color: #fbd38d;">Module 2a</a>, we loaded GDP data from files. In <a href="02b-apis.html" style="color: #fbd38d;">Module 2b</a>, we fetched CO2 emissions via the World Bank API. Now we'll <strong>scrape additional climate data</strong> from Wikipedia.</p>
          <p style="margin-bottom: 1rem;"><strong>Our target:</strong> <a href="https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions" style="color: #fbd38d;" target="_blank">Wikipedia's "List of countries by carbon dioxide emissions"</a> page. This table contains historical emissions data that we can merge with our World Bank data for a richer analysis.</p>
          <p style="margin-bottom: 1rem;"><strong>Why Wikipedia?</strong> It's public, legal to scrape (under their <a href="https://en.wikipedia.org/wiki/Wikipedia:Copyrights" style="color: #fbd38d;" target="_blank">CC license</a>), has well-structured HTML tables, and lets you verify your results by looking at the actual page.</p>
          <p style="margin-bottom: 0; background: rgba(255,255,255,0.1); padding: 0.75rem 1rem; border-radius: 6px;"><strong>Our goal:</strong> Extract the CO2 emissions table into a pandas DataFrame / R tibble, clean the data, and merge it with our World Bank dataset. This demonstrates a realistic research workflow.</p>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#when-to-scrape"><span class="toc-num">2c.1</span> When to Scrape (and When Not To)</a></li>
            <li><a href="#legal-framework"><span class="toc-num">2c.2</span> Legal Framework</a></li>
            <li><a href="#html-basics"><span class="toc-num">2c.3</span> Understanding HTML Structure</a></li>
            <li><a href="#basic-scraping"><span class="toc-num">2c.4</span> Basic Scraping Techniques</a></li>
            <li><a href="#advanced-techniques"><span class="toc-num">2c.5</span> Advanced Techniques</a></li>
            <li><a href="#best-practices"><span class="toc-num">2c.6</span> Best Practices for Research</a></li>
          </ul>
        </div>

        <!-- Section 1: When to Scrape -->
        <h2 id="when-to-scrape"><span class="section-num">2c.1</span> When to Scrape (and When Not To)</h2>

        <p>Before writing a single line of scraping code, you need to ask: <strong>is scraping actually the right approach?</strong> Scraping should be your last resort, not your first instinct. Let's see why.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p>For our CO2 emissions data, let's check alternatives first:</p>
          <ol style="margin-bottom: 0;">
            <li><strong>Check for existing datasets:</strong> We already used the World Bank API for CO2 data in <a href="02b-apis.html">Module 2b</a>. Wikipedia offers additional historical data and different aggregations we can complement our dataset with.</li>
            <li><strong>Check for an API:</strong> Wikipedia has a <a href="https://www.mediawiki.org/wiki/API:Main_page" target="_blank">MediaWiki API</a> that can retrieve page content. However, parsing HTML tables is often easier than parsing wikitext.</li>
            <li><strong>Why we scrape anyway:</strong> This is a <em>learning exercise</em>. Wikipedia's tables are well-structured, legal to scrape, and let you practice techniques you'll need for sites that <em>don't</em> have APIs.</li>
          </ol>
        </div>

        <div class="concept-box">
          <h4>The Hierarchy of Data Acquisition</h4>
          <p style="margin-bottom: 0.5rem;">Always prefer these options <strong>in order</strong>:</p>
          <ol style="margin-bottom: 0;">
            <li><strong>Official datasets</strong> &mdash; Published data files from the source</li>
            <li><strong>APIs</strong> &mdash; Structured, sanctioned data access (see <a href="02b-apis.html">Module 2b</a>)</li>
            <li><strong>Data request</strong> &mdash; Contact the organization directly</li>
            <li><strong>Web scraping</strong> &mdash; Last resort when above options fail</li>
          </ol>
        </div>

        <div class="info-box tip">
          <div class="info-box-title">Why Scraping Should Be a Last Resort</div>
          <p style="margin-bottom: 0.5rem;">Scraping has real costs:</p>
          <ul style="margin-bottom: 0;">
            <li><strong>It's fragile:</strong> If a website changes its layout, your scraper breaks</li>
            <li><strong>It's slow:</strong> Polite scraping requires delays between requests</li>
            <li><strong>It's legally murky:</strong> APIs and official datasets have clear usage terms</li>
            <li><strong>It requires maintenance:</strong> You'll spend time fixing broken scrapers</li>
          </ul>
        </div>

        <!-- Section 2: Legal Framework -->
        <div class="section-divider"></div>
        <h2 id="legal-framework"><span class="section-num">2c.2</span> Legal Framework</h2>

        <p>You've decided that scraping is necessary for your project. Before writing any code, you need to understand <strong>what you're legally allowed to do</strong>. This section might seem like a detour from the technical content, but skipping it could put your research‚Äîand potentially your institution‚Äîat risk.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">For our Wikipedia CO2 emissions scraper, we're in an excellent legal position: Wikipedia content is published under a <strong>Creative Commons Attribution-ShareAlike license</strong>, which explicitly permits reuse. Their <a href="https://en.wikipedia.org/robots.txt" target="_blank">robots.txt</a> allows scraping of article pages. We just need to be polite (rate limit our requests) and provide attribution if we publish the data.</p>
        </div>

        <div class="legal-box">
          <h4>I Am Not a Lawyer</h4>
          <p style="margin-bottom: 0;">This section provides educational information about legal concepts related to web scraping. <strong>It is not legal advice.</strong> Consult with your institution's legal counsel or a qualified attorney for guidance on specific situations.</p>
        </div>

        <h3>Key Legal Considerations</h3>

        <p>The legal landscape for web scraping involves several overlapping frameworks. <strong>Click each topic below</strong> to expand the details.</p>

        <!-- Accordion 1: Terms of Service -->
        <div class="accordion-section">
          <button class="accordion-header">1. Terms of Service (ToS)</button>
          <div class="accordion-content">
            <p>Most websites have Terms of Service that may prohibit automated data collection. Violating ToS can result in:</p>
            <ul>
              <li>Being blocked from the website</li>
              <li>Civil lawsuits for breach of contract</li>
              <li>In some jurisdictions, criminal charges under computer fraud laws</li>
            </ul>
            <div class="info-box warning" style="margin-top: 1rem;">
              <div class="info-box-title">Always Check the Terms of Service</div>
              <p style="margin-bottom: 0;">Before scraping, find and read the website's ToS. Look for keywords like "automated", "scraping", "crawling", "bot", "data collection".</p>
            </div>
          </div>
        </div>

        <!-- Accordion 2: robots.txt -->
        <div class="accordion-section">
          <button class="accordion-header">2. robots.txt</button>
          <div class="accordion-content">
            <p>The <code>robots.txt</code> file tells automated systems which parts of a site they can access. It's located at the root of every website (e.g., <code>example.com/robots.txt</code>).</p>
            <div class="html-diagram">
<span class="comment"># Example robots.txt file</span>

User-agent: *          <span class="comment"># Rules for all bots</span>
Disallow: /private/    <span class="comment"># Don't access /private/</span>
Disallow: /admin/      <span class="comment"># Don't access /admin/</span>
Allow: /public/        <span class="comment"># Explicitly allowed</span>

Crawl-delay: 10        <span class="comment"># Wait 10 seconds between requests</span>
            </div>
            <div class="info-box note" style="margin-top: 1rem;">
              <div class="info-box-title">robots.txt is Not Legally Binding</div>
              <p style="margin-bottom: 0;">Technically, robots.txt is a voluntary standard. However, ignoring it demonstrates bad faith and may be used against you in legal proceedings. <strong>Always respect robots.txt.</strong></p>
            </div>
          </div>
        </div>

        <!-- Accordion 3: Copyright -->
        <div class="accordion-section">
          <button class="accordion-header">3. Copyright and Database Rights</button>
          <div class="accordion-content">
            <p>Even if you can legally <em>access</em> data, you may not have the right to <em>use</em> or <em>republish</em> it:</p>
            <ul>
              <li><strong>Copyright</strong> protects creative works (articles, images, unique descriptions)</li>
              <li><strong>Database rights</strong> (EU) protect substantial investments in compiling data</li>
              <li><strong>Facts themselves</strong> are generally not copyrightable, but their presentation may be</li>
            </ul>
          </div>
        </div>

        <!-- Accordion 4: CFAA -->
        <div class="accordion-section">
          <button class="accordion-header">4. US: Computer Fraud and Abuse Act (CFAA)</button>
          <div class="accordion-content">
            <p>The CFAA prohibits "unauthorized access" to computer systems. Recent court decisions have <strong>clarified researchers' rights</strong>:</p>
            <ul>
              <li><strong>hiQ Labs v. LinkedIn (2022)</strong>: Scraping <em>publicly accessible</em> data is not "unauthorized access." Landmark decision for researchers.</li>
              <li><strong>Van Buren v. United States (Supreme Court, 2021)</strong>: Violating Terms of Service alone does not trigger criminal CFAA liability.</li>
              <li><strong>Sandvig v. Barr (2020)</strong>: CFAA does not criminalize mere ToS violations for research purposes.</li>
            </ul>
          </div>
        </div>

        <!-- Accordion 5: EU Regulation -->
        <div class="accordion-section">
          <button class="accordion-header">5. EU: Text and Data Mining Exceptions</button>
          <div class="accordion-content">
            <p>The EU <strong>Digital Single Market Directive 2019/790</strong> created explicit protections for research scraping:</p>
            <ul>
              <li><strong>Article 3 (Research Exception)</strong>: Research organizations may perform text and data mining <em>regardless of contractual terms</em>. This overrides ToS restrictions for legitimate research.</li>
              <li><strong>Article 4 (General Exception)</strong>: Any lawful access holder may perform TDM unless the rightsholder has <em>explicitly</em> reserved this right in machine-readable format.</li>
            </ul>
            <p><strong>GDPR considerations:</strong> Article 89 allows processing personal data for research with appropriate safeguards (data minimization, pseudonymization).</p>
          </div>
        </div>

        <!-- Accordion 6: Recent Developments -->
        <div class="accordion-section">
          <button class="accordion-header">6. Recent Legal Developments (2023-2025)</button>
          <div class="accordion-content">
            <ul>
              <li><strong>AI Training Data Debates</strong>: Academic research maintains stronger fair use protections than commercial AI training</li>
              <li><strong>US Copyright Office (2023)</strong>: Factual data extraction generally does not constitute copyright infringement</li>
              <li><strong>EU AI Act (2024)</strong>: Research scraping for AI development has explicit carve-outs</li>
              <li><strong>Meta v. Bright Data (2024)</strong>: Reinforced that scraping public data does not violate the CFAA</li>
            </ul>
          </div>
        </div>

        <div class="info-box warning" style="margin-top: 1.5rem;">
          <div class="info-box-title">Jurisdiction Matters</div>
          <p style="margin-bottom: 0;">Legal protections vary by country. EU researchers generally have stronger statutory protections. Consult with legal counsel about which framework applies to your specific project.</p>
        </div>

        <h3>Research-Specific Considerations</h3>

        <div class="checklist">
          <h4>Pre-Scraping Checklist for Researchers</h4>
          <ul>
            <li><strong>Check for existing datasets</strong> &mdash; ICPSR, Harvard Dataverse, Kaggle, data.gov</li>
            <li><strong>Check for an API</strong> &mdash; Even if undocumented, try adding /api/ to the URL</li>
            <li><strong>Read the Terms of Service</strong> &mdash; Document that you checked and your interpretation</li>
            <li><strong>Check robots.txt</strong> &mdash; Respect all directives and document compliance</li>
            <li><strong>Consider fair use/TDM exceptions</strong> &mdash; Is your use transformative? Non-commercial? Does EU Article 3 apply?</li>
            <li><strong>Contact your IRB</strong> &mdash; Human subjects data may require approval</li>
            <li><strong>Consult your institution</strong> &mdash; Many universities have policies on scraping</li>
          </ul>
        </div>

        <div class="concept-box" style="background: #f0fdf4; border-left-color: #22c55e;">
          <h4 style="color: #166534;">Documentation Template for Research Scraping</h4>
          <p>It's a good idea to create a dated memo that includes:</p>
          <ol>
            <li><strong>Research purpose</strong>: What question are you investigating? Why is web data necessary?</li>
            <li><strong>Data source assessment</strong>: Did you check for official datasets, APIs, or data request options?</li>
            <li><strong>Legal analysis</strong>:
              <ul>
                <li>ToS review: What do the terms say? How does your use comply or qualify for exceptions?</li>
                <li>robots.txt review: What does it permit/prohibit? How will you comply?</li>
                <li>Applicable law: Which jurisdiction applies? What protections exist (CFAA/hiQ, EU TDM)?</li>
              </ul>
            </li>
            <li><strong>Ethical considerations</strong>: Is the data sensitive? Could individuals be harmed? What safeguards will you implement?</li>
            <li><strong>Technical safeguards</strong>: Rate limiting, caching, minimal data collection, secure storage</li>
            <li><strong>Institutional approvals</strong>: IRB status, departmental sign-off, legal consultation</li>
          </ol>
          <p style="margin-bottom: 0;"><strong>Keep this document with your project files.</strong> If questions arise years later during peer review or legal inquiry, this contemporaneous record demonstrates good faith.</p>
        </div>

        <!-- Section 3: HTML Basics -->
        <div class="section-divider"></div>
        <h2 id="html-basics"><span class="section-num">2c.3</span> Understanding HTML Structure</h2>

        <p>Now that you understand when and whether to scrape, it's time to learn <strong>how web pages are structured</strong>. This is the foundation of all scraping work. You can't extract data from a page if you don't understand where that data lives.</p>

        <p>Think of a web page like a document with a very precise organizational system. Just as a research paper has headings, paragraphs, tables, and footnotes, a web page has HTML elements that organize its content. Your scraper's job is to navigate this structure and extract exactly what you need.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">Visit the <a href="https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions" target="_blank">Wikipedia CO2 emissions page</a> and right-click ‚Üí Inspect on the data table. You'll see it's a <code>&lt;table&gt;</code> element with class <code>wikitable</code>. Each row (<code>&lt;tr&gt;</code>) contains cells (<code>&lt;td&gt;</code>) with country names and emissions data. This structure is what we'll target with our scraper.</p>
        </div>

        <div class="concept-box">
          <h4>What is HTML?</h4>
          <p style="margin-bottom: 0;"><strong>HTML (HyperText Markup Language)</strong> is the structure of web pages. It uses nested <strong>tags</strong> to organize content. To scrape data, you need to understand this structure so you can tell your code where to find the information you want.</p>
        </div>

        <h3>HTML Elements</h3>
        <p>HTML is made of <strong>elements</strong>‚Äîbuilding blocks that contain content. Every element has the same basic structure:</p>

        <div class="html-diagram">
<span class="tag">&lt;tagname</span> <span class="attr">attribute</span>=<span class="value">"value"</span><span class="tag">&gt;</span><span class="content">Content goes here</span><span class="tag">&lt;/tagname&gt;</span>

<span class="comment">&lt;!-- Common examples: --&gt;</span>
<span class="tag">&lt;p&gt;</span>This is a paragraph<span class="tag">&lt;/p&gt;</span>
<span class="tag">&lt;a</span> <span class="attr">href</span>=<span class="value">"https://example.com"</span><span class="tag">&gt;</span>This is a link<span class="tag">&lt;/a&gt;</span>
<span class="tag">&lt;div</span> <span class="attr">class</span>=<span class="value">"container"</span><span class="tag">&gt;</span>This is a division/section<span class="tag">&lt;/div&gt;</span>
<span class="tag">&lt;span</span> <span class="attr">id</span>=<span class="value">"price"</span><span class="tag">&gt;</span>$99.99<span class="tag">&lt;/span&gt;</span>
<span class="tag">&lt;table&gt;</span>...<span class="tag">&lt;/table&gt;</span>              <span class="comment">&lt;!-- Tables --&gt;</span>
<span class="tag">&lt;ul&gt;&lt;li&gt;</span>...<span class="tag">&lt;/li&gt;&lt;/ul&gt;</span>         <span class="comment">&lt;!-- Lists --&gt;</span>
        </div>

        <h3>Attributes: class and id</h3>
        <p>Attributes help identify specific elements. The most important for scraping are:</p>
        <ul>
          <li><strong>class</strong> &mdash; Shared by multiple elements (e.g., all prices might have <code>class="price"</code>)</li>
          <li><strong>id</strong> &mdash; Unique identifier for a single element (e.g., <code>id="main-content"</code>)</li>
        </ul>

        <h3>Viewing Page Source</h3>
        <p>To see a page's HTML structure:</p>
        <ol>
          <li>Right-click anywhere on the page</li>
          <li>Select "View Page Source" (sees original HTML) or "Inspect" (interactive developer tools)</li>
          <li>In developer tools, you can hover over elements to see their HTML</li>
        </ol>

        <!-- Step-by-step visual guide with screenshots -->
        <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; margin: 1.5rem 0;">
          <!-- Step 1: Right-click -->
          <div style="background: #f8fafc; border-radius: 12px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
            <div style="background: #1a365d; color: white; padding: 0.5rem 1rem; font-weight: 600; font-size: 0.85rem;">
              Step 1: Right-click
            </div>
            <div style="background: #e2e8f0; min-height: 160px;">
              <img src="../images/inspect-step1-rightclick.png" alt="Right-click on a webpage element to open context menu" style="width: 100%; height: 160px; object-fit: cover; display: block;">
            </div>
            <div style="padding: 0.75rem 1rem; font-size: 0.8rem; color: #475569; border-top: 1px solid #e2e8f0;">
              Right-click on any element ‚Üí Select "Inspect"
            </div>
          </div>

          <!-- Step 2: DevTools opens -->
          <div style="background: #f8fafc; border-radius: 12px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
            <div style="background: #2c5282; color: white; padding: 0.5rem 1rem; font-weight: 600; font-size: 0.85rem;">
              Step 2: DevTools Opens
            </div>
            <div style="background: #1e1e1e; min-height: 160px;">
              <img src="../images/inspect-step2-devtools.png" alt="Chrome DevTools panel showing HTML structure" style="width: 100%; height: 160px; object-fit: cover; display: block;">
            </div>
            <div style="padding: 0.75rem 1rem; font-size: 0.8rem; color: #475569; border-top: 1px solid #e2e8f0;">
              The Elements panel shows the HTML structure
            </div>
          </div>

          <!-- Step 3: Element highlighting -->
          <div style="background: #f8fafc; border-radius: 12px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
            <div style="background: #ed8936; color: white; padding: 0.5rem 1rem; font-weight: 600; font-size: 0.85rem;">
              Step 3: Hover to Highlight
            </div>
            <div style="background: #e2e8f0; min-height: 160px;">
              <img src="../images/inspect-step3-highlight.png" alt="Hovering over HTML highlights the element on the page" style="width: 100%; height: 160px; object-fit: cover; display: block;">
            </div>
            <div style="padding: 0.75rem 1rem; font-size: 0.8rem; color: #475569; border-top: 1px solid #e2e8f0;">
              Hover over HTML ‚Üí element lights up on page!
            </div>
          </div>
        </div>
        <p style="font-size: 0.85rem; color: #666; text-align: center; margin-top: 0.5rem;"><em>This is how you find the HTML selectors needed for your scraper</em></p>

        <div class="info-box tip">
          <div class="info-box-title">Pro Tip: Use the Element Picker</div>
          <p style="margin-bottom: 0;">In Chrome/Firefox developer tools, click the element picker icon (cursor in box) then click any element on the page. The HTML for that element will be highlighted in the developer panel.</p>
        </div>

        <h3>Example: A Simple Web Page</h3>
        <div class="html-diagram">
<span class="tag">&lt;html&gt;</span>
  <span class="tag">&lt;head&gt;</span>
    <span class="tag">&lt;title&gt;</span>Country GDP Data<span class="tag">&lt;/title&gt;</span>
  <span class="tag">&lt;/head&gt;</span>
  <span class="tag">&lt;body&gt;</span>
    <span class="tag">&lt;h1&gt;</span>GDP by Country<span class="tag">&lt;/h1&gt;</span>

    <span class="tag">&lt;table</span> <span class="attr">class</span>=<span class="value">"data-table"</span><span class="tag">&gt;</span>
      <span class="tag">&lt;tr&gt;</span>
        <span class="tag">&lt;th&gt;</span>Country<span class="tag">&lt;/th&gt;</span>
        <span class="tag">&lt;th&gt;</span>GDP (Billion USD)<span class="tag">&lt;/th&gt;</span>
      <span class="tag">&lt;/tr&gt;</span>
      <span class="tag">&lt;tr&gt;</span>
        <span class="tag">&lt;td&gt;</span>United States<span class="tag">&lt;/td&gt;</span>
        <span class="tag">&lt;td</span> <span class="attr">class</span>=<span class="value">"gdp-value"</span><span class="tag">&gt;</span>21,000<span class="tag">&lt;/td&gt;</span>
      <span class="tag">&lt;/tr&gt;</span>
      <span class="tag">&lt;tr&gt;</span>
        <span class="tag">&lt;td&gt;</span>China<span class="tag">&lt;/td&gt;</span>
        <span class="tag">&lt;td</span> <span class="attr">class</span>=<span class="value">"gdp-value"</span><span class="tag">&gt;</span>14,700<span class="tag">&lt;/td&gt;</span>
      <span class="tag">&lt;/tr&gt;</span>
    <span class="tag">&lt;/table&gt;</span>
  <span class="tag">&lt;/body&gt;</span>
<span class="tag">&lt;/html&gt;</span>
        </div>

        <!-- Section 4: Basic Scraping -->
        <div class="section-divider"></div>
        <h2 id="basic-scraping"><span class="section-num">2c.4</span> Basic Scraping Techniques</h2>

        <p>Now comes the practical part: <strong>actually writing code to extract data</strong>. We'll start with the fundamentals and build up to more complex techniques. By the end of this section, you'll be able to fetch a web page, find the data you need, and extract it into a usable format.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">For our CO2 emissions scraper, we need to: (1) fetch the page at <code>en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions</code>, (2) locate the table with class <code>wikitable</code>, and (3) extract each row into a DataFrame. The code examples below show exactly how to do this.</p>
        </div>

        <p>We'll use <strong>Python with BeautifulSoup</strong> (most common) and <strong>R with rvest</strong>. Stata doesn't have native scraping capabilities, but you can call Python from Stata (not covered here, but you can ask the chatbot if you're interested!).</p>

        <h3>Step 1: Fetch the Page</h3>

        <p><strong>The problem:</strong> Before you can extract data, you need to get the HTML content of the page into your program. This is like downloading the page's source code.</p>

        <p><strong>The solution:</strong> Use a library that sends HTTP requests (the same protocol your browser uses) and receives the HTML back.</p>

        <div class="code-tabs" data-runnable="fetch-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Fetch a web page</span>
<span class="code-tooltip" data-tip="IMPORT: This keyword loads external code libraries into your script. Think of it like adding tools to your toolbox - you need to import them before you can use them."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: Python's most popular library for fetching web pages. It handles all the complex HTTP communication so you can simply say 'get this URL' and receive the page content.">requests</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: This loads just ONE specific tool from a larger library. Instead of loading everything from bs4, we only load BeautifulSoup - this is more efficient."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="BS4: Short for BeautifulSoup4, this library parses HTML code and lets you search through it. The name 'beautiful soup' refers to the messy HTML code you often find on web pages.">bs4</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP: The main class that turns raw HTML text into a searchable structure. Once you create a BeautifulSoup object, you can use methods like find() and find_all() to locate specific elements.">BeautifulSoup</span>

<span class="code-comment"># Define a User-Agent (identifies your scraper)</span>
<span class="code-comment"># Be honest - some sites block generic Python requests</span>
<span class="code-tooltip" data-tip="HEADERS: A Python dictionary containing metadata about your request. Websites use this information to know who is visiting. Being transparent about your scraper helps build trust and avoids getting blocked.">headers</span> <span class="code-tooltip" data-tip="EQUALS SIGN: In Python, = assigns a value to a variable. The variable on the left (headers) will now contain the value on the right (the dictionary).">= </span><span class="code-tooltip" data-tip="DICTIONARY (curly braces): A Python data structure that stores key-value pairs. Here, 'User-Agent' is the key and the long string is its value. Dictionaries are perfect for storing labeled information.">{
    <span class="code-tooltip" data-tip="USER-AGENT KEY: This tells the server what browser or program is making the request. Including your contact email is good etiquette - it lets website owners reach you if there's a problem.">'User-Agent'</span>: <span class="code-tooltip" data-tip="USER-AGENT VALUE: This string identifies your scraper. The Mozilla/5.0 prefix makes it look like a browser, followed by a description and your contact info. Always include real contact information for research projects.">'Mozilla/5.0 (Research scraper for academic project; contact@university.edu)'</span>
}</span>

<span class="code-comment"># Fetch the page</span>
<span class="code-tooltip" data-tip="URL VARIABLE: Stores the web address you want to scrape. Using a variable makes your code cleaner and easier to modify - you only need to change this one line to scrape a different page.">url</span> = <span class="code-tooltip" data-tip="URL STRING: The complete web address of the page you want to fetch. Always use https:// (secure) when available. This is the exact address you would type in a browser.">"https://example.com/data"</span>
<span class="code-tooltip" data-tip="RESPONSE VARIABLE: Stores everything the server sends back - the page content, status codes, headers, etc. We call it 'response' because it's the server's response to our request.">response</span> = <span class="code-tooltip" data-tip="REQUESTS.GET(): This function sends an HTTP GET request to fetch a webpage. GET is the standard way to retrieve data (as opposed to POST which sends data). This is like typing a URL in your browser and pressing Enter.">requests.get</span>(<span class="code-tooltip" data-tip="URL PARAMETER: The first argument tells requests.get() which webpage to fetch. This is the address you want to visit.">url</span>, <span class="code-tooltip" data-tip="HEADERS PARAMETER: Passes your custom headers (including User-Agent) to the request. This identifies your scraper to the website server.">headers=headers</span>, <span class="code-tooltip" data-tip="TIMEOUT PARAMETER: Maximum seconds to wait for a response before giving up. Without this, your script could hang forever if a server doesn't respond. 30 seconds is a reasonable default.">timeout=<span class="code-number">30</span></span>)

<span class="code-comment"># Check if request was successful</span>
<span class="code-tooltip" data-tip="IF STATEMENT: Checks a condition and only runs the indented code below if the condition is true. Here we check if the request succeeded before trying to process the response."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS_CODE: HTTP responses include a number indicating success or failure. 200 means 'OK - everything worked'. 404 means 'not found', 500 means 'server error'. Always check this before processing!">response.status_code</span> <span class="code-tooltip" data-tip="EQUALS COMPARISON (==): Two equals signs check if two values are the same. This is different from single = which assigns values. Here we're asking 'is the status code equal to 200?'">==</span> <span class="code-tooltip" data-tip="200: The HTTP status code for success. When a server returns 200, it means your request was received, understood, and processed correctly. The page content should be available."><span class="code-number">200</span></span>:
    <span class="code-tooltip" data-tip="PRINT(): Displays text in the console/terminal. Useful for seeing what your code is doing, debugging problems, and confirming that operations succeeded."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="SUCCESS MESSAGE: This string confirms the page was fetched. Adding status messages helps you track what your scraper is doing, especially when processing many pages.">"Success! Page fetched."</span>)
    <span class="code-comment"># Parse the HTML</span>
    <span class="code-tooltip" data-tip="SOUP VARIABLE: A common name for BeautifulSoup objects. Once created, this variable lets you search through the HTML using methods like find(), find_all(), and select(). The name 'soup' is a convention in the Python community.">soup</span> = <span class="code-tooltip" data-tip="BEAUTIFULSOUP CONSTRUCTOR: Creates a new BeautifulSoup object from HTML text. The first argument is the HTML content, the second specifies which parser to use. This transforms raw text into a searchable tree structure.">BeautifulSoup</span>(<span class="code-tooltip" data-tip="RESPONSE.TEXT: The actual HTML content of the webpage as a string. This is what you see when you 'View Page Source' in a browser. The .text property extracts just the content, not headers or metadata.">response.text</span>, <span class="code-tooltip" data-tip="HTML.PARSER: Python's built-in HTML parser. It's reliable and doesn't require extra installation. Other options include 'lxml' (faster) or 'html5lib' (more lenient with broken HTML).">'html.parser'</span>)
<span class="code-tooltip" data-tip="ELSE: Runs this code block when the IF condition is false. Here, if the status code is NOT 200 (something went wrong), we print an error message instead of trying to parse the response."><span class="code-keyword">else</span></span>:
    <span class="code-keyword">print</span>(<span class="code-tooltip" data-tip="F-STRING (f prefix): A Python feature that lets you embed variables directly in strings using curly braces. The {response.status_code} will be replaced with the actual number (like 404 or 500).">f"Error: {response.status_code}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Fetch a web page</span>
<span class="code-tooltip" data-tip="LIBRARY(): This R function loads a package (library) so you can use its functions. You must call library() before using any functions from that package. It's like opening a toolbox."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="RVEST: R's main web scraping package, created by Hadley Wickham as part of the tidyverse. The name is a play on 'harvest' - you're harvesting data from the web. It provides simple functions for extracting data from HTML.">rvest</span>)
<span class="code-tooltip" data-tip="LIBRARY() for httr: Loading a second package. R lets you use multiple packages together."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="HTTR: R package for working with HTTP (web requests). It handles the low-level details of communicating with web servers. The name comes from 'HTTP' + 'R'. Used here to set custom headers.">httr</span>)

<span class="code-comment"># Define User-Agent</span>
<span class="code-tooltip" data-tip="SET_CONFIG(): Sets global options that apply to ALL subsequent web requests in your R session. Here we're setting a User-Agent header that identifies our scraper. This is polite and helps avoid being blocked.">set_config</span>(<span class="code-tooltip" data-tip="USER_AGENT(): Creates a User-Agent header that tells websites who is making the request. Always include contact info so website owners can reach you if there's an issue with your scraper."><span class="code-function">user_agent</span></span>(<span class="code-tooltip" data-tip="USER-AGENT STRING: Your scraper's identification. Include a brief description of your project and a real contact email. This transparency is professional and helps you avoid being mistaken for a malicious bot."><span class="code-string">"Research scraper for academic project; contact@university.edu"</span></span>))

<span class="code-comment"># Fetch and parse the page in one step</span>
<span class="code-tooltip" data-tip="URL VARIABLE: Stores the web address to scrape. In R, variable names can contain dots and underscores. Using a variable makes code easier to read and modify.">url</span> <span class="code-tooltip" data-tip="ASSIGNMENT OPERATOR (<-): R's way of assigning values to variables. The arrow points from the value to the variable name. You can also use = but <- is the traditional R style.">&lt;-</span> <span class="code-tooltip" data-tip="URL STRING: The complete web address you want to fetch. R strings can use single or double quotes. This is where your target webpage lives."><span class="code-string">"https://example.com/data"</span></span>
<span class="code-tooltip" data-tip="PAGE VARIABLE: Stores the parsed HTML document. We call it 'page' because it represents the entire webpage. This object can then be searched using html_element() and html_elements().">page</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result of read_html() in the 'page' variable for later use.">&lt;-</span> <span class="code-tooltip" data-tip="READ_HTML(): The rvest function that does TWO things at once: (1) fetches the webpage from the URL, and (2) parses the HTML into a searchable structure. This is rvest's most important function - it's your starting point for any scraping task."><span class="code-function">read_html</span></span>(<span class="code-tooltip" data-tip="URL ARGUMENT: Passing the URL to read_html(). The function will fetch this webpage and return the parsed HTML content.">url</span>)

<span class="code-comment"># Page is now ready for extraction</span>
<span class="code-tooltip" data-tip="PRINT(): Displays output in the R console. Useful for confirming your code is working and for debugging. In RStudio, you'll see this message in the Console pane."><span class="code-function">print</span></span>(<span class="code-tooltip" data-tip="STATUS MESSAGE: A simple confirmation that the operation succeeded. Adding these messages helps you track what your script is doing, especially when processing multiple pages."><span class="code-string">"Page fetched successfully"</span></span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="fetch-1" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-green">Success! Page fetched.</span>

<span class="out-blue">Response details:</span>
Status: <span class="out-num">200</span> OK
Content-Type: text/html; charset=utf-8
Content-Length: <span class="out-num">45,231</span> bytes</div>
        </div>

        <div class="output-simulation" data-output="fetch-1" data-lang="r">
          <div class="output-header">
            <span>R Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-green">Page fetched successfully</span>

{html_document}
&lt;html&gt;
[1] &lt;head&gt;...
[2] &lt;body&gt;...</div>
        </div>

        <h3>Step 2: Extract Data</h3>

        <div class="code-tabs" data-runnable="extract-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Extract data from HTML</span>

<span class="code-comment"># Find a single element</span>
<span class="code-tooltip" data-tip="TITLE VARIABLE: Stores the first h1 element found on the page. We expect this to be the main page title. If no h1 exists, this will be None.">title</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result of soup.find() in the title variable.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND(): BeautifulSoup's method to find the FIRST element matching your criteria. Think of it like 'Ctrl+F' in a document - it stops at the first match. Returns None if nothing matches.">soup.find</span>(<span class="code-tooltip" data-tip="TAG NAME 'h1': HTML uses tags like h1 (heading 1), p (paragraph), div (division). Here we're looking for the first h1 tag, which is typically the main page title.">'h1'</span>)
<span class="code-tooltip" data-tip="PRINT(): Displays the result in your console so you can see what was extracted."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="F-STRING WITH .TEXT: The f prefix enables variable insertion. {title.text} extracts just the text content from the h1 element, removing the HTML tags. So <h1>GDP Data</h1> becomes just 'GDP Data'.">f"Title: {title.text}"</span>)

<span class="code-comment"># Find by class name</span>
<span class="code-tooltip" data-tip="PRICE VARIABLE: Will hold the first span element with class='price'. Websites often use consistent class names for similar data (all prices, all product names, etc.).">price</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the found element.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND() WITH CLASS: Finds the first element matching BOTH criteria - must be a 'span' tag AND have the class 'price'. This is more precise than searching by tag alone.">soup.find</span>(<span class="code-tooltip" data-tip="TAG NAME 'span': A generic inline HTML element often used to wrap small pieces of text like prices, labels, or highlights.">'span'</span>, <span class="code-tooltip" data-tip="CLASS_ PARAMETER: Note the underscore! In Python, 'class' is a reserved keyword, so BeautifulSoup uses 'class_' instead. This filters to only elements with this CSS class.">class_=<span class="code-string">'price'</span></span>)

<span class="code-comment"># Find by id</span>
<span class="code-tooltip" data-tip="MAIN_CONTENT VARIABLE: Will hold the element with id='main-content'. IDs are unique on a page - there can only be one element with each id.">main_content</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the found element.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND() BY ID: When you specify just id=, find() searches for any element with that id attribute. IDs are unique, so this will always return at most one element.">soup.find</span>(<span class="code-tooltip" data-tip="ID PARAMETER: Searches for an element with this exact id attribute. Unlike classes (which can be shared), each id should be unique on a page. Great for finding specific page sections.">id=<span class="code-string">'main-content'</span></span>)

<span class="code-comment"># Find ALL matching elements</span>
<span class="code-tooltip" data-tip="ALL_PRICES VARIABLE: Will hold a LIST of all matching elements, not just the first one. You can loop through this list to process each price.">all_prices</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the list of all found elements.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND_ALL(): Unlike find() which returns ONE element, find_all() returns a LIST of ALL elements matching your criteria. Use this when you need to collect multiple items (all prices, all links, all table rows).">soup.find_all</span>(<span class="code-tooltip" data-tip="TAG NAME 'span': Looking for all span elements...">'span'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: ...that also have the class 'price'. Only elements matching BOTH criteria are returned.">class_=<span class="code-string">'price'</span></span>)
<span class="code-tooltip" data-tip="FOR LOOP: Iterates through each element in the all_prices list. The variable 'p' holds one price element at a time. This is how you process multiple found elements."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="LOOP VARIABLE 'p': On each iteration, 'p' holds the current price element. You can use any variable name here - 'p' is short for 'price'.">p</span> <span class="code-tooltip" data-tip="IN KEYWORD: Part of the for loop syntax. Means 'for each item in this collection'."><span class="code-keyword">in</span></span> <span class="code-tooltip" data-tip="ALL_PRICES LIST: The list we're iterating through. Each element in this list is a BeautifulSoup element with .text and other properties.">all_prices</span>:
    <span class="code-tooltip" data-tip="PRINT INSIDE LOOP: This runs once for each price in the list, printing each price's text content."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="P.TEXT: Extracts the text content from the current element, removing HTML tags. For <span class='price'>$99.99</span>, this returns '$99.99'.">p.text</span>)

<span class="code-comment"># Extract from a table</span>
<span class="code-tooltip" data-tip="TABLE VARIABLE: Stores the table element. HTML tables are common for structured data like GDP figures, stock prices, or any tabular information.">table</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the found table element.">=</span> <span class="code-tooltip" data-tip="FINDING A TABLE: Locates the table with class 'data-table'. Tables contain rows (tr), which contain header cells (th) or data cells (td).">soup.find</span>(<span class="code-tooltip" data-tip="TABLE TAG: HTML tables use the <table> tag. They contain rows (<tr>), header cells (<th>), and data cells (<td>).">'table'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: Only finds the table with this specific class, ignoring other tables on the page.">class_=<span class="code-string">'data-table'</span></span>)
<span class="code-tooltip" data-tip="ROWS VARIABLE: A list of all <tr> (table row) elements inside the table. The first row is usually the header (Country, GDP), and subsequent rows contain the data.">rows</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the list of rows.">=</span> <span class="code-tooltip" data-tip="TABLE.FIND_ALL(): Searches WITHIN the table element (not the whole page) for all tr elements. This is called chaining - find inside a find.">table.find_all</span>(<span class="code-tooltip" data-tip="TR TAG: 'tr' stands for 'table row'. Each tr contains either header cells (th) or data cells (td).">'tr'</span>)
<span class="code-tooltip" data-tip="FOR LOOP: Iterating through table rows to extract data from each one."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="ROW VARIABLE: Holds one table row at a time as we loop through.">row</span> <span class="code-keyword">in</span> <span class="code-tooltip" data-tip="ROWS[1:] - SLICE NOTATION: The [1:] means 'start from index 1 to the end'. Since Python uses 0-based indexing, this SKIPS the first row (index 0), which is typically the header row with column titles.">rows[<span class="code-number">1</span>:]</span>:  <span class="code-comment"># Skip header row</span>
    <span class="code-tooltip" data-tip="CELLS VARIABLE: A list of all <td> (table data) cells in this row. For a Country/GDP table, cells[0] would be the country name and cells[1] would be the GDP value.">cells</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the list of cells from this row.">=</span> <span class="code-tooltip" data-tip="ROW.FIND_ALL(): Finds all td elements WITHIN this specific row. This gives us the individual data cells.">row.find_all</span>(<span class="code-tooltip" data-tip="TD TAG: 'td' stands for 'table data'. These are the individual cells containing actual data values.">'td'</span>)
    <span class="code-tooltip" data-tip="COUNTRY VARIABLE: Extracts the country name from the first cell (index 0). The .strip() removes any extra whitespace.">country</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the country name.">=</span> <span class="code-tooltip" data-tip="CELLS[0].TEXT.STRIP(): cells[0] gets the first cell, .text extracts the text content, and .strip() removes leading/trailing whitespace. This ensures clean data.">cells[<span class="code-number">0</span>].text.strip()</span>
    <span class="code-tooltip" data-tip="GDP VARIABLE: Extracts the GDP value from the second cell (index 1).">gdp</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the GDP value.">=</span> <span class="code-tooltip" data-tip="CELLS[1].TEXT.STRIP(): Gets the second cell's text content, cleaned of whitespace.">cells[<span class="code-number">1</span>].text.strip()</span>
    <span class="code-tooltip" data-tip="PRINT EXTRACTED DATA: Displays each country and its GDP. The f-string formats them nicely."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="F-STRING OUTPUT: Combines the country and GDP variables into a readable format like 'United States: 21,000'.">f"{country}: {gdp}"</span>)

<span class="code-comment"># Extract attribute values (like href from links)</span>
<span class="code-tooltip" data-tip="LINKS VARIABLE: A list of all anchor (<a>) elements on the page. Anchor tags create hyperlinks and contain href attributes with the URLs.">links</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing all found link elements.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND_ALL('a'): Finds all anchor (link) elements on the page. Useful for collecting URLs, navigating to other pages, or finding pagination links.">soup.find_all</span>(<span class="code-tooltip" data-tip="ANCHOR TAG 'a': The HTML tag for hyperlinks. The actual URL is stored in the href attribute, and the clickable text is between the opening and closing tags.">'a'</span>)
<span class="code-tooltip" data-tip="FOR LOOP: Iterating through each link element to extract its URL."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="LINK VARIABLE: Holds one anchor element at a time.">link</span> <span class="code-keyword">in</span> <span class="code-tooltip" data-tip="LINKS LIST: The collection of all anchor elements we're iterating through.">links</span>:
    <span class="code-tooltip" data-tip="HREF VARIABLE: Will store the URL from the link's href attribute. This is where the link points to.">href</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the extracted URL.">=</span> <span class="code-tooltip" data-tip="LINK.GET('href'): Safely extracts the href attribute from the link. Using .get() instead of ['href'] is safer because it returns None if the attribute doesn't exist, rather than crashing your code.">link.get</span>(<span class="code-tooltip" data-tip="HREF ATTRIBUTE: The href (hypertext reference) attribute contains the URL that the link points to. This could be a relative path like '/about' or a full URL like 'https://example.com'.">'href'</span>)
    <span class="code-tooltip" data-tip="PRINT URL: Displays each extracted URL. You might see relative paths (/page) or absolute URLs (https://...)."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="HREF VALUE: The URL extracted from the link. Could be None if the link has no href attribute.">href</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Extract data from HTML using rvest</span>

<span class="code-comment"># Find elements using CSS selectors</span>
<span class="code-tooltip" data-tip="TITLE VARIABLE: Will store the text content of the first h1 element on the page. This is typically the main page heading.">title</span> <span class="code-tooltip" data-tip="ASSIGNMENT OPERATOR: R's way of assigning values. The arrow points from the value (right) to the variable name (left).">&lt;-</span> <span class="code-tooltip" data-tip="PAGE OBJECT: The parsed HTML document from read_html(). This is our starting point for all extractions.">page</span> <span class="code-tooltip" data-tip="PIPE OPERATOR (%>%): Passes the result from the left side as input to the function on the right. Think of it as 'then do this'. It makes code read like a sequence of steps: take page, THEN find h1, THEN extract text.">%>%</span>
  <span class="code-tooltip" data-tip="HTML_ELEMENT(): rvest function that finds the FIRST element matching a CSS selector. Use this when you expect exactly one match (like a page title). For multiple matches, use html_elements() with an 's'."><span class="code-function">html_element</span></span>(<span class="code-tooltip" data-tip="CSS SELECTOR 'h1': Selects the first h1 (heading 1) element. CSS selectors are a powerful way to target HTML elements. Here we use just the tag name, but you can combine with classes (.class) and IDs (#id).">"h1"</span>) <span class="code-tooltip" data-tip="PIPE: Passes the found h1 element to the next function.">%>%</span>
  <span class="code-tooltip" data-tip="HTML_TEXT(): Extracts the text content from an HTML element, removing all tags. For example, <h1>GDP Data</h1> becomes just 'GDP Data'. This is how you get the actual readable text."><span class="code-function">html_text</span></span>()
<span class="code-tooltip" data-tip="PRINT(): Displays output in the R console. Essential for seeing your results and debugging."><span class="code-function">print</span></span>(<span class="code-tooltip" data-tip="PASTE(): Combines multiple strings into one. Here it creates 'Title: GDP Data' by joining 'Title:' with the extracted title value."><span class="code-function">paste</span></span>(<span class="code-tooltip" data-tip="LABEL STRING: A literal text label to make the output more readable.">"Title:"</span>, <span class="code-tooltip" data-tip="TITLE VARIABLE: The extracted title text we stored earlier.">title</span>))

<span class="code-comment"># Find by class (use . prefix for class)</span>
<span class="code-tooltip" data-tip="PRICES VARIABLE: Will store a character vector (list of strings) containing all extracted price texts. Unlike Python which returns a list of elements, rvest here returns just the text values.">prices</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the extracted prices.">&lt;-</span> <span class="code-tooltip" data-tip="PAGE: Our parsed HTML document.">page</span> <span class="code-tooltip" data-tip="PIPE: Passes the page to html_elements().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_ELEMENTS() (with 's'): Returns ALL matching elements, not just the first one. Use the plural form when you expect multiple matches, like all prices on a product listing page."><span class="code-function">html_elements</span></span>(<span class="code-tooltip" data-tip="CSS SELECTOR '.price': The DOT prefix means 'find elements with this class'. So .price finds all elements with class='price'. This is standard CSS selector syntax, same as used in web design.">".price"</span>) <span class="code-tooltip" data-tip="PIPE: Passes all found price elements to html_text().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_TEXT(): When applied to multiple elements, extracts text from each one and returns a character vector (R's equivalent of a list of strings)."><span class="code-function">html_text</span></span>()

<span class="code-comment"># Find by id (use # prefix)</span>
<span class="code-tooltip" data-tip="MAIN_CONTENT VARIABLE: Stores the text content of the element with id='main-content'. IDs are unique on a page, so there's only one possible match.">main_content</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the extracted content.">&lt;-</span> <span class="code-tooltip" data-tip="PAGE: Starting from our parsed HTML.">page</span> <span class="code-tooltip" data-tip="PIPE: Passing to the next step.">%>%</span>
  <span class="code-tooltip" data-tip="HTML_ELEMENT(): Using singular (not plural) because IDs are unique - there can only be one match."><span class="code-function">html_element</span></span>(<span class="code-tooltip" data-tip="CSS SELECTOR '#main-content': The HASH prefix means 'find element with this ID'. IDs must be unique on a page, so this always returns at most one element. Great for targeting specific page sections.">"#main-content"</span>) <span class="code-tooltip" data-tip="PIPE: Passing the found element to html_text().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_TEXT(): Extracts the text content from the main-content element."><span class="code-function">html_text</span></span>()

<span class="code-comment"># Extract a table (rvest makes this easy!)</span>
<span class="code-tooltip" data-tip="TABLE_DATA VARIABLE: Will store the table as an R data frame (tibble). This is one of rvest's most powerful features - automatic table extraction!">table_data</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the extracted table data.">&lt;-</span> <span class="code-tooltip" data-tip="PAGE: Our parsed HTML document.">page</span> <span class="code-tooltip" data-tip="PIPE: Passing to html_element().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_ELEMENT(): Finding the specific table we want."><span class="code-function">html_element</span></span>(<span class="code-tooltip" data-tip="CSS SELECTOR 'table.data-table': Combines tag name (table) with class (.data-table). This finds a <table> element that has class='data-table'. More specific than just 'table' which would match any table.">"table.data-table"</span>) <span class="code-tooltip" data-tip="PIPE: Passing the table element to html_table().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_TABLE(): Automatically converts an HTML table into an R data frame/tibble. It handles headers, rows, and columns for you. This is incredibly convenient - what would take many lines in Python happens in one function!"><span class="code-function">html_table</span></span>()
<span class="code-tooltip" data-tip="PRINT(): Displays the extracted table. In RStudio, you'll see a nicely formatted tibble with column names and data types."><span class="code-function">print</span></span>(<span class="code-tooltip" data-tip="TABLE_DATA: The data frame containing our extracted table data. You can now use all R's data manipulation functions on this!">table_data</span>)

<span class="code-comment"># Extract attribute values (like href)</span>
<span class="code-tooltip" data-tip="LINKS VARIABLE: Will store a character vector of all URLs found in href attributes.">links</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the extracted URLs.">&lt;-</span> <span class="code-tooltip" data-tip="PAGE: Our parsed HTML.">page</span> <span class="code-tooltip" data-tip="PIPE: Passing to html_elements().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_ELEMENTS(): Finding ALL anchor (link) elements on the page."><span class="code-function">html_elements</span></span>(<span class="code-tooltip" data-tip="CSS SELECTOR 'a': The anchor tag creates hyperlinks. We're finding all of them to extract their destinations.">"a"</span>) <span class="code-tooltip" data-tip="PIPE: Passing all links to html_attr().">%>%</span>
  <span class="code-tooltip" data-tip="HTML_ATTR(): Extracts a specific attribute from elements. While html_text() gets the content between tags, html_attr() gets the value of an attribute like href, src, class, etc."><span class="code-function">html_attr</span></span>(<span class="code-tooltip" data-tip="HREF ATTRIBUTE: The href (hypertext reference) attribute contains the URL that a link points to. This is what we want to extract from each anchor element.">"href"</span>)
<span class="code-tooltip" data-tip="PRINT(): Displays all extracted URLs. You'll see a character vector listing each link destination."><span class="code-function">print</span></span>(<span class="code-tooltip" data-tip="LINKS: The vector of all extracted URLs from the page.">links</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="extract-1" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">Title: <span class="out-str">GDP by Country</span>

<span class="out-blue">Table data extracted:</span>
<span class="out-str">United States</span>: <span class="out-num">21,000</span>
<span class="out-str">China</span>: <span class="out-num">14,700</span>
<span class="out-str">Japan</span>: <span class="out-num">5,100</span>
<span class="out-str">Germany</span>: <span class="out-num">3,800</span>

<span class="out-blue">Links found:</span>
/about
/data/countries
https://example.com/methodology</div>
        </div>

        <div class="output-simulation" data-output="extract-1" data-lang="r">
          <div class="output-header">
            <span>R Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">[1] "Title: GDP by Country"

<span class="out-green"># A tibble: 4 x 2</span>
  Country       `GDP (Billion USD)`
  <span class="out-blue">&lt;chr&gt;</span>                       <span class="out-blue">&lt;dbl&gt;</span>
<span class="out-num">1</span> United States               21000
<span class="out-num">2</span> China                       14700
<span class="out-num">3</span> Japan                        5100
<span class="out-num">4</span> Germany                      3800

[1] <span class="out-str">"/about"</span> <span class="out-str">"/data/countries"</span> <span class="out-str">"https://example.com/methodology"</span></div>
        </div>

        <h3>CSS Selectors Quick Reference</h3>
        <table style="width: 100%; margin: 1rem 0;">
          <thead>
            <tr>
              <th>Selector</th>
              <th>Meaning</th>
              <th>Example</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>tag</code></td>
              <td>Element by tag name</td>
              <td><code>div</code>, <code>table</code>, <code>a</code></td>
            </tr>
            <tr>
              <td><code>.class</code></td>
              <td>Element by class</td>
              <td><code>.price</code>, <code>.data-table</code></td>
            </tr>
            <tr>
              <td><code>#id</code></td>
              <td>Element by id</td>
              <td><code>#main-content</code></td>
            </tr>
            <tr>
              <td><code>tag.class</code></td>
              <td>Tag with specific class</td>
              <td><code>span.price</code></td>
            </tr>
            <tr>
              <td><code>parent child</code></td>
              <td>Nested elements</td>
              <td><code>table tr td</code></td>
            </tr>
            <tr>
              <td><code>[attr=value]</code></td>
              <td>By attribute</td>
              <td><code>[data-country="USA"]</code></td>
            </tr>
          </tbody>
        </table>

        <!-- Section 5: Advanced Techniques -->
        <div class="section-divider"></div>
        <h2 id="advanced-techniques"><span class="section-num">2c.5</span> Advanced Techniques</h2>

        <p>The basic techniques work great for simple pages. But real-world scraping often hits complications. In this section, we'll tackle the <strong>common challenges you'll encounter</strong> and show you how to solve them.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Beyond Our CO2 Example: Real-World Challenges</h4>
          <p style="margin-bottom: 0.75rem;">Our Wikipedia example is intentionally simple. In real research projects, you'll face additional challenges:</p>
          <ul style="margin-bottom: 0;">
            <li><strong>Pagination:</strong> Data spread across multiple pages (e.g., search results showing "Page 1 of 50")</li>
            <li><strong>Dynamic content:</strong> Modern sites load data with JavaScript‚Äîyour basic scraper sees an empty page</li>
            <li><strong>Rate limiting:</strong> Hitting a server too fast will get your IP blocked</li>
            <li><strong>Authentication:</strong> Some data requires login (not covered here‚Äîask the chatbot if you need this)</li>
          </ul>
        </div>

        <h3>Handling Pagination</h3>

        <p><strong>The problem:</strong> You've successfully scraped the first page of results, but the website shows "Page 1 of 10"‚Äîthe data you need is spread across multiple pages.</p>

        <p><strong>Why this happens:</strong> Websites break large datasets into pages to improve load times and user experience. A municipal budget might list hundreds of line items across dozens of pages.</p>

        <p><strong>The solution:</strong> Write a loop that visits each page in sequence, extracting data from each one and combining the results.</p>

        <div class="code-tabs" data-runnable="pagination">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Handle pagination</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the requests library for fetching web pages."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: Python's HTTP library. We've seen this before - it handles fetching web pages.">requests</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading just BeautifulSoup from the bs4 package."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="BS4: The BeautifulSoup package for parsing HTML.">bs4</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP: The HTML parsing class we'll use to search through page content.">BeautifulSoup</span>
<span class="code-tooltip" data-tip="IMPORT TIME: Loading Python's time module, which provides functions for working with time, including pausing execution."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="TIME MODULE: Python's built-in module for time-related functions. The key function here is time.sleep() which pauses your code - essential for polite scraping that doesn't overwhelm servers.">time</span>

<span class="code-comment"># Store all results</span>
<span class="code-tooltip" data-tip="ALL_DATA LIST: An empty list that will collect all items from all pages. We start empty and append items as we find them. This is our 'shopping cart' for scraped data.">all_data</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating an empty list.">=</span> <span class="code-tooltip" data-tip="EMPTY LIST []: Square brackets with nothing inside create an empty list. As we scrape each page, we'll add (append) items to this list.">[]</span>

<span class="code-comment"># Loop through pages</span>
<span class="code-tooltip" data-tip="FOR LOOP: This will repeat the indented code below once for each page number. Each iteration, page_num takes the next value in the sequence."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="PAGE_NUM VARIABLE: Holds the current page number (1, then 2, then 3, etc.). We use this to build the URL for each page.">page_num</span> <span class="code-tooltip" data-tip="IN KEYWORD: Part of the for loop syntax."><span class="code-keyword">in</span></span> <span class="code-tooltip" data-tip="RANGE(1, 11): Generates numbers from 1 to 10 (the second number is exclusive). So this creates: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Perfect for pagination!"><span class="code-function">range</span>(<span class="code-number">1</span>, <span class="code-number">11</span>)</span>:  <span class="code-comment"># Pages 1-10</span>
    <span class="code-tooltip" data-tip="URL VARIABLE: Builds the URL for the current page by inserting the page number. Many websites use ?page=N or /page/N patterns for pagination.">url</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the constructed URL.">=</span> <span class="code-tooltip" data-tip="F-STRING WITH VARIABLE: The {page_num} gets replaced with the actual number. So when page_num is 3, this becomes 'https://example.com/data?page=3'.">f"https://example.com/data?page={page_num}"</span>

    <span class="code-comment"># Be polite: wait between requests</span>
    <span class="code-tooltip" data-tip="TIME.SLEEP(): Pauses your code for the specified number of seconds. CRITICAL for responsible scraping! Without delays, you could overwhelm the server with too many requests per second, which is both impolite and might get you blocked.">time.sleep</span>(<span class="code-tooltip" data-tip="DELAY VALUE (2): Wait 2 seconds between each request. Some sites specify this in robots.txt (Crawl-delay). When in doubt, be more conservative (longer delays). For research, 2-5 seconds is typically polite."><span class="code-number">2</span></span>)  <span class="code-comment"># Wait 2 seconds</span>

    <span class="code-tooltip" data-tip="RESPONSE: Fetching the current page. Same pattern we used before - send request, store response.">response</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the server's response.">=</span> <span class="code-tooltip" data-tip="REQUESTS.GET(): Fetching the page at the current URL.">requests.get</span>(<span class="code-tooltip" data-tip="URL: The page address we constructed above.">url</span>, <span class="code-tooltip" data-tip="HEADERS: Including our User-Agent to identify our scraper (defined earlier in the code).">headers=headers</span>)
    <span class="code-tooltip" data-tip="IF STATEMENT: Checking for errors before processing."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS CODE CHECK: If NOT 200 (success), something went wrong. Could be 404 (not found), 429 (rate limited), 500 (server error), etc.">response.status_code</span> <span class="code-tooltip" data-tip="NOT EQUALS (!=): Checks if two values are different. Here we're checking if the status is NOT 200 (not successful).">!=</span> <span class="code-tooltip" data-tip="200: The success status code. Anything else indicates a problem."><span class="code-number">200</span></span>:
        <span class="code-tooltip" data-tip="PRINT ERROR: Letting you know which page failed."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Reports which page number caused the error.">f"Error on page {page_num}, stopping"</span>)
        <span class="code-tooltip" data-tip="BREAK: Immediately exits the for loop. We stop scraping because something went wrong. This prevents wasting time on subsequent pages that might also fail."><span class="code-keyword">break</span></span>

    <span class="code-tooltip" data-tip="SOUP: Creating a BeautifulSoup object from this page's HTML.">soup</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the parsed HTML.">=</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP(): Parsing the response text into a searchable structure.">BeautifulSoup</span>(<span class="code-tooltip" data-tip="RESPONSE.TEXT: The HTML content of the current page.">response.text</span>, <span class="code-tooltip" data-tip="PARSER: Using Python's built-in HTML parser.">'html.parser'</span>)

    <span class="code-comment"># Extract data from this page</span>
    <span class="code-tooltip" data-tip="ITEMS: A list of all items on this page. Each page might have 10-50 items that we need to extract.">items</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing all found items.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND_ALL(): Finding all div elements with class 'item'. This is how we locate each individual data entry on the page.">soup.find_all</span>(<span class="code-tooltip" data-tip="DIV TAG: A common container element. Websites often wrap each item (product, article, etc.) in its own div.">'div'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: Only finding divs that have class='item'. This targets the specific elements containing our data.">class_=<span class="code-string">'item'</span></span>)

    <span class="code-comment"># Check if page is empty (end of data)</span>
    <span class="code-tooltip" data-tip="IF STATEMENT: Checking if we've reached the end of the data."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="NOT KEYWORD: Inverts a boolean. An empty list is 'falsy' in Python, so 'not items' is True when items is empty."><span class="code-keyword">not</span></span> <span class="code-tooltip" data-tip="EMPTY CHECK: If items is an empty list (no items found), this condition is True. This usually means we've gone past the last page of data.">items</span>:
        <span class="code-tooltip" data-tip="PRINT END MESSAGE: Reporting that we've reached the end of available data."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="END MESSAGE: Reports the last page that had data. We subtract 1 because this page (page_num) was empty.">f"No more data after page {page_num-1}"</span>)
        <span class="code-tooltip" data-tip="BREAK: Exits the loop since there's no more data to scrape."><span class="code-keyword">break</span></span>

    <span class="code-tooltip" data-tip="NESTED FOR LOOP: Now we loop through each item found on this page to extract its data."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="ITEM VARIABLE: Holds one item at a time as we process the page's items.">item</span> <span class="code-keyword">in</span> <span class="code-tooltip" data-tip="ITEMS LIST: The list of all item elements we found on this page.">items</span>:
        <span class="code-tooltip" data-tip="ALL_DATA.APPEND(): Adds a new dictionary to our collection. Each dictionary represents one scraped item with its properties.">all_data.append</span>(<span class="code-tooltip" data-tip="DICTIONARY: Creating a structured record for this item. Keys ('name', 'value') label the data, and values are the extracted text.">{
            <span class="code-tooltip" data-tip="NAME KEY: The label for the first piece of data we're extracting.">'name'</span>: <span class="code-tooltip" data-tip="ITEM.FIND('h2').TEXT: Finds the h2 element INSIDE this specific item and extracts its text. This gets the item's title/name.">item.find('h2').text</span>,
            <span class="code-tooltip" data-tip="VALUE KEY: The label for the second piece of data.">'value'</span>: <span class="code-tooltip" data-tip="ITEM.FIND().TEXT: Finds the span with class 'value' inside this item and gets its text. This extracts the numeric/data value.">item.find('span', class_='value').text</span>
        })</span>

    <span class="code-tooltip" data-tip="PROGRESS MESSAGE: Keeping track of our scraping progress. This helps you monitor the scraper and spot any issues."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="STATUS UPDATE: Shows the current page number and how many items were found. len() counts the items in the list.">f"Page {page_num}: {len(items)} items"</span>)

<span class="code-tooltip" data-tip="FINAL SUMMARY: After the loop finishes, report the total amount of data collected."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="TOTAL COUNT: len(all_data) counts all items in our collection. This is your final dataset size!">f"Total: {len(all_data)} items collected"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Handle pagination</span>
<span class="code-tooltip" data-tip="LIBRARY(): Loading the rvest package for web scraping."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="RVEST: R's web scraping package for parsing HTML and extracting data.">rvest</span>)
<span class="code-tooltip" data-tip="LIBRARY(): Loading another package."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="PURRR: Part of the tidyverse, provides functional programming tools. We use it here for map_dfr() which applies a function to multiple inputs and combines results.">purrr</span>)

<span class="code-comment"># Function to scrape one page</span>
<span class="code-tooltip" data-tip="SCRAPE_PAGE: We're creating a custom function that handles scraping a single page. This makes our code reusable and cleaner - we can call it for any page number.">scrape_page</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Assigning our function definition to the scrape_page variable.">&lt;-</span> <span class="code-tooltip" data-tip="FUNCTION(): R's way of creating a new function. Everything inside the curly braces {} is the function body - the code that runs when you call the function."><span class="code-keyword">function</span></span>(<span class="code-tooltip" data-tip="PAGE_NUM PARAMETER: The input to our function. When we call scrape_page(3), page_num becomes 3 inside the function.">page_num</span>) {
  <span class="code-comment"># Be polite: wait between requests</span>
  <span class="code-tooltip" data-tip="SYS.SLEEP(): R's function to pause execution. Like time.sleep() in Python. This is CRITICAL for polite scraping - it prevents overwhelming servers and getting blocked."><span class="code-function">Sys.sleep</span></span>(<span class="code-tooltip" data-tip="DELAY VALUE (2): Wait 2 seconds. Adjust this based on the site's robots.txt or their rate limiting. More is more polite; less risks getting blocked."><span class="code-number">2</span></span>)

  <span class="code-tooltip" data-tip="URL VARIABLE: Constructing the URL for this specific page.">url</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the URL.">&lt;-</span> <span class="code-tooltip" data-tip="PASTE0(): Concatenates strings without any separator. Here it joins the base URL with the page number to create 'https://example.com/data?page=3' (for example)."><span class="code-function">paste0</span></span>(<span class="code-tooltip" data-tip="BASE URL: The URL pattern with the page number at the end. Many sites use ?page= for pagination."><span class="code-string">"https://example.com/data?page="</span></span>, <span class="code-tooltip" data-tip="PAGE_NUM: The current page number passed to this function.">page_num</span>)
  <span class="code-tooltip" data-tip="PAGE VARIABLE: Storing the fetched and parsed HTML.">page</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the parsed page.">&lt;-</span> <span class="code-tooltip" data-tip="READ_HTML(): Fetches the URL and parses the HTML in one step."><span class="code-function">read_html</span></span>(<span class="code-tooltip" data-tip="URL: The constructed URL for the current page.">url</span>)

  <span class="code-comment"># Extract table or items</span>
  <span class="code-tooltip" data-tip="PAGE: Starting with the parsed HTML document.">page</span> <span class="code-tooltip" data-tip="PIPE: Passing to the next function.">%>%</span>
    <span class="code-tooltip" data-tip="HTML_ELEMENT(): Finding the table element."><span class="code-function">html_element</span></span>(<span class="code-tooltip" data-tip="CSS SELECTOR: Finding a table with class 'data-table'."><span class="code-string">"table.data-table"</span></span>) <span class="code-tooltip" data-tip="PIPE: Passing to html_table().">%>%</span>
    <span class="code-tooltip" data-tip="HTML_TABLE(): Converting the HTML table to an R data frame. The LAST expression in a function is automatically returned - so this data frame becomes the function's output."><span class="code-function">html_table</span></span>()
}

<span class="code-comment"># Scrape pages 1-10</span>
<span class="code-tooltip" data-tip="ALL_DATA: Will contain all scraped data combined into one data frame.">all_data</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the combined results.">&lt;-</span> <span class="code-tooltip" data-tip="MAP_DFR(): A powerful purrr function that (1) applies scrape_page to each number 1-10, (2) collects all the returned data frames, and (3) row-binds them into one big data frame. The '_dfr' suffix means 'data frame, bind by rows'."><span class="code-function">map_dfr</span></span>(<span class="code-tooltip" data-tip="SEQUENCE 1:10: Creates the numbers 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Each number gets passed to scrape_page() in turn."><span class="code-number">1</span>:<span class="code-number">10</span></span>, <span class="code-tooltip" data-tip="SCRAPE_PAGE: Our custom function. map_dfr will call scrape_page(1), then scrape_page(2), etc., and combine all results.">scrape_page</span>)

<span class="code-tooltip" data-tip="PRINT(): Displaying the final result."><span class="code-function">print</span></span>(<span class="code-tooltip" data-tip="PASTE(): Combining the label with the count."><span class="code-function">paste</span></span>(<span class="code-tooltip" data-tip="LABEL: A descriptive text for the output."><span class="code-string">"Total rows collected:"</span></span>, <span class="code-tooltip" data-tip="NROW(): Counts the number of rows in a data frame. This tells us how many total records we scraped across all pages."><span class="code-function">nrow</span></span>(<span class="code-tooltip" data-tip="ALL_DATA: Our combined data frame containing all scraped data.">all_data</span>)))</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="pagination" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">Page <span class="out-num">1</span>: <span class="out-num">20</span> items
Page <span class="out-num">2</span>: <span class="out-num">20</span> items
Page <span class="out-num">3</span>: <span class="out-num">20</span> items
Page <span class="out-num">4</span>: <span class="out-num">20</span> items
Page <span class="out-num">5</span>: <span class="out-num">20</span> items
Page <span class="out-num">6</span>: <span class="out-num">15</span> items
Page <span class="out-num">7</span>: <span class="out-num">0</span> items
No more data after page 6

<span class="out-green">Total: 115 items collected</span></div>
        </div>

        <div class="output-simulation" data-output="pagination" data-lang="r">
          <div class="output-header">
            <span>R Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">Scraping page 1...
Scraping page 2...
Scraping page 3...
...
[1] <span class="out-green">"Total rows collected: 115"</span></div>
        </div>

        <h3>Handling Dynamic Content (JavaScript)</h3>

        <p><strong>The problem:</strong> You run your scraper but get an empty result‚Äîor the HTML you receive doesn't contain the data you can clearly see on the page. What's happening?</p>

        <p><strong>Why this happens:</strong> Modern websites often load data <em>after</em> the initial page loads, using JavaScript. When you visit the page in a browser, JavaScript runs and fetches the data. But when your scraper fetches the page, it only gets the initial HTML‚Äî<em>before</em> JavaScript has run. The data simply isn't there yet.</p>

        <p><strong>How to diagnose this:</strong> In your browser, right-click and select "View Page Source" (not "Inspect"). If you can't find your data in the source but you can see it on the rendered page, JavaScript is loading it dynamically.</p>

        <div class="info-box warning">
          <div class="info-box-title">Solutions for JavaScript-Loaded Content</div>
          <p>You have three options, in order of preference:</p>
          <ol style="margin-bottom: 0;">
            <li><strong>Find the hidden API:</strong> Open Developer Tools ‚Üí Network tab ‚Üí reload the page ‚Üí look for XHR/Fetch requests. The data often comes from a JSON API that's much easier to scrape directly!</li>
            <li><strong>Use Selenium/Playwright:</strong> These tools control a real browser that executes JavaScript. More complex, but works for any site.</li>
            <li><strong>Check for a static version:</strong> Some sites offer non-JS versions for accessibility or older browsers.</li>
          </ol>
        </div>

        <div class="code-tabs" data-runnable="selenium">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Handle JavaScript with Selenium</span>
<span class="code-comment"># First: pip install selenium webdriver-manager</span>

<span class="code-tooltip" data-tip="FROM...IMPORT: Loading specific modules from the selenium package. Selenium is different from requests - it controls a real web browser!"><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="SELENIUM: A library that controls web browsers programmatically. Unlike requests which just fetches HTML, Selenium opens a real browser that can execute JavaScript, click buttons, fill forms, etc.">selenium</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="WEBDRIVER: The main Selenium module that provides browser control. It can drive Chrome, Firefox, Safari, and other browsers.">webdriver</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading the Service class for managing the browser driver."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="CHROME.SERVICE: Module for managing the ChromeDriver service that connects Selenium to Chrome.">selenium.webdriver.chrome.service</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="SERVICE: A class that manages the ChromeDriver process. It handles starting and stopping the browser driver.">Service</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading the By class for specifying how to find elements."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="COMMON.BY: Module containing constants for different ways to locate elements.">selenium.webdriver.common.by</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BY: A class with constants for locating elements - BY.CLASS_NAME, BY.ID, BY.CSS_SELECTOR, etc. Similar to BeautifulSoup's find methods but for Selenium.">By</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading WebDriverWait for waiting until elements appear."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="SUPPORT.UI: Module with utilities for waiting and user interaction.">selenium.webdriver.support.ui</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="WEBDRIVERWAIT: A class that waits for certain conditions before continuing. Essential for JavaScript-heavy pages where content loads after the initial page load.">WebDriverWait</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading expected_conditions for defining what to wait for."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="SUPPORT: Module with waiting utilities.">selenium.webdriver.support</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="EXPECTED_CONDITIONS: A collection of common conditions to wait for - element present, element clickable, text appears, etc. Aliased as 'EC' for brevity.">expected_conditions</span> <span class="code-tooltip" data-tip="AS KEYWORD: Creates an alias. Now we can write EC instead of expected_conditions."><span class="code-keyword">as</span></span> <span class="code-tooltip" data-tip="EC: Short alias for expected_conditions. Makes code cleaner and easier to type.">EC</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading webdriver-manager for automatic driver installation."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="WEBDRIVER_MANAGER: A helpful library that automatically downloads and manages the correct ChromeDriver version for your Chrome browser. Saves you from manual driver management!">webdriver_manager.chrome</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="CHROMEDRIVERMANAGER: Automatically finds and downloads the correct ChromeDriver version. Without this, you'd need to manually download the matching driver version.">ChromeDriverManager</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Also loading BeautifulSoup because we'll use it to parse the final HTML."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="BS4: BeautifulSoup library.">bs4</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP: After Selenium loads the page (with JavaScript executed), we can hand the HTML to BeautifulSoup for easier parsing.">BeautifulSoup</span>

<span class="code-comment"># Set up Chrome in headless mode (no visible window)</span>
<span class="code-tooltip" data-tip="OPTIONS: A configuration object for Chrome. We can set various browser preferences and command-line arguments.">options</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating a new options object.">=</span> <span class="code-tooltip" data-tip="WEBDRIVER.CHROMEOPTIONS(): Creates a Chrome-specific options object. You can configure things like headless mode, window size, user preferences, etc.">webdriver.ChromeOptions()</span>
<span class="code-tooltip" data-tip="OPTIONS.ADD_ARGUMENT(): Adds a command-line argument to Chrome. This is how you configure browser behavior.">options.add_argument</span>(<span class="code-tooltip" data-tip="HEADLESS FLAG: Runs Chrome without opening a visible window. Perfect for servers or automated scripts where you don't need to see the browser. The browser still works normally, just invisibly.">'--headless'</span>)
<span class="code-tooltip" data-tip="DRIVER: The WebDriver instance that controls the browser. This is your main interface for browser automation - you can load pages, click elements, fill forms, etc.">driver</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating and configuring the Chrome browser.">=</span> <span class="code-tooltip" data-tip="WEBDRIVER.CHROME(): Creates a new Chrome browser instance controlled by Selenium.">webdriver.Chrome</span>(
    <span class="code-tooltip" data-tip="SERVICE PARAMETER: Provides the ChromeDriver service. ChromeDriverManager().install() automatically downloads the correct driver version for your Chrome browser.">service=Service(ChromeDriverManager().install())</span>,
    <span class="code-tooltip" data-tip="OPTIONS PARAMETER: Passes our configuration (headless mode) to the browser.">options=options</span>
)

<span class="code-tooltip" data-tip="TRY BLOCK: Wraps code that might fail. This is important because if an error occurs, we still want to close the browser (in the finally block)."><span class="code-keyword">try</span></span>:
    <span class="code-comment"># Load the page</span>
    <span class="code-tooltip" data-tip="DRIVER.GET(): Tells the browser to navigate to a URL. Like typing an address in the URL bar and pressing Enter. The browser loads the page and executes any JavaScript.">driver.get</span>(<span class="code-tooltip" data-tip="URL: The page to visit. Selenium will load this page and wait for initial load, but dynamic content might still be loading via JavaScript.">"https://example.com/dynamic-data"</span>)

    <span class="code-comment"># Wait for specific element to load (max 10 seconds)</span>
    <span class="code-tooltip" data-tip="WAIT VARIABLE: Creates a wait object that will pause execution until certain conditions are met. The 10 is the maximum seconds to wait before giving up.">wait</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating the WebDriverWait object.">=</span> <span class="code-tooltip" data-tip="WEBDRIVERWAIT(): Creates a wait object. First argument is the driver, second is max wait time in seconds. If the condition isn't met within 10 seconds, it raises a timeout error.">WebDriverWait</span>(<span class="code-tooltip" data-tip="DRIVER: Our browser instance.">driver</span>, <span class="code-tooltip" data-tip="TIMEOUT (10): Maximum seconds to wait. Choose based on how long the page typically takes to load. Too short = errors; too long = wasted time."><span class="code-number">10</span></span>)
    <span class="code-tooltip" data-tip="WAIT.UNTIL(): Pauses execution until the specified condition is true OR timeout is reached. This is how we wait for JavaScript-loaded content to appear.">wait.until</span>(<span class="code-tooltip" data-tip="EC.PRESENCE_OF_ELEMENT_LOCATED(): A condition that checks if an element exists in the DOM (even if not visible). Once an element with class 'data-table' appears, the wait ends.">EC.presence_of_element_located</span>((<span class="code-tooltip" data-tip="BY.CLASS_NAME: Tells Selenium to look for an element by its CSS class name.">By.CLASS_NAME</span>, <span class="code-tooltip" data-tip="CLASS NAME STRING: The class we're waiting for. When JavaScript creates/loads the data-table element, our code continues.">"data-table"</span>)))

    <span class="code-comment"># Now page is fully loaded - get the HTML</span>
    <span class="code-tooltip" data-tip="HTML VARIABLE: Will contain the complete page HTML, including all content that JavaScript added after the initial load.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the page HTML.">=</span> <span class="code-tooltip" data-tip="DRIVER.PAGE_SOURCE: Gets the current HTML of the page. Unlike requests.get(), this HTML includes everything JavaScript has added/modified. This is the 'after JavaScript' version of the page.">driver.page_source</span>
    <span class="code-tooltip" data-tip="SOUP: Now we can use BeautifulSoup to parse this JavaScript-rendered HTML.">soup</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating the BeautifulSoup object.">=</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP(): Parsing the HTML for easy searching. From here, you can use all the BeautifulSoup methods we learned earlier.">BeautifulSoup</span>(<span class="code-tooltip" data-tip="HTML: The page source including JavaScript-loaded content.">html</span>, <span class="code-tooltip" data-tip="PARSER: Using Python's built-in parser.">'html.parser'</span>)

    <span class="code-comment"># Extract data as usual</span>
    <span class="code-tooltip" data-tip="TABLE: Finding the table element using BeautifulSoup. Now that JavaScript has loaded the content, it's in the HTML and we can find it.">table</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the found table.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND(): Standard BeautifulSoup search, same as before.">soup.find</span>(<span class="code-tooltip" data-tip="TABLE TAG: Finding a table element.">'table'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: With class 'data-table'.">class_=<span class="code-string">'data-table'</span></span>)
    <span class="code-tooltip" data-tip="SUCCESS MESSAGE: Confirming the extraction worked."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="STATUS: Letting you know the scraping succeeded.">"Data extracted successfully!"</span>)

<span class="code-tooltip" data-tip="FINALLY BLOCK: This code ALWAYS runs, whether try succeeded or raised an error. Perfect for cleanup tasks like closing the browser."><span class="code-keyword">finally</span></span>:
    <span class="code-comment"># Always close the browser</span>
    <span class="code-tooltip" data-tip="DRIVER.QUIT(): Closes the browser and ends the ChromeDriver process. CRITICAL! Without this, you'll have zombie Chrome processes running in the background, eating up memory. Always close your browsers!">driver.quit()</span></code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="selenium" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-blue">Starting Chrome in headless mode...</span>
<span class="out-blue">Navigating to page...</span>
<span class="out-blue">Waiting for data-table to load...</span>
<span class="out-green">Element found after 2.3 seconds</span>
<span class="out-green">Data extracted successfully!</span></div>
        </div>

        <!-- Section 6: Best Practices -->
        <div class="section-divider"></div>
        <h2 id="best-practices"><span class="section-num">2c.6</span> Best Practices for Research</h2>

        <p>You now know <em>how</em> to scrape. This section is about <strong>how to scrape responsibly</strong>‚Äîboth for ethical reasons and practical ones. A well-designed scraper is more likely to succeed and less likely to get you in trouble.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">For our CO2 emissions scraper, best practices mean: (1) adding a 1-2 second delay between requests if scraping multiple pages, (2) setting a User-Agent that identifies our scraper, (3) caching pages locally so we don't re-download during development, and (4) handling errors gracefully if Wikipedia is temporarily unavailable.</p>
        </div>

        <div class="checklist">
          <h4>The Gentleman-Scraper's Checklist</h4>
          <ul>
            <li><strong>Identify yourself:</strong> Use a clear User-Agent with contact info</li>
            <li><strong>Respect robots.txt:</strong> Don't access disallowed pages</li>
            <li><strong>Rate limit:</strong> Add delays (2+ seconds) between requests</li>
            <li><strong>Cache responses:</strong> Don't re-fetch pages you've already downloaded</li>
            <li><strong>Handle errors gracefully:</strong> Don't hammer a server if it returns errors</li>
            <li><strong>Scrape during off-hours:</strong> Minimize impact on the server</li>
            <li><strong>Only take what you need:</strong> Don't download entire websites</li>
            <li><strong>Store data securely:</strong> Especially if it contains personal information</li>
            <li><strong>Document your methodology:</strong> For reproducibility and ethics review</li>
            <li><strong>Consider reaching out:</strong> Website owners may provide data directly</li>
          </ul>
        </div>

        <h3>Rate Limiting and Caching</h3>

        <p>These two techniques solve different problems, but they're both essential for any serious scraping project.</p>

        <div class="concept-box">
          <h4>üö¶ Rate Limiting: Why You Need It</h4>
          <p><strong>The problem:</strong> Your scraper can request pages much faster than a human would‚Äîpotentially hundreds per second. This can overwhelm the server, slow down the website for other users, or trigger security measures that block you.</p>
          <p style="margin-bottom: 0;"><strong>The solution:</strong> Add a deliberate delay between requests. A 2-3 second delay is polite. Some sites specify a <code>Crawl-delay</code> in their robots.txt‚Äîalways respect it.</p>
        </div>

        <div class="concept-box">
          <h4>üíæ Caching: Why You Need It</h4>
          <p><strong>The problem:</strong> While developing your scraper, you'll run it many times‚Äîtesting, fixing bugs, adjusting selectors. Each run re-downloads pages you already have, wasting time and putting unnecessary load on the server.</p>
          <p><strong>The solution:</strong> Save each page's HTML to a local file. Before fetching a URL, check if you already have it cached. This makes development faster and reduces server load.</p>
          <p style="margin-bottom: 0;"><strong>Bonus benefit:</strong> If your scraper crashes halfway through, you don't lose progress‚Äîcached pages don't need to be re-downloaded.</p>
        </div>

        <div class="code-tabs" data-runnable="polite">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Polite scraping with rate limiting and caching</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the requests library."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: For fetching web pages.">requests</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the time module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="TIME: For adding delays and tracking request timing.">time</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the hashlib module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="HASHLIB: Provides hashing functions like MD5. We use it to create unique cache filenames from URLs.">hashlib</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the os module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="OS: Operating system utilities. Used for file/folder operations.">os</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading Path from pathlib."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="PATHLIB: A modern way to work with file paths in Python. More readable than os.path.">pathlib</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="PATH: A class representing filesystem paths. Makes working with directories and files much cleaner.">Path</span>

<span class="code-tooltip" data-tip="CLASS DEFINITION: Creating a new class called PoliteScraper. A class is like a blueprint - it bundles related data and functions together. This class handles rate limiting and caching automatically."><span class="code-keyword">class</span></span> <span class="code-tooltip" data-tip="POLITESCRAPER: Our custom scraper class name. The name reflects its purpose - being polite to servers by rate limiting and caching."><span class="code-function">PoliteScraper</span></span>:
    <span class="code-tooltip" data-tip="__INIT__ METHOD (Constructor): A special method that runs when you create a new PoliteScraper object. It sets up all the initial values the scraper needs."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="__INIT__: The initialization method. Double underscores indicate special Python methods."><span class="code-function">__init__</span></span>(<span class="code-tooltip" data-tip="SELF: A reference to the object being created. Required as the first parameter of all methods. Through 'self', methods can access and modify the object's data.">self</span>, <span class="code-tooltip" data-tip="DELAY PARAMETER: How many seconds to wait between requests. Default is 2. You can change this when creating a scraper.">delay=<span class="code-number">2</span></span>, <span class="code-tooltip" data-tip="CACHE_DIR PARAMETER: The folder name for storing cached pages. Default is 'scraper_cache'.">cache_dir=<span class="code-string">"scraper_cache"</span></span>):
        <span class="code-tooltip" data-tip="SELF.DELAY: Stores the delay value on the object. Using 'self.' makes it accessible to other methods in the class.">self.delay</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the delay value.">=</span> <span class="code-tooltip" data-tip="DELAY: The value passed to the constructor.">delay</span>
        <span class="code-tooltip" data-tip="SELF.CACHE_DIR: Stores the cache directory path. Path() converts the string to a Path object for easier file operations.">self.cache_dir</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating and storing the Path object.">=</span> <span class="code-tooltip" data-tip="PATH(): Creates a Path object from the directory name string.">Path(cache_dir)</span>
        <span class="code-tooltip" data-tip="MKDIR(): Creates the directory if it doesn't exist. exist_ok=True means it won't raise an error if the directory already exists.">self.cache_dir.mkdir</span>(<span class="code-tooltip" data-tip="EXIST_OK: If True, don't raise an error when the directory exists. Perfect for setup code that might run multiple times.">exist_ok=<span class="code-keyword">True</span></span>)
        <span class="code-tooltip" data-tip="SELF.LAST_REQUEST: Tracks when we last made a request (Unix timestamp). Starts at 0, meaning 'never'. Used for rate limiting.">self.last_request</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Initializing to 0.">=</span> <span class="code-tooltip" data-tip="ZERO: No requests made yet."><span class="code-number">0</span></span>
        <span class="code-tooltip" data-tip="SELF.SESSION: A requests Session object. Sessions persist settings like headers and cookies across multiple requests, which is more efficient and polite.">self.session</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating a new Session.">=</span> <span class="code-tooltip" data-tip="REQUESTS.SESSION(): Creates a session that maintains settings across requests. More efficient than creating new connections each time.">requests.Session()</span>
        <span class="code-tooltip" data-tip="SESSION.HEADERS.UPDATE(): Sets default headers for all requests made through this session. The User-Agent identifies our scraper.">self.session.headers.update</span>({
            <span class="code-tooltip" data-tip="USER-AGENT HEADER: Identifies our scraper to websites. Include your contact info so website owners can reach you.">'User-Agent'</span>: <span class="code-tooltip" data-tip="USER-AGENT VALUE: Our scraper identification string.">'Research Scraper (contact@university.edu)'</span>
        })

    <span class="code-tooltip" data-tip="METHOD DEFINITION: Creating a helper method (the underscore prefix _ suggests it's for internal use)."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="_GET_CACHE_PATH: A private helper method (underscore = internal). Converts a URL to a cache file path. Each URL gets a unique filename based on its hash."><span class="code-function">_get_cache_path</span></span>(<span class="code-tooltip" data-tip="SELF: Reference to the scraper object.">self</span>, <span class="code-tooltip" data-tip="URL PARAMETER: The URL we want to cache.">url</span>):
        <span class="code-comment"># Create unique filename from URL</span>
        <span class="code-tooltip" data-tip="URL_HASH: A unique 32-character string generated from the URL. Even slightly different URLs produce completely different hashes.">url_hash</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the hash.">=</span> <span class="code-tooltip" data-tip="HASHLIB.MD5().HEXDIGEST(): Creates an MD5 hash of the URL and converts it to a hexadecimal string. This creates a unique, filesystem-safe filename from any URL.">hashlib.md5(url.encode()).hexdigest()</span>
        <span class="code-tooltip" data-tip="RETURN: Sends a value back to the caller. The method returns the full path to the cache file."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="PATH CONCATENATION: Using / operator with Path objects to build file paths. Creates something like 'scraper_cache/a1b2c3d4.html'.">self.cache_dir / f"{url_hash}.html"</span>

    <span class="code-tooltip" data-tip="METHOD DEFINITION: The main fetch method that users will call."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="FETCH METHOD: The main function to get a webpage. Handles caching and rate limiting automatically."><span class="code-function">fetch</span></span>(<span class="code-tooltip" data-tip="SELF: Reference to the scraper.">self</span>, <span class="code-tooltip" data-tip="URL PARAMETER: The webpage to fetch.">url</span>, <span class="code-tooltip" data-tip="USE_CACHE PARAMETER: If True, use cached version if available. If False, always fetch fresh. Default is True for efficiency.">use_cache=<span class="code-keyword">True</span></span>):
        <span class="code-tooltip" data-tip="CACHE_PATH: Get the path where this URL's cache file would be stored.">cache_path</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the cache path.">=</span> <span class="code-tooltip" data-tip="CALLING _GET_CACHE_PATH: Using our helper method to generate the cache filename.">self._get_cache_path(url)</span>

        <span class="code-comment"># Check cache first</span>
        <span class="code-tooltip" data-tip="IF STATEMENT: Checking if we should use the cache."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="USE_CACHE: Only check cache if caching is enabled.">use_cache</span> <span class="code-tooltip" data-tip="AND: Both conditions must be true."><span class="code-keyword">and</span></span> <span class="code-tooltip" data-tip="CACHE_PATH.EXISTS(): Checks if the cache file exists on disk. If so, we can skip the network request!">cache_path.exists()</span>:
            <span class="code-tooltip" data-tip="PRINT CACHE HIT: Informing the user we're using cached data."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="CACHE MESSAGE: Shows which URL is being loaded from cache.">f"Using cached version of {url}"</span>)
            <span class="code-tooltip" data-tip="RETURN: Send the cached content back immediately, skipping the network request."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="CACHE_PATH.READ_TEXT(): Reads the entire cache file as a string. Fast and no network needed!">cache_path.read_text()</span>

        <span class="code-comment"># Rate limiting</span>
        <span class="code-tooltip" data-tip="ELAPSED: Calculate how long since our last request. time.time() returns the current time in seconds.">elapsed</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the elapsed time.">=</span> <span class="code-tooltip" data-tip="TIME DIFFERENCE: Current time minus last request time gives seconds elapsed.">time.time() - self.last_request</span>
        <span class="code-tooltip" data-tip="IF STATEMENT: Checking if we need to wait."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="RATE LIMIT CHECK: If less time has passed than our required delay, we need to wait.">elapsed < self.delay</span>:
            <span class="code-tooltip" data-tip="TIME.SLEEP(): Pause for just the right amount of time. If delay is 2 seconds and 0.5 seconds passed, sleep for 1.5 seconds. Smart rate limiting!">time.sleep</span>(<span class="code-tooltip" data-tip="WAIT TIME: Only wait the remaining time needed, not the full delay.">self.delay - elapsed</span>)

        <span class="code-comment"># Make request</span>
        <span class="code-tooltip" data-tip="RESPONSE: Fetching the page through our session (which includes our headers).">response</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the response.">=</span> <span class="code-tooltip" data-tip="SESSION.GET(): Making the request through our session, which automatically includes our User-Agent header.">self.session.get</span>(<span class="code-tooltip" data-tip="URL: The page to fetch.">url</span>, <span class="code-tooltip" data-tip="TIMEOUT: Maximum wait time for response.">timeout=<span class="code-number">30</span></span>)
        <span class="code-tooltip" data-tip="UPDATE LAST_REQUEST: Recording when we made this request for future rate limiting.">self.last_request</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the current timestamp.">=</span> <span class="code-tooltip" data-tip="TIME.TIME(): Current Unix timestamp.">time.time()</span>

        <span class="code-tooltip" data-tip="IF STATEMENT: Checking for success."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS CHECK: 200 means success.">response.status_code == <span class="code-number">200</span></span>:
            <span class="code-comment"># Cache the response</span>
            <span class="code-tooltip" data-tip="WRITE_TEXT(): Saves the HTML content to the cache file. Next time we request this URL, we'll use this cached version.">cache_path.write_text</span>(<span class="code-tooltip" data-tip="RESPONSE.TEXT: The HTML content to cache.">response.text</span>)
            <span class="code-tooltip" data-tip="RETURN: Send the HTML back to the caller."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="RESPONSE.TEXT: The fetched HTML content.">response.text</span>
        <span class="code-tooltip" data-tip="ELSE: Handle errors."><span class="code-keyword">else</span></span>:
            <span class="code-tooltip" data-tip="RAISE: Throws an error that stops execution. This alerts the caller that something went wrong."><span class="code-keyword">raise</span></span> <span class="code-tooltip" data-tip="EXCEPTION: Creates an error with a descriptive message."><span class="code-function">Exception</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Includes the status code for debugging.">f"HTTP {response.status_code}"</span>)

<span class="code-comment"># Usage</span>
<span class="code-tooltip" data-tip="SCRAPER INSTANCE: Creating a new PoliteScraper object with a 3-second delay between requests.">scraper</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing our scraper object.">=</span> <span class="code-tooltip" data-tip="POLITESCRAPER(): Creates a new scraper. We're customizing the delay to 3 seconds.">PoliteScraper</span>(<span class="code-tooltip" data-tip="DELAY ARGUMENT: Setting a 3-second minimum wait between requests.">delay=<span class="code-number">3</span></span>)
<span class="code-tooltip" data-tip="HTML VARIABLE: Will store the fetched/cached page content.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result.">=</span> <span class="code-tooltip" data-tip="SCRAPER.FETCH(): Fetching a page. First call will make a network request and cache it.">scraper.fetch</span>(<span class="code-tooltip" data-tip="URL: The first page to fetch.">"https://example.com/page1"</span>)
<span class="code-tooltip" data-tip="HTML: Fetching a second page. The scraper automatically waits 3 seconds between requests.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the second page.">=</span> <span class="code-tooltip" data-tip="SCRAPER.FETCH(): Fetching another page. Rate limiting ensures we wait before requesting.">scraper.fetch</span>(<span class="code-tooltip" data-tip="URL: The second page to fetch.">"https://example.com/page2"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="polite" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-blue">Fetching https://example.com/page1...</span>
<span class="out-green">Cached to scraper_cache/a1b2c3d4.html</span>

<span class="out-blue">Waiting 3 seconds...</span>

<span class="out-blue">Fetching https://example.com/page2...</span>
<span class="out-green">Cached to scraper_cache/e5f6g7h8.html</span>

<span class="out-str">Next run will use cached versions!</span></div>
        </div>

        <h3>Error Handling and Retries</h3>

        <p><strong>The problem:</strong> When scraping 200+ sites, things <em>will</em> go wrong. Servers will be temporarily unavailable, your internet connection will hiccup, some pages will return errors. If your scraper crashes at error #50, you lose all progress.</p>

        <p><strong>The solution:</strong> Build resilience into your scraper. When a request fails, wait and try again (this is called "retrying with exponential backoff"). Only give up after multiple failures.</p>

        <div class="info-box note">
          <div class="info-box-title">What is Exponential Backoff?</div>
          <p style="margin-bottom: 0;">Instead of retrying immediately (which might fail again), you wait progressively longer: 5 seconds, then 10 seconds, then 20 seconds. This gives the server time to recover and avoids hammering it when it's already struggling.</p>
        </div>

        <div class="code-tabs" data-runnable="errors">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Robust error handling</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the requests library."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: HTTP library for making web requests.">requests</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the time module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="TIME: For adding delays between retry attempts.">time</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading specific exception type."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="REQUESTS.EXCEPTIONS: Module containing all exception types that requests can raise.">requests.exceptions</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="REQUESTEXCEPTION: The base exception class for all requests errors - connection failures, timeouts, etc. Catching this handles all network-related errors.">RequestException</span>

<span class="code-tooltip" data-tip="FUNCTION DEFINITION: Creating a reusable function for fetching URLs with automatic retry logic."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="FETCH_WITH_RETRY: A function that tries multiple times to fetch a URL. If the first attempt fails, it waits and tries again. This makes your scraper resilient to temporary network issues."><span class="code-function">fetch_with_retry</span></span>(<span class="code-tooltip" data-tip="URL PARAMETER: The webpage to fetch.">url</span>, <span class="code-tooltip" data-tip="MAX_RETRIES: How many times to try before giving up. Default is 3 attempts total.">max_retries=<span class="code-number">3</span></span>, <span class="code-tooltip" data-tip="BASE_DELAY: Starting wait time in seconds. This gets multiplied for exponential backoff (5s, then 10s, then 20s...).">base_delay=<span class="code-number">5</span></span>):
    <span class="code-tooltip" data-tip="DOCSTRING: A documentation string explaining what this function does. Triple quotes allow multi-line strings. Good practice for documenting your code."><span class="code-string">"""Fetch URL with exponential backoff on failure."""</span></span>

    <span class="code-tooltip" data-tip="FOR LOOP: Try up to max_retries times. 'attempt' will be 0, 1, 2 (for 3 retries)."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="ATTEMPT VARIABLE: Tracks which attempt we're on (0-based). Used to calculate exponential backoff.">attempt</span> <span class="code-tooltip" data-tip="IN KEYWORD: Part of the for loop."><span class="code-keyword">in</span></span> <span class="code-tooltip" data-tip="RANGE(MAX_RETRIES): Creates sequence 0, 1, 2 for 3 retries. Each iteration is one attempt."><span class="code-function">range</span>(max_retries)</span>:
        <span class="code-tooltip" data-tip="TRY BLOCK: Wraps code that might raise an exception. If an error occurs, execution jumps to the except block instead of crashing."><span class="code-keyword">try</span></span>:
            <span class="code-tooltip" data-tip="RESPONSE: Making the actual request.">response</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the response.">=</span> <span class="code-tooltip" data-tip="REQUESTS.GET(): Fetching the URL with a 30-second timeout.">requests.get</span>(<span class="code-tooltip" data-tip="URL: The page to fetch.">url</span>, <span class="code-tooltip" data-tip="TIMEOUT: Maximum wait time before giving up on this request.">timeout=<span class="code-number">30</span></span>)

            <span class="code-comment"># Success</span>
            <span class="code-tooltip" data-tip="IF STATEMENT: Checking for success status."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS CODE 200: Success! Everything worked.">response.status_code == <span class="code-number">200</span></span>:
                <span class="code-tooltip" data-tip="RETURN: Exit the function immediately and send back the HTML. Success means we don't need to retry."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="RESPONSE.TEXT: The HTML content.">response.text</span>

            <span class="code-comment"># Rate limited - wait and retry</span>
            <span class="code-tooltip" data-tip="ELIF (else if): Another condition to check if the previous one was false."><span class="code-keyword">elif</span></span> <span class="code-tooltip" data-tip="STATUS CODE 429: 'Too Many Requests' - the server is telling us to slow down. This is rate limiting in action.">response.status_code == <span class="code-number">429</span></span>:
                <span class="code-tooltip" data-tip="WAIT_TIME: How long to wait. We check the Retry-After header first (the server tells us), or fall back to base_delay.">wait_time</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the wait time.">=</span> <span class="code-tooltip" data-tip="INT(): Converts to integer since the header might be a string.">int</span>(<span class="code-tooltip" data-tip="HEADERS.GET(): Safely gets a header value. Retry-After is a standard header telling you how long to wait. If missing, we use base_delay as default.">response.headers.get</span>(<span class="code-tooltip" data-tip="RETRY-AFTER HEADER: A standard HTTP header that tells you how many seconds to wait before trying again.">'Retry-After'</span>, <span class="code-tooltip" data-tip="DEFAULT VALUE: If the header doesn't exist, use base_delay instead.">base_delay</span>))
                <span class="code-tooltip" data-tip="PRINT STATUS: Informing the user about the rate limit."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="RATE LIMIT MESSAGE: Shows how long we'll wait.">f"Rate limited. Waiting {wait_time}s..."</span>)
                <span class="code-tooltip" data-tip="TIME.SLEEP(): Pausing for the specified duration before the next attempt.">time.sleep</span>(<span class="code-tooltip" data-tip="WAIT_TIME: The duration to pause.">wait_time</span>)

            <span class="code-comment"># Server error - wait and retry</span>
            <span class="code-tooltip" data-tip="ELIF: Checking for server errors."><span class="code-keyword">elif</span></span> <span class="code-tooltip" data-tip="STATUS >= 500: Any 5xx status code indicates a server error (500 Internal Error, 502 Bad Gateway, 503 Service Unavailable, etc.). These are usually temporary - worth retrying.">response.status_code >= <span class="code-number">500</span></span>:
                <span class="code-tooltip" data-tip="DELAY CALCULATION: Exponential backoff - each retry waits longer. First retry: 5s, second: 10s, third: 20s. This gives the server time to recover.">delay</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Calculating the delay.">=</span> <span class="code-tooltip" data-tip="EXPONENTIAL BACKOFF FORMULA: base_delay * (2 ^ attempt). When attempt=0: 5*1=5s. attempt=1: 5*2=10s. attempt=2: 5*4=20s. This prevents hammering a struggling server.">base_delay * (<span class="code-number">2</span> ** attempt)</span>
                <span class="code-tooltip" data-tip="PRINT STATUS: Informing about the server error."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Shows the wait time.">f"Server error. Retry in {delay}s..."</span>)
                <span class="code-tooltip" data-tip="TIME.SLEEP(): Waiting before retry.">time.sleep</span>(<span class="code-tooltip" data-tip="DELAY: The calculated exponential backoff time.">delay</span>)

            <span class="code-comment"># Client error - don't retry</span>
            <span class="code-tooltip" data-tip="ELSE: Handles other status codes (4xx client errors like 404 Not Found, 403 Forbidden). These usually won't be fixed by retrying - the problem is with our request."><span class="code-keyword">else</span></span>:
                <span class="code-tooltip" data-tip="RAISE: Immediately stop and report the error. Client errors (like 404) won't be fixed by retrying, so we give up."><span class="code-keyword">raise</span></span> <span class="code-tooltip" data-tip="EXCEPTION: Creating an error with the status code."><span class="code-function">Exception</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Includes the status code for debugging.">f"HTTP {response.status_code}"</span>)

        <span class="code-tooltip" data-tip="EXCEPT BLOCK: Catches network-level errors like connection refused, timeout, DNS failure. These are different from HTTP status codes - the request didn't even complete."><span class="code-keyword">except</span></span> <span class="code-tooltip" data-tip="REQUESTEXCEPTION: Catches all requests-related errors - connection failed, timed out, DNS error, etc.">RequestException</span> <span class="code-tooltip" data-tip="AS KEYWORD: Captures the exception object in variable 'e' so we can examine it."><span class="code-keyword">as</span></span> <span class="code-tooltip" data-tip="E: The exception object containing error details.">e</span>:
            <span class="code-tooltip" data-tip="PRINT ERROR: Showing what went wrong."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Shows the exception details.">f"Request failed: {e}"</span>)
            <span class="code-tooltip" data-tip="IF STATEMENT: Check if we have retries remaining."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="RETRIES REMAINING CHECK: If attempt is less than max_retries-1, we still have more tries. For 3 max_retries, we retry when attempt is 0 or 1.">attempt < max_retries - <span class="code-number">1</span></span>:
                <span class="code-tooltip" data-tip="DELAY: Calculate exponential backoff for this retry.">delay</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the delay.">=</span> <span class="code-tooltip" data-tip="EXPONENTIAL BACKOFF: Same formula as before.">base_delay * (<span class="code-number">2</span> ** attempt)</span>
                <span class="code-tooltip" data-tip="PRINT RETRY MESSAGE: Informing about the upcoming retry."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="RETRY MESSAGE: Shows when we'll try again.">f"Retrying in {delay}s..."</span>)
                <span class="code-tooltip" data-tip="TIME.SLEEP(): Wait before retrying.">time.sleep</span>(<span class="code-tooltip" data-tip="DELAY: The backoff duration.">delay</span>)

    <span class="code-tooltip" data-tip="RAISE AFTER LOOP: If we exit the for loop, all retries have been exhausted without success. Give up and report failure."><span class="code-keyword">raise</span></span> <span class="code-tooltip" data-tip="EXCEPTION: Final failure after all retries."><span class="code-function">Exception</span></span>(<span class="code-tooltip" data-tip="FAILURE MESSAGE: Reports how many attempts were made before giving up.">f"Failed after {max_retries} attempts"</span>)

<span class="code-comment"># Usage</span>
<span class="code-tooltip" data-tip="HTML VARIABLE: Will store the successfully fetched content.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result.">=</span> <span class="code-tooltip" data-tip="FETCH_WITH_RETRY(): Calling our robust fetching function. It will automatically retry on failures.">fetch_with_retry</span>(<span class="code-tooltip" data-tip="URL: The page to fetch. The function handles errors automatically.">"https://example.com/data"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="errors" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-str">Fetching https://example.com/data...</span>
<span class="out-yellow">Server error (503). Retry in 5s...</span>
<span class="out-str">Attempt 2...</span>
<span class="out-yellow">Server error (503). Retry in 10s...</span>
<span class="out-str">Attempt 3...</span>
<span class="out-green">Success!</span></div>
        </div>

        <div class="info-box tip">
          <div class="info-box-title">Save Your Work Incrementally</div>
          <p style="margin-bottom: 0;">When scraping large amounts of data, save to disk after each page. If your script crashes after 2 hours, you don't want to lose everything. Use caching (shown above) or append to a CSV/database as you go.</p>
        </div>

        <!-- Common Challenges Quick Reference -->
        <div class="section-divider"></div>
        <h3 style="color: #1e3a5f; font-size: 1.4rem; margin-top: 2rem;">üõ†Ô∏è Troubleshooting: Common Problems & Solutions</h3>

        <p>Here's a quick reference for problems you'll likely encounter. Bookmark this section!</p>

        <div style="display: grid; gap: 1rem; margin: 1.5rem 0;">
          <!-- Problem 1 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "My scraper returns empty results"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> JavaScript is loading the content. <strong>Fix:</strong> Check if View Page Source shows the data. If not, use Selenium or find the underlying API in the Network tab.</p>
          </div>

          <!-- Problem 2 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "I'm getting blocked (403 Forbidden)"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> You're hitting the server too fast, or missing a User-Agent header. <strong>Fix:</strong> Add longer delays (5+ seconds), set a proper User-Agent, and respect robots.txt.</p>
          </div>

          <!-- Problem 3 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "My selector worked yesterday but fails today"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> The website changed its HTML structure. <strong>Fix:</strong> Re-inspect the page and update your selectors. This is why scraping requires ongoing maintenance.</p>
          </div>

          <!-- Problem 4 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "I'm getting encoding errors (weird characters)"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> Character encoding mismatch. <strong>Fix:</strong> Check the page's encoding in the response headers or meta tag. Use <code>response.encoding = 'utf-8'</code> (Python) before accessing <code>response.text</code>.</p>
          </div>

          <!-- Problem 5 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "My scraper works but is painfully slow"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> You're re-downloading pages unnecessarily. <strong>Fix:</strong> Implement caching! Also consider if you really need every page‚Äîmaybe you can sample or prioritize.</p>
          </div>
        </div>

        <!-- Project Completion Summary -->
        <div class="research-banner" style="background: linear-gradient(135deg, #059669 0%, #10b981 100%); margin-top: 2rem;">
          <h3>‚úÖ What You've Learned: CO2 Emissions Scraper</h3>
          <p style="margin-bottom: 1rem;">You can now build a complete scraper for the Wikipedia CO2 emissions table‚Äîand merge it with your World Bank data for a richer climate analysis:</p>
          <ol style="margin-bottom: 0; padding-left: 1.5rem;">
            <li><strong>Legal check:</strong> Verified Wikipedia's CC license and robots.txt permit scraping</li>
            <li><strong>HTML inspection:</strong> Used browser DevTools to find the <code>table.wikitable</code> selector</li>
            <li><strong>Data extraction:</strong> Fetched the page and parsed the table with BeautifulSoup/rvest</li>
            <li><strong>Error handling:</strong> Built resilience with retries and exponential backoff</li>
            <li><strong>Best practices:</strong> Rate limiting, caching, and respectful User-Agent headers</li>
          </ol>
          <p style="margin-top: 1rem; margin-bottom: 0; font-size: 0.95rem; opacity: 0.9;"><strong>Next steps:</strong> In <a href="03-data-exploration.html" style="color: #fbd38d;">Module 3: Data Exploration</a>, we'll continue with our Climate Vulnerability project, exploring the combined World Bank and scraped datasets. Try extending this scraper to also grab <a href="https://en.wikipedia.org/wiki/List_of_countries_by_greenhouse_gas_emissions" style="color: #fbd38d;" target="_blank">greenhouse gas emissions</a> data!</p>
        </div>

        <div class="nav-footer">
          <a href="02b-apis.html" class="nav-link prev">Working with APIs (02b)</a>
          <a href="03-data-exploration.html" class="nav-link next">Module 3: Data Exploration</a>
        </div>
      </div>
    </main>
  </div>

  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Questions about web scraping? I can help with HTML parsing, legal considerations, or troubleshooting your scraper.</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <script>
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.run-btn').forEach(btn => {
      btn.addEventListener('click', function() {
        const lang = this.dataset.lang;
        const codeBlock = this.closest('.code-tabs');
        const outputId = codeBlock.dataset.runnable;
        document.querySelectorAll(`.output-simulation[data-output="${outputId}"]`).forEach(out => {
          out.classList.remove('visible');
        });
        const output = document.querySelector(`.output-simulation[data-output="${outputId}"][data-lang="${lang}"]`);
        if (output) {
          output.classList.add('visible');
          output.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
        }
      });
    });

    document.querySelectorAll('.close-output').forEach(btn => {
      btn.addEventListener('click', function() {
        this.closest('.output-simulation').classList.remove('visible');
      });
    });

    document.querySelectorAll('.code-tabs .tab-button').forEach(btn => {
      btn.addEventListener('click', function() {
        const codeBlock = this.closest('.code-tabs');
        const outputId = codeBlock.dataset.runnable;
        if (outputId) {
          document.querySelectorAll(`.output-simulation[data-output="${outputId}"]`).forEach(out => {
            out.classList.remove('visible');
          });
        }
      });
    });

    // Accordion functionality for collapsible sections
    document.querySelectorAll('.accordion-header').forEach(header => {
      header.addEventListener('click', function() {
        const section = this.closest('.accordion-section');
        section.classList.toggle('open');
      });
    });
  });
  </script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    // Create a single tooltip element that will be reused
    let tooltipEl = null;
    let currentTarget = null;
    let hideTimeout = null;

    function createTooltip() {
      if (tooltipEl) return tooltipEl;
      tooltipEl = document.createElement('div');
      tooltipEl.className = 'tooltip-popup';
      document.body.appendChild(tooltipEl);
      return tooltipEl;
    }

    function positionTooltip(target) {
      const tooltip = createTooltip();
      const tipText = target.getAttribute('data-tip');
      if (!tipText) return;

      tooltip.textContent = tipText;
      tooltip.className = 'tooltip-popup'; // Reset classes

      // Get target position
      const targetRect = target.getBoundingClientRect();

      // Find the code block container (pre or .tab-content or .code-tabs)
      let container = target.closest('pre') || target.closest('.tab-content') || target.closest('.code-tabs');
      let containerRect = container ? container.getBoundingClientRect() : {
        left: 0, right: window.innerWidth, top: 0, bottom: window.innerHeight
      };

      // Use viewport as fallback boundary
      const viewportWidth = window.innerWidth;
      const viewportHeight = window.innerHeight;
      const padding = 10; // Minimum distance from edges

      // Temporarily show to measure
      tooltip.style.visibility = 'hidden';
      tooltip.style.display = 'block';
      tooltip.classList.add('visible');

      const tooltipRect = tooltip.getBoundingClientRect();
      const tooltipWidth = tooltipRect.width;
      const tooltipHeight = tooltipRect.height;

      // Calculate ideal position (above the element, centered)
      let left = targetRect.left + (targetRect.width / 2) - (tooltipWidth / 2);
      let top = targetRect.top - tooltipHeight - 8; // 8px gap
      let arrowClass = 'arrow-bottom';

      // Check if tooltip would go above viewport - if so, show below
      if (top < padding) {
        top = targetRect.bottom + 8;
        arrowClass = 'arrow-top';
      }

      // Check if tooltip would go below viewport when shown below
      if (top + tooltipHeight > viewportHeight - padding) {
        top = targetRect.top - tooltipHeight - 8;
        arrowClass = 'arrow-bottom';
      }

      // Horizontal boundary checks - keep within viewport
      if (left < padding) {
        left = padding;
      }
      if (left + tooltipWidth > viewportWidth - padding) {
        left = viewportWidth - tooltipWidth - padding;
      }

      // Additional check: keep within code container horizontally if possible
      if (container) {
        const minLeft = Math.max(padding, containerRect.left);
        const maxRight = Math.min(viewportWidth - padding, containerRect.right);

        if (left < minLeft) {
          left = minLeft;
        }
        if (left + tooltipWidth > maxRight) {
          left = maxRight - tooltipWidth;
        }
      }

      // Apply position
      tooltip.style.left = left + 'px';
      tooltip.style.top = top + 'px';
      tooltip.style.visibility = 'visible';
      tooltip.classList.add(arrowClass);
    }

    function showTooltip(target) {
      if (hideTimeout) {
        clearTimeout(hideTimeout);
        hideTimeout = null;
      }
      currentTarget = target;
      positionTooltip(target);
    }

    function hideTooltip() {
      hideTimeout = setTimeout(function() {
        if (tooltipEl) {
          tooltipEl.classList.remove('visible');
        }
        currentTarget = null;
      }, 100);
    }

    // Event delegation for all tooltips
    document.addEventListener('mouseenter', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) {
        showTooltip(e.target);
      }
    }, true);

    document.addEventListener('mouseleave', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) {
        hideTooltip();
      }
    }, true);

    // Handle scroll - reposition if visible
    document.addEventListener('scroll', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) {
        positionTooltip(currentTarget);
      }
    }, true);

    // Handle window resize
    window.addEventListener('resize', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) {
        positionTooltip(currentTarget);
      }
    });
  })();
  </script>
</body>
</html>
