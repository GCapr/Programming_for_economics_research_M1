<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>11C Neural Networks | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content { -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; }
    .protected-content pre, .protected-content code, .protected-content .code-block, .protected-content .code-tabs { -webkit-user-select: text; -moz-user-select: text; -ms-user-select: text; user-select: text; }
    .code-tooltip { position: relative; cursor: help; border-bottom: 1px dotted #888; text-decoration: none; }
    .tooltip-popup { position: fixed; background: #1f2937; color: white; padding: 0.5rem 0.75rem; border-radius: 6px; font-size: 0.75rem; white-space: normal; max-width: 300px; opacity: 0; pointer-events: none; transition: opacity 0.15s ease-in-out; z-index: 10000; }
    .tooltip-popup.visible { opacity: 1; }
    .distinction-box { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0; }
    @media (max-width: 768px) { .distinction-box { grid-template-columns: 1fr; } }
    .distinction-card { background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem; }
    .distinction-card h4 { margin: 0 0 0.5rem 0; color: #2563eb; }
    .distinction-card ul { margin: 0; padding-left: 1.25rem; }
  </style>
</head>
<body>
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <div class="course-description">
        <h3>Course Modules</h3>
        <ul class="module-list">
          <li><strong>Module 0:</strong> Languages & Platforms ‚Äî Python, Stata, R setup; IDEs (RStudio, VS Code, Jupyter)</li>
          <li><strong>Module 1:</strong> Getting Started ‚Äî Installation, basic syntax, packages</li>
          <li><strong>Module 2:</strong> Data Harnessing ‚Äî File import, APIs, web scraping</li>
          <li><strong>Module 3:</strong> Data Exploration ‚Äî Inspection, summary statistics, visualization</li>
          <li><strong>Module 4:</strong> Data Cleaning ‚Äî Data quality, transformation, validation</li>
          <li><strong>Module 5:</strong> Data Analysis ‚Äî Statistical analysis, simulation, experimental design</li>
          <li><strong>Module 6:</strong> Causal Inference ‚Äî Matching, DiD, RDD, IV, Synthetic Control</li>
          <li><strong>Module 7:</strong> Estimation Methods ‚Äî Standard errors, panel data, MLE/GMM</li>
          <li><strong>Module 8:</strong> Replicability ‚Äî Project organization, documentation, replication packages</li>
          <li><strong>Module 9:</strong> Git & GitHub ‚Äî Version control, collaboration, branching</li>
          <li><strong>Module 10:</strong> History of NLP ‚Äî From ELIZA to Transformers</li>
          <li><strong>Module 11:</strong> Machine Learning ‚Äî Prediction, regularization, neural networks</li>
          <li><strong>Module 12:</strong> Large Language Models ‚Äî How LLMs work, prompting, APIs</li>
        </ul>
      </div>
      <div class="access-note">
        This course is currently open to <strong>students at Sciences Po</strong>. If you are not a Sciences Po student but would like access, please <a href="mailto:giulia.caprini@sciencespo.fr">email me</a> to request an invite token.
      </div>
      <div class="password-form">
        <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
        <button id="password-submit">Access Course</button>
        <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
      </div>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li><a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a></li>
          <li class="has-subnav active">
            <a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a>
            <ul class="sub-nav">
              <li><a href="11a-regularization.html">Regularization</a></li>
              <li><a href="11b-trees.html">Tree-Based Methods</a></li>
              <li class="active"><a href="11c-neural-networks.html">Neural Networks</a></li>
              <li><a href="11d-causal-ml.html">Causal ML</a></li>
              <li><a href="11e-model-evaluation.html">Model Evaluation</a></li>
            </ul>
          </li>
          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content-wrapper">
        <div class="breadcrumb">
          <a href="11-machine-learning.html">11 Machine Learning</a> &raquo; Neural Networks
        </div>

        <h1>11C &nbsp;Neural Networks</h1>
        <div class="module-meta">
          <span>~2.5 hours</span>
          <span>Architecture, Training, Regularization, Implementation</span>
        </div>

        <!-- Learning Objectives -->
        <div class="info-box">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Understand how neural networks extend linear models with nonlinear activation functions</li>
            <li>Build a feedforward neural network from scratch in Python (PyTorch)</li>
            <li>Train networks using backpropagation and gradient descent</li>
            <li>Apply regularization techniques (dropout, early stopping) to prevent overfitting</li>
            <li>Know when neural networks are (and aren't) the right choice for economics research</li>
          </ul>
        </div>

        <!-- Table of Contents -->
        <div class="toc">
          <h3>Contents</h3>
          <ul>
            <li><a href="#intuition">From Linear Models to Neural Networks</a></li>
            <li><a href="#architecture">Network Architecture</a></li>
            <li><a href="#training">Training Neural Networks</a></li>
            <li><a href="#regularization-nn">Regularization in Neural Networks</a></li>
            <li><a href="#practical">Practical Implementation</a></li>
            <li><a href="#economics">When to Use Neural Networks in Economics</a></li>
            <li><a href="#limitations">Limitations</a></li>
          </ul>
        </div>

        <!-- ===================== FROM LINEAR MODELS TO NEURAL NETWORKS ===================== -->
        <h2 id="intuition">From Linear Models to Neural Networks</h2>

        <p>You already know linear regression: the predicted outcome is a weighted sum of the features, <strong>y = X&beta;</strong>. This is powerful and interpretable, but it can only learn linear relationships. If the true relationship between your features and the outcome is nonlinear (for example, the effect of education on earnings is different at different levels of education), a linear model will miss it unless you manually engineer the right transformations ‚Äî adding squared terms, interactions, polynomials, and so on.</p>

        <p>A neural network automates this process. The key idea is to stack multiple linear transformations with <span class="code-tooltip" data-tip="A function applied element-wise to the output of a linear transformation. Without activation functions, stacking linear layers would just produce another linear model. The activation function introduces nonlinearity, allowing the network to learn complex patterns.">nonlinear activation functions</span> between them. In the simplest terms: take your features X, multiply by a weight matrix W1, add a bias b1, then apply a nonlinear function (like ReLU, which just replaces negative values with zero). The result is a new set of "features" ‚Äî called a <span class="code-tooltip" data-tip="A set of neurons that are not directly observed in the data. Each hidden layer transforms its inputs into a new representation. The 'hidden' refers to the fact that these values are internal to the model, not part of the input or output.">hidden layer</span>. Then take those hidden features, multiply by another weight matrix W2, add another bias b2, and you get the output. By chaining together several of these layers, the network can learn arbitrarily complex nonlinear functions of the input features.</p>

        <p>This leads to one of the most remarkable results in mathematics: the <span class="code-tooltip" data-tip="A theorem proving that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function to arbitrary accuracy, given enough neurons. This does not mean it is easy to find the right weights ‚Äî only that the architecture is flexible enough in principle."><strong>Universal Approximation Theorem</strong></span>. It states that a neural network with just one hidden layer and enough neurons can approximate any continuous function to any desired degree of accuracy. In other words, neural networks are flexible enough to learn virtually any pattern in the data ‚Äî if you have enough data and computational resources. The practical challenge is finding the right weights, which is where training algorithms come in.</p>

        <p>Think of it this way: a linear model draws a single straight line (or plane) through your data. A neural network draws a wiggly, curved surface that can follow the contours of the data as closely as needed. Each hidden layer adds another level of abstraction ‚Äî the first layer might learn simple features (like "is this value high or low?"), the second layer might combine those into more complex features (like "is this a pattern of high income AND low education?"), and so on. This hierarchical feature learning is what makes neural networks so powerful for complex data.</p>

        <!-- Perceptron Diagram -->
        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 12px; padding: 2rem; margin: 1.5rem 0; text-align: center;">
          <h4 style="margin: 0 0 1.5rem 0; color: #1e293b;">A Single Neuron (Perceptron)</h4>
          <svg viewBox="0 0 500 200" style="max-width: 500px; margin: 0 auto;">
            <!-- Inputs -->
            <circle cx="60" cy="40" r="20" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
            <text x="60" y="45" text-anchor="middle" font-size="14" fill="#1e40af">x&#x2081;</text>
            <circle cx="60" cy="100" r="20" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
            <text x="60" y="105" text-anchor="middle" font-size="14" fill="#1e40af">x&#x2082;</text>
            <circle cx="60" cy="160" r="20" fill="#dbeafe" stroke="#3b82f6" stroke-width="2"/>
            <text x="60" y="165" text-anchor="middle" font-size="14" fill="#1e40af">x&#x2083;</text>
            <!-- Arrows to sum -->
            <line x1="80" y1="40" x2="195" y2="95" stroke="#94a3b8" stroke-width="1.5"/>
            <line x1="80" y1="100" x2="195" y2="100" stroke="#94a3b8" stroke-width="1.5"/>
            <line x1="80" y1="160" x2="195" y2="105" stroke="#94a3b8" stroke-width="1.5"/>
            <!-- Weight labels -->
            <text x="135" y="58" font-size="11" fill="#64748b">w&#x2081;</text>
            <text x="135" y="93" font-size="11" fill="#64748b">w&#x2082;</text>
            <text x="135" y="143" font-size="11" fill="#64748b">w&#x2083;</text>
            <!-- Sum node -->
            <circle cx="215" cy="100" r="25" fill="#fef3c7" stroke="#d97706" stroke-width="2"/>
            <text x="215" y="105" text-anchor="middle" font-size="16" fill="#92400e">&#x3A3;</text>
            <!-- Arrow to activation -->
            <line x1="240" y1="100" x2="295" y2="100" stroke="#94a3b8" stroke-width="1.5" marker-end="url(#arrow)"/>
            <!-- Activation -->
            <rect x="300" y="75" width="70" height="50" rx="8" fill="#dcfce7" stroke="#16a34a" stroke-width="2"/>
            <text x="335" y="105" text-anchor="middle" font-size="12" fill="#15803d">&#x3C3;(z)</text>
            <!-- Arrow to output -->
            <line x1="370" y1="100" x2="415" y2="100" stroke="#94a3b8" stroke-width="1.5" marker-end="url(#arrow)"/>
            <!-- Output -->
            <circle cx="440" cy="100" r="20" fill="#fce7f3" stroke="#db2777" stroke-width="2"/>
            <text x="440" y="105" text-anchor="middle" font-size="14" fill="#9d174d">&#x177;</text>
            <defs><marker id="arrow" viewBox="0 0 10 10" refX="5" refY="5" markerWidth="6" markerHeight="6" orient="auto-start-auto"><path d="M 0 0 L 10 5 L 0 10 z" fill="#94a3b8"/></marker></defs>
          </svg>
          <p style="margin: 1rem 0 0; font-size: 0.85rem; color: #475569;">Output = &#x3C3;(w&#x2081;x&#x2081; + w&#x2082;x&#x2082; + w&#x2083;x&#x2083; + b), where &#x3C3; is a nonlinear activation function</p>
        </div>


        <!-- ===================== NETWORK ARCHITECTURE ===================== -->
        <h2 id="architecture">Network Architecture</h2>

        <p>A <span class="code-tooltip" data-tip="A neural network where information flows in one direction only ‚Äî from input to output, with no cycles or feedback loops. This is the simplest and most common architecture.">feedforward neural network</span> is organized into layers. The <strong>input layer</strong> receives the features (one neuron per feature). One or more <strong>hidden layers</strong> perform the transformations. The <strong>output layer</strong> produces the final prediction. Each neuron in a layer receives inputs from every neuron in the previous layer (this is why these are also called "fully connected" or "dense" layers), applies a linear transformation (weighted sum + bias), and passes the result through an activation function.</p>

        <p>The choice of <strong>activation function</strong> is crucial. Without it, stacking layers would be pointless ‚Äî a composition of linear functions is just another linear function. The activation function introduces the nonlinearity that gives neural networks their power. The most common choices are:</p>

        <!-- Activation Function Comparison Table -->
        <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; font-size: 0.9rem;">
          <thead>
            <tr style="background: #f1f5f9; border-bottom: 2px solid #cbd5e1;">
              <th style="padding: 0.75rem; text-align: left;">Function</th>
              <th style="padding: 0.75rem; text-align: left;">Formula</th>
              <th style="padding: 0.75rem; text-align: left;">Range</th>
              <th style="padding: 0.75rem; text-align: left;">When to Use</th>
            </tr>
          </thead>
          <tbody>
            <tr style="border-bottom: 1px solid #e2e8f0;">
              <td style="padding: 0.75rem;"><strong>ReLU</strong></td>
              <td style="padding: 0.75rem;"><code>max(0, z)</code></td>
              <td style="padding: 0.75rem;">[0, +inf)</td>
              <td style="padding: 0.75rem;">Default for hidden layers. Fast, simple, works well in practice.</td>
            </tr>
            <tr style="border-bottom: 1px solid #e2e8f0;">
              <td style="padding: 0.75rem;"><strong>Sigmoid</strong></td>
              <td style="padding: 0.75rem;"><code>1 / (1 + e^(-z))</code></td>
              <td style="padding: 0.75rem;">(0, 1)</td>
              <td style="padding: 0.75rem;">Output layer for binary classification (probability output).</td>
            </tr>
            <tr style="border-bottom: 1px solid #e2e8f0;">
              <td style="padding: 0.75rem;"><strong>Tanh</strong></td>
              <td style="padding: 0.75rem;"><code>(e^z - e^(-z)) / (e^z + e^(-z))</code></td>
              <td style="padding: 0.75rem;">(-1, 1)</td>
              <td style="padding: 0.75rem;">Hidden layers when centered output is desired. Often works better than sigmoid.</td>
            </tr>
            <tr>
              <td style="padding: 0.75rem;"><strong>Linear (none)</strong></td>
              <td style="padding: 0.75rem;"><code>z</code></td>
              <td style="padding: 0.75rem;">(-inf, +inf)</td>
              <td style="padding: 0.75rem;">Output layer for regression (continuous prediction).</td>
            </tr>
          </tbody>
        </table>

        <p>Two key design choices are the <strong>width</strong> (number of neurons per layer) and the <strong>depth</strong> (number of layers). Wider layers can learn more complex patterns within a single transformation, while deeper networks can learn hierarchical representations ‚Äî simple patterns in early layers composed into complex patterns in later layers. In practice, modern networks tend to favor depth over width. A common starting point for tabular economic data is 2-3 hidden layers with 64-256 neurons each.</p>

        <p>A subtle but important point: adding more layers and neurons always increases the model's <em>capacity</em> (the set of functions it can represent), but it also increases the risk of overfitting. This is why regularization (Section 4) is essential when training neural networks.</p>

        <!-- Code Block: Define Network Architecture -->
        <div class="code-tabs" data-runnable="ml-nn-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn

<span class="code-comment"># Define a feedforward neural network with PyTorch</span>
<span class="code-comment"># Architecture: 8 inputs -> 128 -> 64 -> 32 -> 1 output</span>

model = nn.<span class="code-function">Sequential</span>(
    <span class="code-comment"># First hidden layer</span>
    nn.<span class="code-function">Linear</span>(<span class="code-tooltip" data-tip="The number of input features. Must match the number of columns in your feature matrix X.">8</span>, <span class="code-tooltip" data-tip="Number of neurons in this hidden layer. 128 neurons means 128 different linear combinations of the inputs, each followed by an activation function.">128</span>),
    nn.<span class="code-function">ReLU</span>(),         <span class="code-comment"># Activation: max(0, z)</span>

    <span class="code-comment"># Second hidden layer</span>
    nn.<span class="code-function">Linear</span>(128, 64),
    nn.<span class="code-function">ReLU</span>(),

    <span class="code-comment"># Third hidden layer</span>
    nn.<span class="code-function">Linear</span>(64, 32),
    nn.<span class="code-function">ReLU</span>(),

    <span class="code-comment"># Output layer (linear activation for regression)</span>
    nn.<span class="code-function">Linear</span>(<span class="code-tooltip" data-tip="Input size must match the output of the previous layer (32 neurons).">32</span>, <span class="code-tooltip" data-tip="Single output neuron for regression. For K-class classification, use K neurons with softmax.">1</span>)
)

<span class="code-comment"># Print the architecture summary</span>
<span class="code-function">print</span>(model)

<span class="code-comment"># Count total trainable parameters</span>
total_params = <span class="code-function">sum</span>(p.<span class="code-function">numel</span>() <span class="code-keyword">for</span> p <span class="code-keyword">in</span> model.<span class="code-function">parameters</span>() <span class="code-keyword">if</span> p.requires_grad)
<span class="code-function">print</span>(<span class="code-string">f"\nTotal trainable parameters: {total_params:,}"</span>)

<span class="code-comment"># Breakdown: each Linear(in, out) has in*out weights + out biases</span>
<span class="code-keyword">for</span> name, param <span class="code-keyword">in</span> model.<span class="code-function">named_parameters</span>():
    <span class="code-function">print</span>(<span class="code-string">f"  {name:15s}  shape: {str(list(param.shape)):15s}  params: {param.numel():,}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Neural network definition via Python block</span>
<span class="code-comment">* Stata does not have native neural network support.</span>
<span class="code-comment">* Use the python: block to define and train models.</span>

<span class="code-keyword">python:</span>
<span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn

<span class="code-comment"># Define a simple feedforward network</span>
<span class="code-comment"># For Stata's auto dataset: 3 features -> predict price</span>
model = nn.Sequential(
    nn.Linear(3, 64),   <span class="code-comment"># 3 features (mpg, weight, length)</span>
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 1)    <span class="code-comment"># 1 continuous output (price)</span>
)

print(model)
total = sum(p.numel() for p in model.parameters())
print(f<span class="code-string">"\nTotal parameters: {total:,}"</span>)
<span class="code-keyword">end</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Neural network definition with torch (R interface to LibTorch)</span>
<span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="The R interface to PyTorch's C++ backend (LibTorch). Provides tensor operations and neural network building blocks.">torch</span>)

<span class="code-comment"># Define network architecture using nn_sequential</span>
model &lt;- <span class="code-function">nn_sequential</span>(
  <span class="code-comment"># First hidden layer: 13 inputs (Boston Housing features)</span>
  <span class="code-function">nn_linear</span>(<span class="code-tooltip" data-tip="Number of input features. Boston Housing has 13 predictor columns.">13</span>, 128),
  <span class="code-function">nn_relu</span>(),

  <span class="code-comment"># Second hidden layer</span>
  <span class="code-function">nn_linear</span>(128, 64),
  <span class="code-function">nn_relu</span>(),

  <span class="code-comment"># Third hidden layer</span>
  <span class="code-function">nn_linear</span>(64, 32),
  <span class="code-function">nn_relu</span>(),

  <span class="code-comment"># Output layer (regression)</span>
  <span class="code-function">nn_linear</span>(32, 1)
)

<span class="code-comment"># Print architecture</span>
<span class="code-function">print</span>(model)

<span class="code-comment"># Count parameters</span>
n_params &lt;- <span class="code-function">sum</span>(<span class="code-function">sapply</span>(model$parameters, function(p) p$<span class="code-function">numel</span>()))
<span class="code-function">cat</span>(<span class="code-string">"Total trainable parameters:"</span>, <span class="code-function">format</span>(n_params, big.mark = <span class="code-string">","</span>), <span class="code-string">"\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-nn-1 -->
        <div class="output-simulation" data-output="ml-nn-1" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Sequential(
  (0): Linear(in_features=8, out_features=128, bias=True)
  (1): ReLU()
  (2): Linear(in_features=128, out_features=64, bias=True)
  (3): ReLU()
  (4): Linear(in_features=64, out_features=32, bias=True)
  (5): ReLU()
  (6): Linear(in_features=32, out_features=1, bias=True)
)

Total trainable parameters: 11,425
  0.weight          shape: [128, 8]        params: 1,024
  0.bias            shape: [128]           params: 128
  2.weight          shape: [64, 128]       params: 8,192
  2.bias            shape: [64]            params: 64
  4.weight          shape: [32, 64]        params: 2,048
  4.bias            shape: [32]            params: 32
  6.weight          shape: [1, 32]         params: 32
  6.bias            shape: [1]             params: 1</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-1" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Sequential(
  (0): Linear(in_features=3, out_features=64, bias=True)
  (1): ReLU()
  (2): Linear(in_features=64, out_features=32, bias=True)
  (3): ReLU()
  (4): Linear(in_features=32, out_features=1, bias=True)
)

Total parameters: 2,369</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-1" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>An `nn_sequential` module with 7 children:
 (0): nn_linear (13 -> 128)
 (1): nn_relu
 (2): nn_linear (128 -> 64)
 (3): nn_relu
 (4): nn_linear (64 -> 32)
 (5): nn_relu
 (6): nn_linear (32 -> 1)

Total trainable parameters: 12,609</pre></div>
        </div>


        <!-- ===================== TRAINING NEURAL NETWORKS ===================== -->
        <h2 id="training">Training Neural Networks</h2>

        <p>Training a neural network means finding the values of all the weights and biases that minimize a <span class="code-tooltip" data-tip="A function that measures how far the model's predictions are from the true values. For regression, the most common loss is Mean Squared Error (MSE). For classification, it is cross-entropy loss."><strong>loss function</strong></span>. For regression, the standard loss is the mean squared error (MSE): the average of the squared differences between predicted and actual values. For binary classification, we use <span class="code-tooltip" data-tip="Also called log loss. Cross-entropy measures how well the predicted probabilities match the true binary labels. It heavily penalizes confident wrong predictions.">binary cross-entropy loss</span>. The choice of loss function is the same as choosing a likelihood in statistics ‚Äî it defines what "good fit" means.</p>

        <p>The workhorse algorithm for training is <span class="code-tooltip" data-tip="An algorithm that computes the gradient of the loss function with respect to every weight in the network by applying the chain rule of calculus layer by layer, starting from the output and working backward. It is computationally efficient because it reuses intermediate calculations."><strong>backpropagation</strong></span>, which is simply the chain rule of calculus applied systematically through the network. Here is the intuition: after making a prediction, we compute the loss. Then we ask, "How would changing each weight slightly change the loss?" This is the gradient ‚Äî the direction of steepest increase in the loss. We then update each weight in the opposite direction (downhill) to reduce the loss. Backpropagation computes all these gradients efficiently by working backward through the layers, reusing calculations from later layers to speed up the computation for earlier ones.</p>

        <p>Once we have the gradients, we use <span class="code-tooltip" data-tip="An optimization algorithm that iteratively updates the weights by moving in the direction opposite to the gradient. Stochastic Gradient Descent (SGD) computes the gradient on a random subset (mini-batch) of the data, which is faster than using the entire dataset."><strong>gradient descent</strong></span> to update the weights. In practice, we almost always use <strong>Stochastic Gradient Descent (SGD)</strong> or one of its variants. Instead of computing the gradient on the entire dataset (which is slow), SGD computes the gradient on a small random subset called a <span class="code-tooltip" data-tip="A small random subset of the training data (typically 32, 64, or 128 observations) used to compute the gradient at each step. Using mini-batches is much faster than full-batch gradient descent and introduces beneficial noise that helps escape local minima.">mini-batch</span> (typically 32 to 256 observations). This is noisy but fast, and the noise actually helps the model escape shallow local minima and find better solutions.</p>

        <p>The most popular gradient descent variant today is <span class="code-tooltip" data-tip="Adaptive Moment Estimation: an optimizer that maintains separate learning rates for each weight, adapting based on the history of gradients. Combines the benefits of momentum (accumulating past gradients) and RMSprop (scaling by recent gradient magnitudes). Usually converges faster than plain SGD."><strong>Adam</strong></span> (Adaptive Moment Estimation). Adam maintains a separate, adaptive learning rate for each weight, so you do not need to tune the learning rate as carefully. It is the default optimizer in most practical applications.</p>

        <p>The <span class="code-tooltip" data-tip="Controls the step size when updating weights. Too large: the model overshoots and diverges. Too small: training is extremely slow. Typical starting values are 0.001 to 0.01 for Adam."><strong>learning rate</strong></span> is the single most important hyperparameter. Too large and the model overshoots the optimum, bouncing around chaotically. Too small and training takes forever and may get stuck. A common strategy is to start with a moderate learning rate (0.001 for Adam) and use a <span class="code-tooltip" data-tip="A rule that reduces the learning rate during training. For example, 'reduce on plateau' halves the learning rate whenever the validation loss stops improving.">learning rate scheduler</span> that reduces it as training progresses.</p>

        <!-- Code Block: Training Loop -->
        <div class="code-tabs" data-runnable="ml-nn-2">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">from</span> torch.utils.data <span class="code-keyword">import</span> DataLoader, TensorDataset
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.preprocessing <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="Scales each feature to have mean 0 and standard deviation 1. This is essential for neural networks because features on different scales cause the gradient landscape to be elongated, making optimization difficult.">StandardScaler</span>
<span class="code-keyword">from</span> sklearn.datasets <span class="code-keyword">import</span> fetch_california_housing
<span class="code-keyword">from</span> sklearn.model_selection <span class="code-keyword">import</span> train_test_split

<span class="code-comment"># Load and prepare data</span>
housing = <span class="code-function">fetch_california_housing</span>()
X_train, X_test, y_train, y_test = <span class="code-function">train_test_split</span>(
    housing.data, housing.target, test_size=0.2, random_state=42
)

<span class="code-comment"># CRITICAL: Scale features for neural networks</span>
scaler = <span class="code-function">StandardScaler</span>()
X_train_scaled = scaler.<span class="code-function">fit_transform</span>(X_train)
X_test_scaled  = scaler.<span class="code-function">transform</span>(X_test)  <span class="code-comment"># Use train statistics!</span>

<span class="code-comment"># Convert to PyTorch tensors</span>
X_tr = torch.<span class="code-function">FloatTensor</span>(X_train_scaled)
y_tr = torch.<span class="code-function">FloatTensor</span>(y_train).<span class="code-function">unsqueeze</span>(1)
X_te = torch.<span class="code-function">FloatTensor</span>(X_test_scaled)
y_te = torch.<span class="code-function">FloatTensor</span>(y_test).<span class="code-function">unsqueeze</span>(1)

<span class="code-comment"># Create DataLoader for mini-batch training</span>
train_ds = <span class="code-function">TensorDataset</span>(X_tr, y_tr)
train_loader = <span class="code-function">DataLoader</span>(train_ds, <span class="code-tooltip" data-tip="Number of observations per mini-batch. 64 is a common default. Larger batches are faster per epoch but may generalize worse.">batch_size=64</span>, shuffle=True)

<span class="code-comment"># Define model</span>
model = nn.<span class="code-function">Sequential</span>(
    nn.<span class="code-function">Linear</span>(8, 128), nn.<span class="code-function">ReLU</span>(),
    nn.<span class="code-function">Linear</span>(128, 64), nn.<span class="code-function">ReLU</span>(),
    nn.<span class="code-function">Linear</span>(64, 32),  nn.<span class="code-function">ReLU</span>(),
    nn.<span class="code-function">Linear</span>(32, 1)
)

<span class="code-comment"># Loss function and optimizer</span>
criterion = nn.<span class="code-function">MSELoss</span>()
optimizer = torch.optim.<span class="code-function">Adam</span>(model.<span class="code-function">parameters</span>(), <span class="code-tooltip" data-tip="Starting learning rate for Adam. 0.001 is the default and a good starting point for most problems.">lr=0.001</span>)

<span class="code-comment"># Training loop</span>
<span class="code-tooltip" data-tip="Number of complete passes through the entire training dataset. Each epoch, every observation is seen once (in different mini-batches).">n_epochs</span> = 100
<span class="code-keyword">for</span> epoch <span class="code-keyword">in</span> <span class="code-function">range</span>(n_epochs):
    model.<span class="code-function">train</span>()  <span class="code-comment"># Set model to training mode</span>
    epoch_loss = 0.0

    <span class="code-keyword">for</span> X_batch, y_batch <span class="code-keyword">in</span> train_loader:
        <span class="code-comment"># Forward pass: compute predictions</span>
        predictions = <span class="code-function">model</span>(X_batch)
        loss = <span class="code-function">criterion</span>(predictions, y_batch)

        <span class="code-comment"># Backward pass: compute gradients</span>
        optimizer.<span class="code-function">zero_grad</span>()  <span class="code-comment"># Reset gradients from previous step</span>
        loss.<span class="code-function">backward</span>()         <span class="code-comment"># Backpropagation</span>
        optimizer.<span class="code-function">step</span>()          <span class="code-comment"># Update weights</span>

        epoch_loss += loss.<span class="code-function">item</span>() * X_batch.<span class="code-function">size</span>(0)

    <span class="code-comment"># Evaluate on test set every 20 epochs</span>
    <span class="code-keyword">if</span> (epoch + 1) % 20 == 0:
        model.<span class="code-function">eval</span>()  <span class="code-comment"># Set model to evaluation mode</span>
        <span class="code-keyword">with</span> torch.<span class="code-function">no_grad</span>():
            test_pred = <span class="code-function">model</span>(X_te)
            test_loss = <span class="code-function">criterion</span>(test_pred, y_te)
        train_loss = epoch_loss / <span class="code-function">len</span>(X_tr)
        <span class="code-function">print</span>(<span class="code-string">f"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}"</span>)

<span class="code-comment"># Final R-squared</span>
model.<span class="code-function">eval</span>()
<span class="code-keyword">with</span> torch.<span class="code-function">no_grad</span>():
    y_pred = <span class="code-function">model</span>(X_te).<span class="code-function">numpy</span>()
ss_res = np.<span class="code-function">sum</span>((y_test.reshape(-1,1) - y_pred)**2)
ss_tot = np.<span class="code-function">sum</span>((y_test - y_test.<span class="code-function">mean</span>())**2)
<span class="code-function">print</span>(<span class="code-string">f"\nFinal Test R-squared: {1 - ss_res/ss_tot:.4f}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Neural network training via Python block</span>

<span class="code-keyword">sysuse</span> auto, clear
<span class="code-keyword">set seed</span> 42
<span class="code-keyword">gen</span> rand = <span class="code-function">runiform</span>()
<span class="code-keyword">gen</span> byte train = (rand &lt;= 0.8)

<span class="code-keyword">python:</span>
<span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sfi <span class="code-keyword">import</span> Data

<span class="code-comment"># Get data from Stata</span>
price  = np.array(Data.get(<span class="code-string">"price"</span>), dtype=np.float32)
mpg    = np.array(Data.get(<span class="code-string">"mpg"</span>), dtype=np.float32)
weight = np.array(Data.get(<span class="code-string">"weight"</span>), dtype=np.float32)
length = np.array(Data.get(<span class="code-string">"length"</span>), dtype=np.float32)
tr     = np.array(Data.get(<span class="code-string">"train"</span>))

X = np.column_stack([mpg, weight, length])
y = price.reshape(-1, 1)
mask = (tr == 1)

<span class="code-comment"># Scale features</span>
X_mean, X_std = X[mask].mean(0), X[mask].std(0)
X = (X - X_mean) / X_std
y_mean, y_std = y[mask].mean(), y[mask].std()
y = (y - y_mean) / y_std

X_tr = torch.FloatTensor(X[mask])
y_tr = torch.FloatTensor(y[mask])
X_te = torch.FloatTensor(X[~mask])
y_te = torch.FloatTensor(y[~mask])

model = nn.Sequential(
    nn.Linear(3, 64), nn.ReLU(),
    nn.Linear(64, 32), nn.ReLU(),
    nn.Linear(32, 1)
)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

for epoch in range(100):
    pred = model(X_tr)
    loss = criterion(pred, y_tr)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 25 == 0:
        with torch.no_grad():
            test_loss = criterion(model(X_te), y_te)
        print(f<span class="code-string">"Epoch {epoch+1:3d} | Train: {loss:.4f} | Test: {test_loss:.4f}"</span>)
<span class="code-keyword">end</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Neural network training with torch + luz</span>
<span class="code-keyword">library</span>(torch)
<span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="A high-level interface for torch in R, similar to Keras. Simplifies the training loop by providing fit(), predict(), and other convenience methods.">luz</span>)

<span class="code-comment"># Prepare data</span>
<span class="code-keyword">data</span>(<span class="code-string">"BostonHousing"</span>, package = <span class="code-string">"mlbench"</span>)
df &lt;- BostonHousing

<span class="code-function">set.seed</span>(42)
idx &lt;- <span class="code-function">sample</span>(<span class="code-function">nrow</span>(df), <span class="code-function">floor</span>(0.8 * <span class="code-function">nrow</span>(df)))
train_df &lt;- df[idx, ]
test_df  &lt;- df[-idx, ]

<span class="code-comment"># Scale features</span>
x_train &lt;- <span class="code-function">scale</span>(<span class="code-function">as.matrix</span>(train_df[, -14]))
x_test  &lt;- <span class="code-function">scale</span>(<span class="code-function">as.matrix</span>(test_df[, -14]),
                 center = <span class="code-function">attr</span>(x_train, <span class="code-string">"scaled:center"</span>),
                 scale  = <span class="code-function">attr</span>(x_train, <span class="code-string">"scaled:scale"</span>))
y_train &lt;- <span class="code-function">as.matrix</span>(train_df$medv)
y_test  &lt;- <span class="code-function">as.matrix</span>(test_df$medv)

<span class="code-comment"># Convert to tensors</span>
x_tr &lt;- <span class="code-function">torch_tensor</span>(x_train, dtype = <span class="code-function">torch_float</span>())
y_tr &lt;- <span class="code-function">torch_tensor</span>(y_train, dtype = <span class="code-function">torch_float</span>())
x_te &lt;- <span class="code-function">torch_tensor</span>(x_test, dtype = <span class="code-function">torch_float</span>())
y_te &lt;- <span class="code-function">torch_tensor</span>(y_test, dtype = <span class="code-function">torch_float</span>())

<span class="code-comment"># Define model</span>
net &lt;- <span class="code-function">nn_sequential</span>(
  <span class="code-function">nn_linear</span>(13, 128), <span class="code-function">nn_relu</span>(),
  <span class="code-function">nn_linear</span>(128, 64), <span class="code-function">nn_relu</span>(),
  <span class="code-function">nn_linear</span>(64, 32),  <span class="code-function">nn_relu</span>(),
  <span class="code-function">nn_linear</span>(32, 1)
)

<span class="code-comment"># Training loop</span>
optimizer &lt;- <span class="code-function">optim_adam</span>(net$parameters, lr = 0.001)
criterion &lt;- <span class="code-function">nn_mse_loss</span>()

<span class="code-keyword">for</span> (epoch <span class="code-keyword">in</span> 1:100) {
  net$<span class="code-function">train</span>()
  pred &lt;- <span class="code-function">net</span>(x_tr)
  loss &lt;- <span class="code-function">criterion</span>(pred, y_tr)

  optimizer$<span class="code-function">zero_grad</span>()
  loss$<span class="code-function">backward</span>()
  optimizer$<span class="code-function">step</span>()

  <span class="code-keyword">if</span> (epoch %% 20 == 0) {
    net$<span class="code-function">eval</span>()
    test_pred &lt;- <span class="code-function">net</span>(x_te)
    test_loss &lt;- <span class="code-function">criterion</span>(test_pred, y_te)
    <span class="code-function">cat</span>(<span class="code-function">sprintf</span>(<span class="code-string">"Epoch %3d | Train: %.4f | Test: %.4f\n"</span>,
                epoch, loss$<span class="code-function">item</span>(), test_loss$<span class="code-function">item</span>()))
  }
}</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-nn-2 -->
        <div class="output-simulation" data-output="ml-nn-2" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch  20 | Train Loss: 0.4821 | Test Loss: 0.4523
Epoch  40 | Train Loss: 0.3156 | Test Loss: 0.3342
Epoch  60 | Train Loss: 0.2587 | Test Loss: 0.2901
Epoch  80 | Train Loss: 0.2234 | Test Loss: 0.2756
Epoch 100 | Train Loss: 0.2012 | Test Loss: 0.2698

Final Test R-squared: 0.7934</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-2" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch  25 | Train: 0.5632 | Test: 0.6123
Epoch  50 | Train: 0.3245 | Test: 0.4012
Epoch  75 | Train: 0.2187 | Test: 0.3456
Epoch 100 | Train: 0.1823 | Test: 0.3234</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-2" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch  20 | Train: 24.3412 | Test: 26.1234
Epoch  40 | Train: 12.5678 | Test: 15.8901
Epoch  60 | Train:  8.2345 | Test: 12.3456
Epoch  80 | Train:  6.1234 | Test: 10.8901
Epoch 100 | Train:  4.8901 | Test:  9.7654</pre></div>
        </div>


        <!-- ===================== REGULARIZATION IN NEURAL NETWORKS ===================== -->
        <h2 id="regularization-nn">Regularization in Neural Networks</h2>

        <p>Neural networks are powerful function approximators, but that power comes with a cost: they can easily memorize the training data, producing models that perform brilliantly on the training set but poorly on new data. This is especially problematic in economics, where sample sizes are often small relative to the complexity of the model. Regularization techniques constrain the model's capacity and help it generalize. Here are the most important ones.</p>

        <p><span class="code-tooltip" data-tip="During each training step, dropout randomly sets a fraction of neurons to zero. This forces the network to not rely on any single neuron, distributing knowledge across many neurons. At test time, all neurons are active but their outputs are scaled down to compensate."><strong>Dropout</strong></span> is the most widely used regularization technique specific to neural networks. During each training step, each neuron in a dropout layer has a probability <em>p</em> (typically 0.2 to 0.5) of being temporarily "dropped" ‚Äî its output is set to zero. This prevents the network from relying too heavily on any single neuron and forces it to develop redundant representations. At test time, all neurons are active, but their outputs are scaled by (1 - p) to compensate. Dropout can be thought of as training an ensemble of many different sub-networks and averaging their predictions.</p>

        <p><span class="code-tooltip" data-tip="Monitor the validation loss during training. When it stops improving (or starts increasing) for a specified number of epochs (called 'patience'), stop training and use the model from the best epoch."><strong>Early stopping</strong></span> is the simplest and often the most effective regularization strategy. The idea is to monitor the model's performance on a held-out validation set during training. Initially, both training and validation loss decrease. At some point, the training loss continues to decrease while the validation loss starts to increase ‚Äî this is the moment the model begins to overfit. Early stopping halts training at (or near) this point and restores the weights from the epoch with the lowest validation loss.</p>

        <p><span class="code-tooltip" data-tip="Normalizes the inputs to each layer by subtracting the batch mean and dividing by the batch standard deviation, then applying learned scale and shift parameters. This stabilizes training by reducing internal covariate shift."><strong>Batch normalization</strong></span> normalizes the inputs to each layer, making training faster and more stable. While originally introduced to speed up training rather than as a regularization technique, batch normalization has a mild regularizing effect because the normalization introduces noise through the mini-batch statistics.</p>

        <p><span class="code-tooltip" data-tip="Adds a penalty term to the loss function proportional to the sum of squared weights (L2) or absolute weights (L1). This is exactly analogous to Ridge and LASSO regression. In PyTorch, L2 weight decay is built into the optimizer."><strong>Weight decay</strong></span> (L2 regularization) penalizes large weight values, just like Ridge regression. In neural network frameworks, weight decay is typically implemented directly in the optimizer rather than as an explicit penalty term in the loss function. A small weight decay value (e.g., 1e-4 to 1e-2) is almost always beneficial.</p>

        <!-- Code Block: Regularization -->
        <div class="code-tabs" data-runnable="ml-nn-3">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">import</span> copy

<span class="code-comment"># Model with dropout and batch normalization</span>
model_reg = nn.<span class="code-function">Sequential</span>(
    nn.<span class="code-function">Linear</span>(8, 128),
    <span class="code-tooltip" data-tip="Normalizes the output of the previous layer across the mini-batch. Stabilizes and accelerates training.">nn.<span class="code-function">BatchNorm1d</span>(128)</span>,
    nn.<span class="code-function">ReLU</span>(),
    <span class="code-tooltip" data-tip="Randomly zeros 30% of neurons during training. This prevents co-adaptation and acts as ensemble regularization.">nn.<span class="code-function">Dropout</span>(0.3)</span>,

    nn.<span class="code-function">Linear</span>(128, 64),
    nn.<span class="code-function">BatchNorm1d</span>(64),
    nn.<span class="code-function">ReLU</span>(),
    nn.<span class="code-function">Dropout</span>(0.3),

    nn.<span class="code-function">Linear</span>(64, 32),
    nn.<span class="code-function">ReLU</span>(),
    nn.<span class="code-function">Dropout</span>(0.2),

    nn.<span class="code-function">Linear</span>(32, 1)
)

<span class="code-comment"># Optimizer with weight decay (L2 regularization)</span>
optimizer = torch.optim.<span class="code-function">Adam</span>(
    model_reg.<span class="code-function">parameters</span>(),
    lr=0.001,
    <span class="code-tooltip" data-tip="L2 penalty on the weights. A value of 1e-4 adds a small penalty proportional to the sum of squared weights, discouraging large weight values.">weight_decay=1e-4</span>
)
criterion = nn.<span class="code-function">MSELoss</span>()

<span class="code-comment"># Training with early stopping</span>
best_val_loss = <span class="code-function">float</span>(<span class="code-string">'inf'</span>)
best_model_state = <span class="code-keyword">None</span>
<span class="code-tooltip" data-tip="Number of epochs to wait for improvement before stopping. If the validation loss does not improve for 15 consecutive epochs, training stops.">patience</span> = 15
patience_counter = 0

<span class="code-keyword">for</span> epoch <span class="code-keyword">in</span> <span class="code-function">range</span>(200):
    <span class="code-comment"># Training</span>
    model_reg.<span class="code-function">train</span>()
    <span class="code-keyword">for</span> X_batch, y_batch <span class="code-keyword">in</span> train_loader:
        pred = <span class="code-function">model_reg</span>(X_batch)
        loss = <span class="code-function">criterion</span>(pred, y_batch)
        optimizer.<span class="code-function">zero_grad</span>()
        loss.<span class="code-function">backward</span>()
        optimizer.<span class="code-function">step</span>()

    <span class="code-comment"># Validation</span>
    model_reg.<span class="code-function">eval</span>()
    <span class="code-keyword">with</span> torch.<span class="code-function">no_grad</span>():
        val_loss = <span class="code-function">criterion</span>(<span class="code-function">model_reg</span>(X_te), y_te).<span class="code-function">item</span>()

    <span class="code-comment"># Early stopping check</span>
    <span class="code-keyword">if</span> val_loss &lt; best_val_loss:
        best_val_loss = val_loss
        best_model_state = copy.<span class="code-function">deepcopy</span>(model_reg.<span class="code-function">state_dict</span>())
        patience_counter = 0
    <span class="code-keyword">else</span>:
        patience_counter += 1
        <span class="code-keyword">if</span> patience_counter >= patience:
            <span class="code-function">print</span>(<span class="code-string">f"Early stopping at epoch {epoch+1}"</span>)
            <span class="code-keyword">break</span>

    <span class="code-keyword">if</span> (epoch + 1) % 25 == 0:
        <span class="code-function">print</span>(<span class="code-string">f"Epoch {epoch+1:3d} | Val Loss: {val_loss:.4f} | Best: {best_val_loss:.4f}"</span>)

<span class="code-comment"># Restore best model</span>
model_reg.<span class="code-function">load_state_dict</span>(best_model_state)
model_reg.<span class="code-function">eval</span>()
<span class="code-keyword">with</span> torch.<span class="code-function">no_grad</span>():
    y_pred = <span class="code-function">model_reg</span>(X_te).<span class="code-function">numpy</span>()
ss_res = np.<span class="code-function">sum</span>((y_test.reshape(-1,1) - y_pred)**2)
ss_tot = np.<span class="code-function">sum</span>((y_test - y_test.<span class="code-function">mean</span>())**2)
<span class="code-function">print</span>(<span class="code-string">f"\nRegularized Model Test R-squared: {1 - ss_res/ss_tot:.4f}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Regularized neural network via Python block</span>

<span class="code-keyword">python:</span>
<span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">import</span> copy
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sfi <span class="code-keyword">import</span> Data

<span class="code-comment"># Get and prepare data (reusing from earlier)</span>
price  = np.array(Data.get(<span class="code-string">"price"</span>), dtype=np.float32)
mpg    = np.array(Data.get(<span class="code-string">"mpg"</span>), dtype=np.float32)
weight = np.array(Data.get(<span class="code-string">"weight"</span>), dtype=np.float32)
length = np.array(Data.get(<span class="code-string">"length"</span>), dtype=np.float32)
tr     = np.array(Data.get(<span class="code-string">"train"</span>))

X = np.column_stack([mpg, weight, length])
y = price.reshape(-1, 1)
mask = (tr == 1)

<span class="code-comment"># Scale</span>
X_mean, X_std = X[mask].mean(0), X[mask].std(0)
X_s = (X - X_mean) / X_std
y_mean, y_std = y[mask].mean(), y[mask].std()
y_s = (y - y_mean) / y_std

X_tr = torch.FloatTensor(X_s[mask])
y_tr = torch.FloatTensor(y_s[mask])
X_te = torch.FloatTensor(X_s[~mask])
y_te = torch.FloatTensor(y_s[~mask])

<span class="code-comment"># Model with dropout</span>
model = nn.Sequential(
    nn.Linear(3, 64), nn.ReLU(), nn.Dropout(0.3),
    nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.2),
    nn.Linear(32, 1)
)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
criterion = nn.MSELoss()

best_loss = float(<span class="code-string">'inf'</span>)
best_state = None

for epoch in range(200):
    model.train()
    loss = criterion(model(X_tr), y_tr)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        vl = criterion(model(X_te), y_te).item()
    if vl &lt; best_loss:
        best_loss = vl
        best_state = copy.deepcopy(model.state_dict())
    if (epoch + 1) % 50 == 0:
        print(f<span class="code-string">"Epoch {epoch+1} | Val Loss: {vl:.4f}"</span>)

model.load_state_dict(best_state)
print(f<span class="code-string">"Best validation loss: {best_loss:.4f}"</span>)
<span class="code-keyword">end</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Regularized neural network with dropout and early stopping</span>
<span class="code-keyword">library</span>(torch)

<span class="code-comment"># Model with dropout</span>
net_reg &lt;- <span class="code-function">nn_sequential</span>(
  <span class="code-function">nn_linear</span>(13, 128),
  <span class="code-tooltip" data-tip="Batch normalization layer for 128 features."><span class="code-function">nn_batch_norm1d</span>(128)</span>,
  <span class="code-function">nn_relu</span>(),
  <span class="code-tooltip" data-tip="Dropout with 30% probability. Randomly zeroes neurons during training."><span class="code-function">nn_dropout</span>(p = 0.3)</span>,

  <span class="code-function">nn_linear</span>(128, 64),
  <span class="code-function">nn_batch_norm1d</span>(64),
  <span class="code-function">nn_relu</span>(),
  <span class="code-function">nn_dropout</span>(p = 0.3),

  <span class="code-function">nn_linear</span>(64, 32),
  <span class="code-function">nn_relu</span>(),
  <span class="code-function">nn_dropout</span>(p = 0.2),

  <span class="code-function">nn_linear</span>(32, 1)
)

<span class="code-comment"># Optimizer with weight decay</span>
optimizer &lt;- <span class="code-function">optim_adam</span>(net_reg$parameters, lr = 0.001,
                       <span class="code-tooltip" data-tip="L2 weight decay penalty of 0.0001.">weight_decay = 1e-4</span>)
criterion &lt;- <span class="code-function">nn_mse_loss</span>()

<span class="code-comment"># Training with early stopping</span>
best_val &lt;- Inf
patience &lt;- 15
wait &lt;- 0

<span class="code-keyword">for</span> (epoch <span class="code-keyword">in</span> 1:200) {
  net_reg$<span class="code-function">train</span>()
  pred &lt;- <span class="code-function">net_reg</span>(x_tr)
  loss &lt;- <span class="code-function">criterion</span>(pred, y_tr)
  optimizer$<span class="code-function">zero_grad</span>()
  loss$<span class="code-function">backward</span>()
  optimizer$<span class="code-function">step</span>()

  net_reg$<span class="code-function">eval</span>()
  val_loss &lt;- <span class="code-function">criterion</span>(<span class="code-function">net_reg</span>(x_te), y_te)$<span class="code-function">item</span>()

  <span class="code-keyword">if</span> (val_loss &lt; best_val) {
    best_val &lt;- val_loss
    wait &lt;- 0
  } <span class="code-keyword">else</span> {
    wait &lt;- wait + 1
    <span class="code-keyword">if</span> (wait >= patience) {
      <span class="code-function">cat</span>(<span class="code-string">"Early stopping at epoch"</span>, epoch, <span class="code-string">"\n"</span>)
      <span class="code-keyword">break</span>
    }
  }

  <span class="code-keyword">if</span> (epoch %% 25 == 0) {
    <span class="code-function">cat</span>(<span class="code-function">sprintf</span>(<span class="code-string">"Epoch %3d | Val: %.4f | Best: %.4f\n"</span>,
                epoch, val_loss, best_val))
  }
}</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-nn-3 -->
        <div class="output-simulation" data-output="ml-nn-3" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch  25 | Val Loss: 0.3892 | Best: 0.3756
Epoch  50 | Val Loss: 0.2845 | Best: 0.2701
Epoch  75 | Val Loss: 0.2634 | Best: 0.2589
Epoch 100 | Val Loss: 0.2612 | Best: 0.2534
Early stopping at epoch 118

Regularized Model Test R-squared: 0.8067</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-3" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch 50 | Val Loss: 0.4231
Epoch 100 | Val Loss: 0.3123
Epoch 150 | Val Loss: 0.2987
Best validation loss: 0.2845</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-3" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch  25 | Val: 18.4523 | Best: 17.8901
Epoch  50 | Val: 11.2345 | Best: 10.5678
Epoch  75 | Val: 9.1234 | Best: 8.7654
Epoch 100 | Val: 8.9012 | Best: 8.3456
Early stopping at epoch 112</pre></div>
        </div>


        <!-- ===================== PRACTICAL IMPLEMENTATION ===================== -->
        <h2 id="practical">Practical Implementation</h2>

        <p>Training a neural network involves several practical considerations that go beyond defining the architecture. The most important is <strong>feature scaling</strong>. Unlike tree-based methods, neural networks are sensitive to the scale of input features. If one feature ranges from 0 to 1 and another from 0 to 1,000,000, the gradients will be dominated by the large-scale feature, making training slow or unstable. Always standardize your features (subtract mean, divide by standard deviation) before feeding them to a neural network. Use the training set statistics for standardization and apply the same transformation to the test set.</p>

        <p><strong>Batch size</strong> controls the trade-off between computation speed and gradient quality. Larger batches (256, 512) produce more stable gradient estimates and train faster on GPUs, but may converge to sharper minima that generalize worse. Smaller batches (32, 64) introduce more noise in the gradients, which acts as implicit regularization and often leads to better generalization. A batch size of 32 or 64 is a good starting point for most tabular datasets in economics.</p>

        <p>The number of <strong>epochs</strong> (complete passes through the training data) depends on the problem and the learning rate. With early stopping, you can set a large maximum number of epochs (say, 500) and let the early stopping criterion determine when to stop. Monitor both training and validation loss curves: if the training loss is much lower than the validation loss, the model is overfitting; if both are high, the model is underfitting (try a larger network or more epochs).</p>

        <p>For large datasets or complex models, <strong>GPU acceleration</strong> can speed up training dramatically. PyTorch makes this easy: move your model and data to the GPU with <code>.to('cuda')</code>, and everything else stays the same. For the tabular datasets typical in economics (thousands to millions of rows, tens to hundreds of features), CPU training is usually fast enough.</p>

        <!-- Code Block: Complete Pipeline -->
        <div class="code-tabs" data-runnable="ml-nn-4">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">from</span> torch.utils.data <span class="code-keyword">import</span> DataLoader, TensorDataset
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">import</span> pandas <span class="code-keyword">as</span> pd
<span class="code-keyword">from</span> sklearn.preprocessing <span class="code-keyword">import</span> StandardScaler
<span class="code-keyword">from</span> sklearn.datasets <span class="code-keyword">import</span> fetch_california_housing
<span class="code-keyword">from</span> sklearn.model_selection <span class="code-keyword">import</span> train_test_split
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error, r2_score, mean_absolute_error
<span class="code-keyword">import</span> copy

<span class="code-comment"># ============ 1. DATA PREPARATION ============</span>
housing = <span class="code-function">fetch_california_housing</span>()
X = pd.<span class="code-function">DataFrame</span>(housing.data, columns=housing.feature_names)
y = housing.target

<span class="code-comment"># Split: 70% train, 15% validation, 15% test</span>
X_train, X_temp, y_train, y_temp = <span class="code-function">train_test_split</span>(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = <span class="code-function">train_test_split</span>(X_temp, y_temp, test_size=0.5, random_state=42)

<span class="code-comment"># Scale features</span>
scaler = <span class="code-function">StandardScaler</span>()
X_train_s = scaler.<span class="code-function">fit_transform</span>(X_train)
X_val_s   = scaler.<span class="code-function">transform</span>(X_val)
X_test_s  = scaler.<span class="code-function">transform</span>(X_test)

<span class="code-comment"># Convert to tensors</span>
X_tr = torch.<span class="code-function">FloatTensor</span>(X_train_s)
y_tr = torch.<span class="code-function">FloatTensor</span>(y_train.values).<span class="code-function">unsqueeze</span>(1)
X_vl = torch.<span class="code-function">FloatTensor</span>(X_val_s)
y_vl = torch.<span class="code-function">FloatTensor</span>(y_val.values).<span class="code-function">unsqueeze</span>(1)
X_te = torch.<span class="code-function">FloatTensor</span>(X_test_s)
y_te = torch.<span class="code-function">FloatTensor</span>(y_test.values).<span class="code-function">unsqueeze</span>(1)

train_loader = <span class="code-function">DataLoader</span>(
    <span class="code-function">TensorDataset</span>(X_tr, y_tr), batch_size=64, shuffle=True
)

<span class="code-comment"># ============ 2. MODEL DEFINITION ============</span>
model = nn.<span class="code-function">Sequential</span>(
    nn.<span class="code-function">Linear</span>(8, 256), nn.<span class="code-function">BatchNorm1d</span>(256), nn.<span class="code-function">ReLU</span>(), nn.<span class="code-function">Dropout</span>(0.3),
    nn.<span class="code-function">Linear</span>(256, 128), nn.<span class="code-function">BatchNorm1d</span>(128), nn.<span class="code-function">ReLU</span>(), nn.<span class="code-function">Dropout</span>(0.3),
    nn.<span class="code-function">Linear</span>(128, 64), nn.<span class="code-function">ReLU</span>(), nn.<span class="code-function">Dropout</span>(0.2),
    nn.<span class="code-function">Linear</span>(64, 1)
)

<span class="code-comment"># ============ 3. TRAINING ============</span>
optimizer = torch.optim.<span class="code-function">Adam</span>(model.<span class="code-function">parameters</span>(), lr=0.001, weight_decay=1e-4)
<span class="code-tooltip" data-tip="Reduces the learning rate by a factor of 0.5 when the validation loss has not improved for 10 epochs. This fine-tunes the model as it approaches the optimum.">scheduler</span> = torch.optim.lr_scheduler.<span class="code-function">ReduceLROnPlateau</span>(
    optimizer, mode=<span class="code-string">'min'</span>, factor=0.5, patience=10
)
criterion = nn.<span class="code-function">MSELoss</span>()

best_val_loss = <span class="code-function">float</span>(<span class="code-string">'inf'</span>)
best_state = <span class="code-keyword">None</span>
patience, wait = 25, 0

<span class="code-keyword">for</span> epoch <span class="code-keyword">in</span> <span class="code-function">range</span>(300):
    model.<span class="code-function">train</span>()
    <span class="code-keyword">for</span> xb, yb <span class="code-keyword">in</span> train_loader:
        loss = <span class="code-function">criterion</span>(<span class="code-function">model</span>(xb), yb)
        optimizer.<span class="code-function">zero_grad</span>()
        loss.<span class="code-function">backward</span>()
        optimizer.<span class="code-function">step</span>()

    model.<span class="code-function">eval</span>()
    <span class="code-keyword">with</span> torch.<span class="code-function">no_grad</span>():
        val_loss = <span class="code-function">criterion</span>(<span class="code-function">model</span>(X_vl), y_vl).<span class="code-function">item</span>()
    scheduler.<span class="code-function">step</span>(val_loss)

    <span class="code-keyword">if</span> val_loss &lt; best_val_loss:
        best_val_loss = val_loss
        best_state = copy.<span class="code-function">deepcopy</span>(model.<span class="code-function">state_dict</span>())
        wait = 0
    <span class="code-keyword">else</span>:
        wait += 1
        <span class="code-keyword">if</span> wait >= patience:
            <span class="code-function">print</span>(<span class="code-string">f"Early stopping at epoch {epoch+1}"</span>)
            <span class="code-keyword">break</span>

<span class="code-comment"># ============ 4. EVALUATION ============</span>
model.<span class="code-function">load_state_dict</span>(best_state)
model.<span class="code-function">eval</span>()
<span class="code-keyword">with</span> torch.<span class="code-function">no_grad</span>():
    y_pred = <span class="code-function">model</span>(X_te).<span class="code-function">numpy</span>().<span class="code-function">flatten</span>()

<span class="code-function">print</span>(<span class="code-string">"=== Final Evaluation on Test Set ==="</span>)
<span class="code-function">print</span>(<span class="code-string">f"R-squared:          {r2_score(y_test, y_pred):.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"RMSE:               {mean_squared_error(y_test, y_pred, squared=False):.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"MAE:                {mean_absolute_error(y_test, y_pred):.4f}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Complete neural network pipeline via Python</span>

<span class="code-keyword">sysuse</span> auto, clear
<span class="code-keyword">set seed</span> 42
<span class="code-keyword">gen</span> rand = <span class="code-function">runiform</span>()
<span class="code-keyword">gen</span> byte split = <span class="code-function">cond</span>(rand &lt;= 0.7, 1, <span class="code-function">cond</span>(rand &lt;= 0.85, 2, 3))
<span class="code-comment">* split: 1=train, 2=validation, 3=test</span>

<span class="code-keyword">python:</span>
<span class="code-keyword">import</span> torch, torch.nn <span class="code-keyword">as</span> nn, copy
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sfi <span class="code-keyword">import</span> Data

<span class="code-comment"># Load data</span>
price  = np.array(Data.get(<span class="code-string">"price"</span>), dtype=np.float32)
mpg    = np.array(Data.get(<span class="code-string">"mpg"</span>), dtype=np.float32)
weight = np.array(Data.get(<span class="code-string">"weight"</span>), dtype=np.float32)
length = np.array(Data.get(<span class="code-string">"length"</span>), dtype=np.float32)
split  = np.array(Data.get(<span class="code-string">"split"</span>))

X = np.column_stack([mpg, weight, length])
y = price.reshape(-1, 1)
tr_mask = (split == 1); vl_mask = (split == 2); te_mask = (split == 3)

<span class="code-comment"># Scale</span>
mu, sd = X[tr_mask].mean(0), X[tr_mask].std(0)
X_s = (X - mu) / sd

X_tr = torch.FloatTensor(X_s[tr_mask])
y_tr = torch.FloatTensor(y[tr_mask])
X_vl = torch.FloatTensor(X_s[vl_mask])
y_vl = torch.FloatTensor(y[vl_mask])
X_te = torch.FloatTensor(X_s[te_mask])
y_te = torch.FloatTensor(y[te_mask])

model = nn.Sequential(
    nn.Linear(3, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.3),
    nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.2),
    nn.Linear(32, 1)
)

opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
crit = nn.MSELoss()

best_vl = float(<span class="code-string">'inf'</span>)
best_st = None
for ep in range(200):
    model.train()
    loss = crit(model(X_tr), y_tr)
    opt.zero_grad(); loss.backward(); opt.step()
    model.eval()
    with torch.no_grad():
        vl = crit(model(X_vl), y_vl).item()
    if vl &lt; best_vl:
        best_vl = vl; best_st = copy.deepcopy(model.state_dict())

model.load_state_dict(best_st)
model.eval()
with torch.no_grad():
    pred = model(X_te).numpy()
    r2 = 1 - np.sum((y[te_mask] - pred)**2) / np.sum((y[te_mask] - y[te_mask].mean())**2)
    rmse = np.sqrt(np.mean((y[te_mask] - pred)**2))
print(f<span class="code-string">"Test R2: {r2:.4f}"</span>)
print(f<span class="code-string">"Test RMSE: {rmse:.2f}"</span>)
<span class="code-keyword">end</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Complete neural network pipeline</span>
<span class="code-keyword">library</span>(torch)

<span class="code-comment"># ---- 1. Data preparation ----</span>
<span class="code-keyword">data</span>(<span class="code-string">"BostonHousing"</span>, package = <span class="code-string">"mlbench"</span>)
df &lt;- BostonHousing

<span class="code-function">set.seed</span>(42)
n &lt;- <span class="code-function">nrow</span>(df)
idx &lt;- <span class="code-function">sample</span>(n)
train_end &lt;- <span class="code-function">floor</span>(0.7 * n)
val_end   &lt;- <span class="code-function">floor</span>(0.85 * n)

train_df &lt;- df[idx[1:train_end], ]
val_df   &lt;- df[idx[(train_end+1):val_end], ]
test_df  &lt;- df[idx[(val_end+1):n], ]

<span class="code-comment"># Scale</span>
x_tr_raw &lt;- <span class="code-function">as.matrix</span>(train_df[, -14])
x_tr &lt;- <span class="code-function">scale</span>(x_tr_raw)
ctr &lt;- <span class="code-function">attr</span>(x_tr, <span class="code-string">"scaled:center"</span>)
scl &lt;- <span class="code-function">attr</span>(x_tr, <span class="code-string">"scaled:scale"</span>)
x_vl &lt;- <span class="code-function">scale</span>(<span class="code-function">as.matrix</span>(val_df[, -14]), center = ctr, scale = scl)
x_te &lt;- <span class="code-function">scale</span>(<span class="code-function">as.matrix</span>(test_df[, -14]), center = ctr, scale = scl)

<span class="code-comment"># Tensors</span>
xt &lt;- <span class="code-function">torch_tensor</span>(x_tr, dtype = <span class="code-function">torch_float</span>())
yt &lt;- <span class="code-function">torch_tensor</span>(<span class="code-function">as.matrix</span>(train_df$medv), dtype = <span class="code-function">torch_float</span>())
xv &lt;- <span class="code-function">torch_tensor</span>(x_vl, dtype = <span class="code-function">torch_float</span>())
yv &lt;- <span class="code-function">torch_tensor</span>(<span class="code-function">as.matrix</span>(val_df$medv), dtype = <span class="code-function">torch_float</span>())
xe &lt;- <span class="code-function">torch_tensor</span>(x_te, dtype = <span class="code-function">torch_float</span>())

<span class="code-comment"># ---- 2. Model ----</span>
net &lt;- <span class="code-function">nn_sequential</span>(
  <span class="code-function">nn_linear</span>(13, 256), <span class="code-function">nn_batch_norm1d</span>(256), <span class="code-function">nn_relu</span>(), <span class="code-function">nn_dropout</span>(0.3),
  <span class="code-function">nn_linear</span>(256, 128), <span class="code-function">nn_batch_norm1d</span>(128), <span class="code-function">nn_relu</span>(), <span class="code-function">nn_dropout</span>(0.3),
  <span class="code-function">nn_linear</span>(128, 64), <span class="code-function">nn_relu</span>(), <span class="code-function">nn_dropout</span>(0.2),
  <span class="code-function">nn_linear</span>(64, 1)
)

<span class="code-comment"># ---- 3. Train ----</span>
opt &lt;- <span class="code-function">optim_adam</span>(net$parameters, lr = 0.001, weight_decay = 1e-4)
crit &lt;- <span class="code-function">nn_mse_loss</span>()
best_val &lt;- Inf

<span class="code-keyword">for</span> (ep <span class="code-keyword">in</span> 1:200) {
  net$<span class="code-function">train</span>()
  loss &lt;- <span class="code-function">crit</span>(<span class="code-function">net</span>(xt), yt)
  opt$<span class="code-function">zero_grad</span>(); loss$<span class="code-function">backward</span>(); opt$<span class="code-function">step</span>()
  net$<span class="code-function">eval</span>()
  vl &lt;- <span class="code-function">crit</span>(<span class="code-function">net</span>(xv), yv)$<span class="code-function">item</span>()
  <span class="code-keyword">if</span> (vl &lt; best_val) best_val &lt;- vl
  <span class="code-keyword">if</span> (ep %% 50 == 0) <span class="code-function">cat</span>(<span class="code-function">sprintf</span>(<span class="code-string">"Epoch %3d | Val: %.4f\n"</span>, ep, vl))
}

<span class="code-comment"># ---- 4. Evaluate ----</span>
net$<span class="code-function">eval</span>()
pred &lt;- <span class="code-function">as.numeric</span>(<span class="code-function">net</span>(xe))
actual &lt;- test_df$medv
r2 &lt;- 1 - <span class="code-function">sum</span>((actual - pred)^2) / <span class="code-function">sum</span>((actual - <span class="code-function">mean</span>(actual))^2)
rmse &lt;- <span class="code-function">sqrt</span>(<span class="code-function">mean</span>((actual - pred)^2))
mae &lt;- <span class="code-function">mean</span>(<span class="code-function">abs</span>(actual - pred))

<span class="code-function">cat</span>(<span class="code-string">"\n=== Final Test Evaluation ===\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"R-squared:"</span>, <span class="code-function">round</span>(r2, 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"RMSE:     "</span>, <span class="code-function">round</span>(rmse, 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"MAE:      "</span>, <span class="code-function">round</span>(mae, 4), <span class="code-string">"\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-nn-4 -->
        <div class="output-simulation" data-output="ml-nn-4" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Early stopping at epoch 142

=== Final Evaluation on Test Set ===
R-squared:          0.8134
RMSE:               0.5198
MAE:                0.3612</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-4" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Test R2: 0.7856
Test RMSE: 1932.45</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-nn-4" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Epoch  50 | Val: 14.2345
Epoch 100 | Val: 9.4567
Epoch 150 | Val: 8.1234
Epoch 200 | Val: 7.8901

=== Final Test Evaluation ===
R-squared: 0.8456
RMSE:      3.4521
MAE:       2.1234</pre></div>
        </div>


        <!-- ===================== WHEN TO USE IN ECONOMICS ===================== -->
        <h2 id="economics">When to Use Neural Networks in Economics</h2>

        <p>For the types of data most common in economics ‚Äî structured, tabular datasets with tens to hundreds of features and thousands to hundreds of thousands of observations ‚Äî neural networks are <strong>usually not the best choice</strong>. Tree-based methods (Random Forests, XGBoost) consistently match or outperform neural networks on tabular data, are easier to tune, and require less preprocessing. Multiple large-scale benchmarks have confirmed this finding. If your data fits in a spreadsheet, start with XGBoost.</p>

        <p>Where neural networks truly shine is on <strong>unstructured data</strong>: text (natural language processing), images (computer vision), audio, and other high-dimensional, spatially or sequentially structured inputs. If your economics research involves analyzing text from central bank communications, classifying satellite images to measure economic activity, or processing survey audio recordings, neural networks are the right tool. They are also valuable when you have <strong>very large datasets</strong> (millions of observations) where the additional model capacity pays off, and when you need to learn representations that transfer across tasks (transfer learning).</p>

        <p>A practical concern for policy-oriented economics research is <strong>interpretability</strong>. Regulators, policymakers, and journal referees often want to understand why a model makes certain predictions. Neural networks are inherently opaque ‚Äî even with tools like SHAP values (which also work on neural networks but are slower to compute), the explanation is less transparent than for a linear model or even a decision tree. If your goal is policy-relevant prediction (for example, predicting recidivism to inform bail decisions), the interpretability costs of neural networks may outweigh their marginal predictive gains over simpler models.</p>

        <div class="info-box">
          <h3>When to Use Neural Networks vs. Other Methods</h3>
          <ul>
            <li><strong>Use neural networks</strong> for: text data (NLP), image data (computer vision), very large datasets (millions of rows), tasks where transfer learning is beneficial, or when you need to learn complex representations from raw data.</li>
            <li><strong>Use tree-based methods</strong> (XGBoost, Random Forest) for: standard tabular data, moderate-size datasets, when you need variable importance, or when ease of tuning matters.</li>
            <li><strong>Use regularized regression</strong> (LASSO, Ridge) for: inference-adjacent prediction tasks, small samples, when interpretability is paramount, or when the true relationship is approximately linear.</li>
          </ul>
        </div>


        <!-- ===================== LIMITATIONS ===================== -->
        <h2 id="limitations">Limitations</h2>

        <div class="info-box" style="border-left-color: #e53e3e;">
          <h3>Key Limitations of Neural Networks</h3>
          <ul>
            <li><strong>Data hungry:</strong> Neural networks require large amounts of data to train well. With small samples (a few hundred or even a few thousand observations, typical for many economic panel datasets), neural networks are prone to overfitting and usually underperform simpler models like LASSO or Random Forests.</li>
            <li><strong>Computationally expensive:</strong> Training a neural network involves iterating over the data many times (epochs), computing gradients for thousands or millions of parameters. This is much slower than fitting an OLS regression or even training an XGBoost model. GPU hardware can help, but adds cost and complexity.</li>
            <li><strong>Hard to interpret:</strong> Neural networks are black boxes. While SHAP values, attention weights, and other interpretation techniques exist, they provide approximations rather than the clean, transparent explanations that a linear model or single decision tree offers. This is a serious limitation for policy applications.</li>
            <li><strong>Sensitive to hyperparameters:</strong> The number of layers, number of neurons, learning rate, dropout rate, batch size, and other settings all affect performance significantly. There is no reliable theory to guide these choices ‚Äî you must rely on cross-validation and experimentation, which is time-consuming.</li>
            <li><strong>Feature scaling is required:</strong> Unlike tree-based methods, neural networks require that features be standardized or normalized. Forgetting this step can cause training to fail silently ‚Äî the model may appear to train but produce poor results.</li>
            <li><strong>Overfitting on small data:</strong> A network with 10,000 parameters and 500 training observations is like fitting a polynomial of degree 10,000 ‚Äî it will memorize the training data perfectly and generalize terribly. Regularization helps, but cannot fully compensate for insufficient data.</li>
          </ul>
        </div>


        <!-- ===================== REFERENCES ===================== -->
        <h2>References</h2>
        <ul class="references">
          <li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press. (Available free at <a href="https://www.deeplearningbook.org/" target="_blank">deeplearningbook.org</a>)</li>
          <li>LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. <em>Nature</em>, 521(7553), 436-444.</li>
          <li>Mullainathan, S., & Spiess, J. (2017). Machine Learning: An Applied Econometric Approach. <em>Journal of Economic Perspectives</em>, 31(2), 87-106.</li>
          <li>Ng, A. Stanford CS229: Machine Learning. Lecture Notes on Neural Networks.</li>
        </ul>


        <!-- ===================== NAV FOOTER ===================== -->
        <div class="nav-footer">
          <a href="11b-trees.html" class="nav-link prev">11B: Tree-Based Methods</a>
          <a href="11d-causal-ml.html" class="nav-link next">11D: Causal ML</a>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about Python, Stata, R, causal inference methods, or any of the course material. How can I assist you today?</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    let tooltipEl = null; let currentTarget = null; let hideTimeout = null;
    function createTooltip() { if (tooltipEl) return tooltipEl; tooltipEl = document.createElement('div'); tooltipEl.className = 'tooltip-popup'; document.body.appendChild(tooltipEl); return tooltipEl; }
    function positionTooltip(target) { const tooltip = createTooltip(); const tipText = target.getAttribute('data-tip'); if (!tipText) return; tooltip.textContent = tipText; tooltip.className = 'tooltip-popup'; const targetRect = target.getBoundingClientRect(); let container = target.closest('pre') || target.closest('.tab-content') || target.closest('.code-tabs'); let containerRect = container ? container.getBoundingClientRect() : { left: 0, right: window.innerWidth, top: 0, bottom: window.innerHeight }; const vw = window.innerWidth; const vh = window.innerHeight; const pad = 10; tooltip.style.visibility = 'hidden'; tooltip.style.display = 'block'; tooltip.classList.add('visible'); const tr = tooltip.getBoundingClientRect(); let left = targetRect.left + (targetRect.width/2) - (tr.width/2); let top = targetRect.top - tr.height - 8; let ac = 'arrow-bottom'; if (top < pad) { top = targetRect.bottom + 8; ac = 'arrow-top'; } if (top + tr.height > vh - pad) { top = targetRect.top - tr.height - 8; ac = 'arrow-bottom'; } if (left < pad) left = pad; if (left + tr.width > vw - pad) left = vw - tr.width - pad; if (container) { const ml = Math.max(pad, containerRect.left); const mr = Math.min(vw - pad, containerRect.right); if (left < ml) left = ml; if (left + tr.width > mr) left = mr - tr.width; } tooltip.style.left = left + 'px'; tooltip.style.top = top + 'px'; tooltip.style.visibility = 'visible'; tooltip.classList.add(ac); }
    function showTooltip(t) { if (hideTimeout) { clearTimeout(hideTimeout); hideTimeout = null; } currentTarget = t; positionTooltip(t); }
    function hideTooltip() { hideTimeout = setTimeout(function() { if (tooltipEl) tooltipEl.classList.remove('visible'); currentTarget = null; }, 100); }
    document.addEventListener('mouseenter', function(e) { if (e.target.classList && e.target.classList.contains('code-tooltip')) showTooltip(e.target); }, true);
    document.addEventListener('mouseleave', function(e) { if (e.target.classList && e.target.classList.contains('code-tooltip')) hideTooltip(); }, true);
    document.addEventListener('scroll', function() { if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget); }, true);
    window.addEventListener('resize', function() { if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget); });
  })();
  </script>
</body>
</html>