<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>11D Causal ML | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content { -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; }
    .protected-content pre, .protected-content code, .protected-content .code-block, .protected-content .code-tabs { -webkit-user-select: text; -moz-user-select: text; -ms-user-select: text; user-select: text; }
    .code-tooltip { position: relative; cursor: help; border-bottom: 1px dotted #888; text-decoration: none; }
    .tooltip-popup { position: fixed; background: #1f2937; color: white; padding: 0.5rem 0.75rem; border-radius: 6px; font-size: 0.75rem; white-space: normal; max-width: 300px; opacity: 0; pointer-events: none; transition: opacity 0.15s ease-in-out; z-index: 10000; }
    .tooltip-popup.visible { opacity: 1; }
    .distinction-box { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0; }
    @media (max-width: 768px) { .distinction-box { grid-template-columns: 1fr; } }
    .distinction-card { background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem; }
    .distinction-card h4 { margin: 0 0 0.5rem 0; color: #2563eb; }
    .distinction-card ul { margin: 0; padding-left: 1.25rem; }
  </style>
</head>
<body>
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <div class="course-description">
        <h3>Course Modules</h3>
        <ul class="module-list">
          <li><strong>Module 0:</strong> Languages & Platforms — Python, Stata, R setup; IDEs (RStudio, VS Code, Jupyter)</li>
          <li><strong>Module 1:</strong> Getting Started — Installation, basic syntax, packages</li>
          <li><strong>Module 2:</strong> Data Harnessing — File import, APIs, web scraping</li>
          <li><strong>Module 3:</strong> Data Exploration — Inspection, summary statistics, visualization</li>
          <li><strong>Module 4:</strong> Data Cleaning — Data quality, transformation, validation</li>
          <li><strong>Module 5:</strong> Data Analysis — Statistical analysis, simulation, experimental design</li>
          <li><strong>Module 6:</strong> Causal Inference — Matching, DiD, RDD, IV, Synthetic Control</li>
          <li><strong>Module 7:</strong> Estimation Methods — Standard errors, panel data, MLE/GMM</li>
          <li><strong>Module 8:</strong> Replicability — Project organization, documentation, replication packages</li>
          <li><strong>Module 9:</strong> Git & GitHub — Version control, collaboration, branching</li>
          <li><strong>Module 10:</strong> History of NLP — From ELIZA to Transformers</li>
          <li><strong>Module 11:</strong> Machine Learning — Prediction, regularization, neural networks</li>
          <li><strong>Module 12:</strong> Large Language Models — How LLMs work, prompting, APIs</li>
        </ul>
      </div>
      <div class="access-note">
        This course is currently open to <strong>students at Sciences Po</strong>. If you are not a Sciences Po student but would like access, please <a href="mailto:giulia.caprini@sciencespo.fr">email me</a> to request an invite token.
      </div>
      <div class="password-form">
        <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
        <button id="password-submit">Access Course</button>
        <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
      </div>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">&#127968;</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li class="has-subnav">
            <a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a>
            <ul class="sub-nav">
              <li><a href="10a-text-analysis-today.html">Text Analysis Today</a></li>
            </ul>
          </li>
          <li class="has-subnav active">
            <a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a>
            <ul class="sub-nav">
              <li><a href="11a-regularization.html">Regularization</a></li>
              <li><a href="11b-trees.html">Tree-Based Methods</a></li>
              <li><a href="11c-neural-networks.html">Neural Networks</a></li>
              <li class="active"><a href="11d-causal-ml.html">Causal ML</a></li>
              <li><a href="11e-model-evaluation.html">Model Evaluation</a></li>
            </ul>
          </li>
          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content-wrapper">
        <div class="breadcrumb">
          <a href="11-machine-learning.html">11 Machine Learning</a> &raquo; Causal ML
        </div>

        <h1>11D &nbsp;Causal ML</h1>
        <div class="module-meta">
          <span>~3 hours</span>
          <span>DML, Causal Forests, Heterogeneous Treatment Effects</span>
        </div>

        <!-- Learning Objectives -->
        <div class="info-box">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Understand why standard ML methods fail for causal inference and how to fix them</li>
            <li>Implement Double/Debiased Machine Learning (DML) to estimate treatment effects with high-dimensional controls</li>
            <li>Use Causal Forests to discover heterogeneous treatment effects across subpopulations</li>
            <li>Apply these methods in Python, Stata, and R using purpose-built packages</li>
            <li>Know when causal ML adds value over traditional econometric approaches</li>
          </ul>
        </div>

        <!-- Table of Contents -->
        <div class="toc">
          <h3>Contents</h3>
          <ul>
            <li><a href="#why-causal-ml">Why Combine ML with Causal Inference?</a></li>
            <li><a href="#dml">Double/Debiased Machine Learning (DML)</a></li>
            <li><a href="#causal-forests">Causal Forests</a></li>
            <li><a href="#hte">Heterogeneous Treatment Effects</a></li>
            <li><a href="#practical-guidance">Practical Guidance</a></li>
            <li><a href="#limitations">Limitations</a></li>
          </ul>
        </div>


        <!-- ===================== WHY COMBINE ML WITH CAUSAL INFERENCE? ===================== -->
        <h2 id="why-causal-ml">Why Combine ML with Causal Inference?</h2>

        <p>Throughout this course, you have learned two distinct sets of tools. In Module 6, you studied <strong>causal inference</strong> methods — difference-in-differences, instrumental variables, regression discontinuity — all designed to answer the question "What is the effect of X on Y?" In Module 11, you have been learning <strong>machine learning</strong> methods — LASSO, random forests, neural networks — designed to answer the question "How can I best predict Y from X?" These two goals are fundamentally different, and conflating them is one of the most common mistakes in applied research.</p>

        <p>But there is a productive middle ground. Machine learning tools can be used <em>in service of</em> causal inference, not to replace it. The key insight is that many causal inference problems contain prediction subproblems. For example, if you want to estimate the effect of a job training program on earnings, controlling for a large set of demographic and labor market variables, you need to predict both the outcome (earnings) and the treatment (program participation) as functions of those controls. ML excels at this kind of flexible prediction. The challenge is using ML predictions as inputs to causal estimation without introducing bias.</p>

        <p>This module covers two major frameworks that solve this problem rigorously: <strong>Double/Debiased Machine Learning</strong> (DML), which estimates average treatment effects while using ML to control for high-dimensional confounders, and <strong>Causal Forests</strong>, which estimate heterogeneous treatment effects — how the effect varies across different subpopulations.</p>

        <!-- ML + Causal Inference Diagram -->
        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 12px; padding: 2rem; margin: 1.5rem 0;">
          <h4 style="margin: 0 0 1.5rem 0; color: #1e293b; text-align: center;">Two Worlds, One Framework</h4>
          <div style="display: grid; grid-template-columns: 1fr auto 1fr; gap: 1rem; align-items: center; max-width: 700px; margin: 0 auto;">
            <!-- ML Box -->
            <div style="background: #dbeafe; border: 2px solid #3b82f6; border-radius: 10px; padding: 1.25rem; text-align: center;">
              <div style="font-weight: 700; color: #1e40af; font-size: 1.05rem; margin-bottom: 0.5rem;">Machine Learning</div>
              <div style="font-size: 0.85rem; color: #1e3a5f; line-height: 1.5;">
                Flexible prediction<br>
                High-dimensional data<br>
                Regularization<br>
                Cross-validation
              </div>
            </div>
            <!-- Arrow / Plus -->
            <div style="text-align: center;">
              <div style="font-size: 2rem; font-weight: 700; color: #6366f1;">+</div>
            </div>
            <!-- Causal Box -->
            <div style="background: #fef3c7; border: 2px solid #d97706; border-radius: 10px; padding: 1.25rem; text-align: center;">
              <div style="font-weight: 700; color: #92400e; font-size: 1.05rem; margin-bottom: 0.5rem;">Causal Inference</div>
              <div style="font-size: 0.85rem; color: #78350f; line-height: 1.5;">
                Identification strategy<br>
                Unbiased estimation<br>
                Valid inference<br>
                Interpretable effects
              </div>
            </div>
          </div>
          <!-- Down Arrow -->
          <div style="text-align: center; margin: 1.25rem 0;">
            <div style="font-size: 1.75rem; color: #6366f1;">&#x25BC;</div>
          </div>
          <!-- Result Box -->
          <div style="background: #dcfce7; border: 2px solid #16a34a; border-radius: 10px; padding: 1.25rem; text-align: center; max-width: 420px; margin: 0 auto;">
            <div style="font-weight: 700; color: #15803d; font-size: 1.05rem; margin-bottom: 0.5rem;">Causal ML</div>
            <div style="font-size: 0.85rem; color: #14532d; line-height: 1.5;">
              Use ML for nuisance estimation (prediction subproblems)<br>
              Maintain valid causal identification and inference<br>
              DML &bull; Causal Forests &bull; Heterogeneous Effects
            </div>
          </div>
        </div>

        <p>The intellectual foundation for this approach was laid by Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018) in their landmark paper on Double/Debiased Machine Learning, and by Athey and Imbens (2019) in their survey of machine learning methods for causal inference in economics. The key principle is: let ML handle the prediction, but use carefully designed statistical procedures to ensure that the final causal estimate remains unbiased and has valid confidence intervals.</p>


        <!-- ===================== DOUBLE/DEBIASED MACHINE LEARNING ===================== -->
        <h2 id="dml">Double/Debiased Machine Learning (DML)</h2>

        <h3>The Problem: Regularization Bias</h3>

        <p>Suppose you want to estimate the causal effect of a treatment D on an outcome Y, controlling for a potentially large set of confounders X. The standard approach in economics is to run a linear regression of Y on D and X. But what if X contains dozens or hundreds of variables — demographic characteristics, geographic fixed effects, lagged outcomes, interactions — and you are not sure which ones matter? You might think: "I will just use LASSO to select the relevant controls and then estimate the treatment effect." This seems sensible, but it leads to a subtle and dangerous bias.</p>

        <p>The problem is <strong>regularization bias</strong>. LASSO works by shrinking coefficients toward zero to prevent overfitting. But this shrinkage is indiscriminate — it shrinks the coefficients on the controls <em>and</em> distorts the relationship between D and Y. Even if LASSO correctly identifies which controls matter, the regularization penalty biases the treatment effect estimate. Worse, the standard errors from a penalized regression are not valid for inference, so you cannot construct reliable confidence intervals.</p>

        <p>To see the intuition, consider a concrete example. Suppose the true model is Y = &theta;D + X&beta; + &epsilon;, and D is correlated with X (as it almost always is in observational data). When LASSO penalizes the coefficients on X, it does not fully partial out the variation in D that is explained by X. The remaining confounding variation leaks into the estimate of &theta;, biasing it. This is not a small-sample problem — it persists even with very large datasets because the bias comes from the regularization itself, not from estimation noise.</p>

        <div class="info-box" style="border-left-color: #e53e3e;">
          <h3>The Naive Approach Fails</h3>
          <p>Simply plugging LASSO (or any ML method) into a regression does <strong>not</strong> give you a valid causal estimate. The two main problems are:</p>
          <ul>
            <li><strong>Regularization bias:</strong> Penalized estimators shrink coefficients, which distorts the treatment effect estimate even in large samples.</li>
            <li><strong>Invalid inference:</strong> Standard errors from penalized regressions do not account for the model selection step, so confidence intervals and p-values are unreliable.</li>
          </ul>
        </div>

        <h3>The Solution: Neyman Orthogonality + Cross-Fitting</h3>

        <p>Double/Debiased Machine Learning (DML) solves both problems with two elegant ideas. The first is <strong>Neyman orthogonality</strong> (also called "double residualization" or "partialling out"), and the second is <strong>cross-fitting</strong> (a form of sample splitting).</p>

        <p>The Neyman orthogonality idea is essentially the <span class="code-tooltip" data-tip="The Frisch-Waugh-Lovell theorem states that in a multivariate OLS regression, the coefficient on any variable D can be obtained by (1) regressing Y on all other regressors X to get residuals, (2) regressing D on all other regressors X to get residuals, and (3) regressing the Y-residuals on the D-residuals. DML generalizes this by allowing the regressions in steps (1) and (2) to use ML instead of OLS."><strong>Frisch-Waugh-Lovell theorem</strong></span> generalized to allow machine learning. Instead of putting D and X into a single regression, DML proceeds in three steps:</p>

        <ol>
          <li><strong>Residualize Y:</strong> Use any ML method (LASSO, random forest, neural network) to predict Y from X. Compute the residuals: <code>Y&#x303; = Y - ML&#x302;(X)</code>. These residuals are the part of Y that cannot be predicted by the controls.</li>
          <li><strong>Residualize D:</strong> Use any ML method to predict D from X. Compute the residuals: <code>D&#x303; = D - ML&#x302;(X)</code>. These residuals are the part of D that cannot be predicted by the controls — the "exogenous" variation in treatment.</li>
          <li><strong>Estimate &theta;:</strong> Regress Y&#x303; on D&#x303; using OLS. The coefficient is your treatment effect estimate, and standard OLS inference is valid.</li>
        </ol>

        <p>Why does this work? By residualizing both Y and D, we remove the confounding effect of X from both sides. The ML prediction errors (the residuals) are orthogonal to the controls by construction. Even if the ML models are imperfect (as they always are), the errors in predicting Y and the errors in predicting D are "doubly robust" — small errors in one model are compensated by the other. This is the "double" in Double Machine Learning.</p>

        <p>The second ingredient is <strong>cross-fitting</strong>, which eliminates overfitting bias. If you use the same observations to train the ML model and to compute residuals, the residuals will be artificially small for the training observations (the model has seen them before), which biases the treatment effect estimate. Cross-fitting avoids this by splitting the data into K folds and using an out-of-fold prediction scheme, analogous to cross-validation:</p>

        <!-- Cross-Fitting Diagram -->
        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 12px; padding: 2rem; margin: 1.5rem 0;">
          <h4 style="margin: 0 0 1rem 0; color: #1e293b; text-align: center;">Cross-Fitting with K = 5 Folds</h4>
          <p style="font-size: 0.85rem; color: #475569; text-align: center; margin-bottom: 1.5rem;">For each fold, train the ML models on the other 4 folds and predict residuals for the held-out fold.</p>
          <div style="max-width: 520px; margin: 0 auto;">
            <!-- Fold 1 -->
            <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
              <span style="width: 60px; font-size: 0.8rem; color: #475569; font-weight: 600;">Fold 1:</span>
              <div style="flex: 1; display: grid; grid-template-columns: repeat(5, 1fr); gap: 4px;">
                <div style="background: #fca5a5; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #7f1d1d; font-weight: 600;">Predict</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
              </div>
            </div>
            <!-- Fold 2 -->
            <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
              <span style="width: 60px; font-size: 0.8rem; color: #475569; font-weight: 600;">Fold 2:</span>
              <div style="flex: 1; display: grid; grid-template-columns: repeat(5, 1fr); gap: 4px;">
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #fca5a5; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #7f1d1d; font-weight: 600;">Predict</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
              </div>
            </div>
            <!-- Fold 3 -->
            <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
              <span style="width: 60px; font-size: 0.8rem; color: #475569; font-weight: 600;">Fold 3:</span>
              <div style="flex: 1; display: grid; grid-template-columns: repeat(5, 1fr); gap: 4px;">
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #fca5a5; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #7f1d1d; font-weight: 600;">Predict</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
              </div>
            </div>
            <!-- Fold 4 -->
            <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
              <span style="width: 60px; font-size: 0.8rem; color: #475569; font-weight: 600;">Fold 4:</span>
              <div style="flex: 1; display: grid; grid-template-columns: repeat(5, 1fr); gap: 4px;">
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #fca5a5; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #7f1d1d; font-weight: 600;">Predict</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
              </div>
            </div>
            <!-- Fold 5 -->
            <div style="display: flex; align-items: center; margin-bottom: 0;">
              <span style="width: 60px; font-size: 0.8rem; color: #475569; font-weight: 600;">Fold 5:</span>
              <div style="flex: 1; display: grid; grid-template-columns: repeat(5, 1fr); gap: 4px;">
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #93c5fd; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #1e3a8a;">Train</div>
                <div style="background: #fca5a5; border-radius: 4px; padding: 0.35rem 0; text-align: center; font-size: 0.7rem; color: #7f1d1d; font-weight: 600;">Predict</div>
              </div>
            </div>
          </div>
          <p style="margin: 1.25rem 0 0; font-size: 0.82rem; color: #475569; text-align: center;">
            <span style="display: inline-block; width: 12px; height: 12px; background: #93c5fd; border-radius: 3px; vertical-align: middle; margin-right: 4px;"></span> Used to train ML models &nbsp;&nbsp;
            <span style="display: inline-block; width: 12px; height: 12px; background: #fca5a5; border-radius: 3px; vertical-align: middle; margin-right: 4px;"></span> Held out for residual prediction
          </p>
        </div>

        <p>Each observation's residual is computed from a model that was <em>not</em> trained on that observation. This eliminates the overfitting bias that would otherwise contaminate the treatment effect estimate. The final estimate averages across all folds, and the resulting estimator is root-n consistent, asymptotically normal, and has valid standard errors — the same guarantees you expect from traditional econometric estimators.</p>


        <h3>Implementation</h3>

        <p>Let us see DML in action. We will first implement the three-step procedure manually using LASSO as the ML method, and then show how purpose-built packages automate the process (including cross-fitting). The setup is a partially linear model: Y = &theta;D + g(X) + &epsilon;, where &theta; is the treatment effect of interest and g(X) is an unknown, potentially complex function of the confounders.</p>

        <h4>Manual DML: Step by Step</h4>

        <p>The following code implements the Frisch-Waugh procedure with LASSO. We residualize both the outcome Y and the treatment D on the controls X, then regress the Y-residuals on the D-residuals. Note that this simplified version uses a single sample split for clarity; the package implementations below use proper K-fold cross-fitting.</p>

        <!-- Code Block: ml-dml-1 (Manual DML) -->
        <div class="code-tabs" data-runnable="ml-dml-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">import</span> statsmodels.api <span class="code-keyword">as</span> sm
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="LASSO with built-in cross-validation to select the optimal regularization parameter (alpha). Uses K-fold CV to choose the alpha that minimizes prediction error.">LassoCV</span>
<span class="code-keyword">from</span> sklearn.model_selection <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="Splits data into K consecutive folds. Each fold is used once as validation while the remaining K-1 folds form the training set.">KFold</span>

<span class="code-comment"># ---- Simulate data: Y = theta*D + g(X) + epsilon ----</span>
np.random.<span class="code-function">seed</span>(42)
n = 2000
p = 50  <span class="code-comment"># Number of control variables</span>

X = np.random.<span class="code-function">randn</span>(n, p)
<span class="code-comment"># True treatment effect: theta = 0.5</span>
<span class="code-tooltip" data-tip="The treatment variable is correlated with X (confounding). The first 5 controls affect both D and Y, creating omitted variable bias if not controlled for.">D</span> = 0.5 * X[:, 0] + 0.3 * X[:, 1] + 0.2 * X[:, 2] + np.random.<span class="code-function">randn</span>(n)
<span class="code-tooltip" data-tip="The true data generating process: theta=0.5 is the causal effect. g(X) is a nonlinear function of the first few controls. The remaining controls are irrelevant noise.">Y</span> = 0.5 * D + 2*X[:, 0] + 1.5*X[:, 1]**2 - X[:, 2]*X[:, 3] + np.random.<span class="code-function">randn</span>(n)

<span class="code-comment"># ---- Step 1: Residualize Y on X using LASSO with cross-fitting ----</span>
kf = <span class="code-function">KFold</span>(n_splits=<span class="code-tooltip" data-tip="Number of cross-fitting folds. K=5 is a common default. Chernozhukov et al. (2018) recommend K between 4 and 10.">5</span>, shuffle=True, random_state=42)
Y_resid = np.<span class="code-function">zeros</span>(n)
D_resid = np.<span class="code-function">zeros</span>(n)

<span class="code-keyword">for</span> train_idx, test_idx <span class="code-keyword">in</span> kf.<span class="code-function">split</span>(X):
    <span class="code-comment"># Fit LASSO for Y ~ X on training fold</span>
    lasso_y = <span class="code-function">LassoCV</span>(cv=5, random_state=42)
    lasso_y.<span class="code-function">fit</span>(X[train_idx], Y[train_idx])
    Y_resid[test_idx] = Y[test_idx] - lasso_y.<span class="code-function">predict</span>(X[test_idx])

    <span class="code-comment"># Step 2: Fit LASSO for D ~ X on training fold</span>
    lasso_d = <span class="code-function">LassoCV</span>(cv=5, random_state=42)
    lasso_d.<span class="code-function">fit</span>(X[train_idx], D[train_idx])
    D_resid[test_idx] = D[test_idx] - lasso_d.<span class="code-function">predict</span>(X[test_idx])

<span class="code-comment"># ---- Step 3: Regress Y-residuals on D-residuals ----</span>
<span class="code-tooltip" data-tip="We add a constant because the residuals may not be exactly mean-zero due to estimation error. In practice, the intercept should be close to zero.">final_model</span> = sm.<span class="code-function">OLS</span>(Y_resid, sm.<span class="code-function">add_constant</span>(D_resid)).<span class="code-function">fit</span>()

<span class="code-function">print</span>(<span class="code-string">"=== Manual DML Results ==="</span>)
<span class="code-function">print</span>(<span class="code-string">f"Estimated treatment effect (theta): {final_model.params[1]:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Standard error:                     {final_model.bse[1]:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"95% CI: [{final_model.conf_int()[1][0]:.4f}, {final_model.conf_int()[1][1]:.4f}]"</span>)
<span class="code-function">print</span>(<span class="code-string">f"\nTrue theta: 0.5000"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* ---- Simulate data ----</span>
<span class="code-keyword">clear</span>
<span class="code-keyword">set</span> seed 42
<span class="code-keyword">set</span> obs 2000

<span class="code-comment">* Generate 50 control variables</span>
<span class="code-keyword">forvalues</span> j = 1/50 {
    <span class="code-keyword">gen</span> x`j' = <span class="code-function">rnormal</span>()
}

<span class="code-comment">* Treatment: correlated with controls (confounding)</span>
<span class="code-keyword">gen</span> D = 0.5*x1 + 0.3*x2 + 0.2*x3 + <span class="code-function">rnormal</span>()

<span class="code-comment">* Outcome: true theta = 0.5, nonlinear g(X)</span>
<span class="code-keyword">gen</span> Y = 0.5*D + 2*x1 + 1.5*x2^2 - x3*x4 + <span class="code-function">rnormal</span>()

<span class="code-comment">* ---- Step 1: Residualize Y on X using LASSO ----</span>
<span class="code-comment">* Note: requires lassopack (ssc install lassopack)</span>
<span class="code-keyword">lasso2</span> Y x1-x50, <span class="code-tooltip" data-tip="Cross-validation to select the penalty parameter lambda. lasso2 uses K-fold CV by default.">cvplot</span>
<span class="code-keyword">predict</span> double Y_hat, xb
<span class="code-keyword">gen</span> double Y_resid = Y - Y_hat

<span class="code-comment">* ---- Step 2: Residualize D on X using LASSO ----</span>
<span class="code-keyword">lasso2</span> D x1-x50
<span class="code-keyword">predict</span> double D_hat, xb
<span class="code-keyword">gen</span> double D_resid = D - D_hat

<span class="code-comment">* ---- Step 3: Regress Y-residuals on D-residuals ----</span>
<span class="code-keyword">reg</span> Y_resid D_resid, <span class="code-tooltip" data-tip="Robust (Huber-White) standard errors account for heteroskedasticity in the residuals.">robust</span>

<span class="code-keyword">display</span> <span class="code-string">"True theta: 0.5000"</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="Fits LASSO, Ridge, and Elastic Net using coordinate descent. cv.glmnet performs K-fold cross-validation to select the optimal penalty parameter lambda.">glmnet</span>)

<span class="code-comment"># ---- Simulate data: Y = theta*D + g(X) + epsilon ----</span>
<span class="code-function">set.seed</span>(42)
n &lt;- 2000
p &lt;- 50

X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * p), n, p)
D &lt;- 0.5 * X[, 1] + 0.3 * X[, 2] + 0.2 * X[, 3] + <span class="code-function">rnorm</span>(n)
Y &lt;- 0.5 * D + 2 * X[, 1] + 1.5 * X[, 2]^2 - X[, 3] * X[, 4] + <span class="code-function">rnorm</span>(n)

<span class="code-comment"># ---- Cross-fitting with K = 5 folds ----</span>
K &lt;- 5
folds &lt;- <span class="code-function">sample</span>(<span class="code-function">rep</span>(1:K, length.out = n))
Y_resid &lt;- <span class="code-function">numeric</span>(n)
D_resid &lt;- <span class="code-function">numeric</span>(n)

<span class="code-keyword">for</span> (k <span class="code-keyword">in</span> 1:K) {
  train &lt;- folds != k
  test  &lt;- folds == k

  <span class="code-comment"># Step 1: LASSO for Y ~ X, predict out-of-fold</span>
  cv_y &lt;- <span class="code-function">cv.glmnet</span>(X[train, ], Y[train], <span class="code-tooltip" data-tip="alpha=1 gives LASSO. alpha=0 gives Ridge. Values between 0 and 1 give Elastic Net.">alpha = 1</span>)
  Y_resid[test] &lt;- Y[test] - <span class="code-function">predict</span>(cv_y, X[test, ], s = <span class="code-string">"lambda.min"</span>)

  <span class="code-comment"># Step 2: LASSO for D ~ X, predict out-of-fold</span>
  cv_d &lt;- <span class="code-function">cv.glmnet</span>(X[train, ], D[train], alpha = 1)
  D_resid[test] &lt;- D[test] - <span class="code-function">predict</span>(cv_d, X[test, ], s = <span class="code-string">"lambda.min"</span>)
}

<span class="code-comment"># ---- Step 3: OLS on residuals ----</span>
fit &lt;- <span class="code-function">lm</span>(Y_resid ~ D_resid)
ci  &lt;- <span class="code-function">confint</span>(fit)[<span class="code-string">"D_resid"</span>, ]

<span class="code-function">cat</span>(<span class="code-string">"=== Manual DML Results ===\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Estimated treatment effect (theta):"</span>, <span class="code-function">round</span>(<span class="code-function">coef</span>(fit)[<span class="code-string">"D_resid"</span>], 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Standard error:                    "</span>, <span class="code-function">round</span>(<span class="code-function">summary</span>(fit)$coefficients[<span class="code-string">"D_resid"</span>, <span class="code-string">"Std. Error"</span>], 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"95% CI: ["</span>, <span class="code-function">round</span>(ci[1], 4), <span class="code-string">","</span>, <span class="code-function">round</span>(ci[2], 4), <span class="code-string">"]\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"\nTrue theta: 0.5000\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-dml-1 -->
        <div class="output-simulation" data-output="ml-dml-1" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Manual DML Results ===
Estimated treatment effect (theta): 0.4932
Standard error:                     0.0248
95% CI: [0.4446, 0.5418]

True theta: 0.5000</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-1" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>. reg Y_resid D_resid, robust

Linear regression                               Number of obs     =      2,000
                                                 F(1, 1998)        =    395.21
                                                 Prob > F          =     0.0000
                                                 R-squared         =     0.1726
                                                 Root MSE          =     1.2834

------------------------------------------------------------------------------
             |               Robust
     Y_resid | Coefficient  std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
     D_resid |   .4918327   .0247418    19.88   0.000     .4433069    .5403585
       _cons |  -.0127843   .0286942    -0.45   0.656    -.0690598    .0434912
------------------------------------------------------------------------------

True theta: 0.5000</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-1" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Manual DML Results ===
Estimated treatment effect (theta): 0.4957
Standard error:                     0.0251
95% CI: [ 0.4464 , 0.5449 ]

True theta: 0.5000</pre></div>
        </div>

        <p>The estimated treatment effect is close to the true value of 0.50, and the 95% confidence interval covers it. Notice that we get a valid, interpretable causal estimate even though the true function g(X) contains nonlinear terms (X&#x2082;&#x00B2;) and interactions (X&#x2083; &times; X&#x2084;) that a simple linear regression would miss. The LASSO handles the variable selection flexibly, and the cross-fitting ensures that we do not overfit.</p>

        <h4>Using Dedicated Packages</h4>

        <p>In practice, you should use purpose-built packages that automate cross-fitting, support multiple ML methods, and provide correct standard errors. The <code>DoubleML</code> package (available for both Python and R) and the <code>ddml</code> package for Stata implement the full DML procedure. These packages also support more complex models, such as interactive treatment effects and instrumental variables.</p>

        <!-- Code Block: ml-dml-2 (Using Packages) -->
        <div class="code-tabs" data-runnable="ml-dml-2">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">import</span> doubleml <span class="code-keyword">as</span> dml
<span class="code-keyword">from</span> doubleml <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="DoubleMLData is a container for the data matrices. It stores the outcome Y, treatment D, and controls X in a structure that DoubleML estimators expect.">DoubleMLData</span>
<span class="code-keyword">from</span> doubleml <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="DoubleMLPLR implements the Partially Linear Regression model: Y = theta*D + g(X) + epsilon. It uses cross-fitting automatically and supports any sklearn-compatible ML method.">DoubleMLPLR</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LassoCV
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="Random Forest can also be used as the ML learner in DML. The choice of ML method affects the precision of the estimate but not its validity, as long as the ML method is consistent.">RandomForestRegressor</span>

<span class="code-comment"># ---- Simulate data (same DGP as before) ----</span>
np.random.<span class="code-function">seed</span>(42)
n, p = 2000, 50
X = np.random.<span class="code-function">randn</span>(n, p)
D = 0.5 * X[:, 0] + 0.3 * X[:, 1] + 0.2 * X[:, 2] + np.random.<span class="code-function">randn</span>(n)
Y = 0.5 * D + 2*X[:, 0] + 1.5*X[:, 1]**2 - X[:, 2]*X[:, 3] + np.random.<span class="code-function">randn</span>(n)

<span class="code-comment"># ---- Prepare DoubleML data object ----</span>
dml_data = <span class="code-function">DoubleMLData</span>.from_arrays(x=X, y=Y, d=D)

<span class="code-comment"># ---- DML with LASSO ----</span>
ml_l = <span class="code-function">LassoCV</span>(cv=5)       <span class="code-comment"># ML model for E[Y|X]</span>
ml_m = <span class="code-function">LassoCV</span>(cv=5)       <span class="code-comment"># ML model for E[D|X]</span>

dml_plr_lasso = <span class="code-function">DoubleMLPLR</span>(
    dml_data,
    ml_l=ml_l,                <span class="code-comment"># Learner for Y ~ X</span>
    ml_m=ml_m,                <span class="code-comment"># Learner for D ~ X</span>
    <span class="code-tooltip" data-tip="Number of cross-fitting folds. The data is split into n_folds parts; each part takes a turn being the prediction set.">n_folds</span>=5,
    <span class="code-tooltip" data-tip="Number of times to repeat the cross-fitting procedure with different random splits, then average. Reduces sensitivity to a particular fold assignment.">n_rep</span>=1,
    score=<span class="code-string">'partialling out'</span>  <span class="code-comment"># Frisch-Waugh approach</span>
)
dml_plr_lasso.<span class="code-function">fit</span>()

<span class="code-function">print</span>(<span class="code-string">"=== DML with LASSO (DoubleML package) ==="</span>)
<span class="code-function">print</span>(dml_plr_lasso.<span class="code-function">summary</span>)

<span class="code-comment"># ---- DML with Random Forest ----</span>
ml_l_rf = <span class="code-function">RandomForestRegressor</span>(n_estimators=200, max_depth=5, random_state=42)
ml_m_rf = <span class="code-function">RandomForestRegressor</span>(n_estimators=200, max_depth=5, random_state=42)

dml_plr_rf = <span class="code-function">DoubleMLPLR</span>(dml_data, ml_l=ml_l_rf, ml_m=ml_m_rf, n_folds=5)
dml_plr_rf.<span class="code-function">fit</span>()

<span class="code-function">print</span>(<span class="code-string">"\n=== DML with Random Forest ==="</span>)
<span class="code-function">print</span>(dml_plr_rf.<span class="code-function">summary</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* ---- DML using ddml (Chernozhukov et al.) ----</span>
<span class="code-comment">* Install: ssc install ddml</span>
<span class="code-comment">* Also requires: ssc install pystacked (or lassopack)</span>

<span class="code-comment">* Using simulated data from before (Y, D, x1-x50)</span>

<span class="code-comment">* ---- Method 1: ddml with LASSO learners ----</span>
<span class="code-keyword">ddml</span> init, <span class="code-tooltip" data-tip="kfolds specifies the number of cross-fitting folds for the DML procedure.">kfolds(5)</span> <span class="code-tooltip" data-tip="reps sets the number of cross-fitting repetitions. Multiple repetitions reduce variability from the random fold assignment.">reps(1)</span>

<span class="code-comment">* Specify the outcome model: Y ~ X using LASSO</span>
<span class="code-keyword">ddml</span> E[Y], <span class="code-tooltip" data-tip="learner(pystacked) uses the pystacked command as the ML backend. Alternatively, use learner(cv_lasso) for cross-validated LASSO from lassopack.">learner(cv_lasso)</span>: Y x1-x50

<span class="code-comment">* Specify the treatment model: D ~ X using LASSO</span>
<span class="code-keyword">ddml</span> E[D], learner(cv_lasso): D x1-x50

<span class="code-comment">* Cross-fit and estimate</span>
<span class="code-keyword">ddml</span> crossfit
<span class="code-keyword">ddml</span> estimate, <span class="code-tooltip" data-tip="The 'robust' option uses heteroskedasticity-robust standard errors for the final OLS step.">robust</span>

<span class="code-keyword">display</span> <span class="code-string">"True theta: 0.5000"</span>

<span class="code-comment">* ---- Method 2: ddml with stacked learners ----</span>
<span class="code-comment">* ddml supports stacking multiple ML methods</span>
<span class="code-keyword">ddml</span> init, kfolds(5) reps(3)

<span class="code-keyword">ddml</span> E[Y], learner(pystacked) ///
    method(lassocv rf): Y x1-x50

<span class="code-keyword">ddml</span> E[D], learner(pystacked) ///
    method(lassocv rf): D x1-x50

<span class="code-keyword">ddml</span> crossfit
<span class="code-keyword">ddml</span> estimate, robust</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="The R implementation of the DoubleML framework. Provides R6 classes for partially linear models, interactive models, and IV models with cross-fitting.">DoubleML</span>)
<span class="code-keyword">library</span>(mlr3)
<span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="Provides ML learners for mlr3, including LASSO (cv_glmnet), random forests, and boosting.">mlr3learners</span>)

<span class="code-comment"># ---- Simulate data (same DGP) ----</span>
<span class="code-function">set.seed</span>(42)
n &lt;- 2000; p &lt;- 50
X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * p), n, p)
<span class="code-function">colnames</span>(X) &lt;- <span class="code-function">paste0</span>(<span class="code-string">"X"</span>, 1:p)
D &lt;- 0.5 * X[, 1] + 0.3 * X[, 2] + 0.2 * X[, 3] + <span class="code-function">rnorm</span>(n)
Y &lt;- 0.5 * D + 2 * X[, 1] + 1.5 * X[, 2]^2 - X[, 3] * X[, 4] + <span class="code-function">rnorm</span>(n)

<span class="code-comment"># ---- Prepare DoubleML data ----</span>
df &lt;- <span class="code-function">data.frame</span>(Y = Y, D = D, X)
dml_data &lt;- <span class="code-function">DoubleMLData$new</span>(
  df,
  y_col = <span class="code-string">"Y"</span>,
  d_cols = <span class="code-string">"D"</span>,
  x_cols = <span class="code-function">paste0</span>(<span class="code-string">"X"</span>, 1:p)
)

<span class="code-comment"># ---- DML with LASSO ----</span>
ml_l &lt;- <span class="code-function">lrn</span>(<span class="code-string">"regr.cv_glmnet"</span>, <span class="code-tooltip" data-tip="s = 'lambda.min' selects the lambda that gives the minimum cross-validation error, rather than the more conservative lambda.1se.">s = <span class="code-string">"lambda.min"</span></span>)
ml_m &lt;- <span class="code-function">lrn</span>(<span class="code-string">"regr.cv_glmnet"</span>, s = <span class="code-string">"lambda.min"</span>)

dml_plr &lt;- <span class="code-function">DoubleMLPLR$new</span>(
  dml_data,
  ml_l = ml_l,          <span class="code-comment"># Learner for E[Y|X]</span>
  ml_m = ml_m,          <span class="code-comment"># Learner for E[D|X]</span>
  n_folds = 5,
  n_rep = 1,
  score = <span class="code-string">"partialling out"</span>
)
dml_plr$<span class="code-function">fit</span>()

<span class="code-function">cat</span>(<span class="code-string">"=== DML with LASSO (DoubleML package) ===\n"</span>)
<span class="code-function">print</span>(dml_plr)

<span class="code-comment"># ---- DML with Random Forest ----</span>
ml_l_rf &lt;- <span class="code-function">lrn</span>(<span class="code-string">"regr.ranger"</span>, num.trees = 200, max.depth = 5)
ml_m_rf &lt;- <span class="code-function">lrn</span>(<span class="code-string">"regr.ranger"</span>, num.trees = 200, max.depth = 5)

dml_plr_rf &lt;- <span class="code-function">DoubleMLPLR$new</span>(
  dml_data,
  ml_l = ml_l_rf,
  ml_m = ml_m_rf,
  n_folds = 5
)
dml_plr_rf$<span class="code-function">fit</span>()

<span class="code-function">cat</span>(<span class="code-string">"\n=== DML with Random Forest ===\n"</span>)
<span class="code-function">print</span>(dml_plr_rf)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-dml-2 -->
        <div class="output-simulation" data-output="ml-dml-2" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== DML with LASSO (DoubleML package) ===
       coef   std err         t     P>|t|     2.5 %    97.5 %
d  0.490741  0.024513  20.01893       0.0  0.442697  0.538785

=== DML with Random Forest ===
       coef   std err         t     P>|t|     2.5 %    97.5 %
d  0.503218  0.025847  19.46724       0.0  0.452559  0.553877</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-2" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>DDML estimation results
=======================================================================
 Y equation: E[Y|X]      learner: cv_lasso     Cross-fitted: Yes
 D equation: E[D|X]      learner: cv_lasso     Cross-fitted: Yes
 Number of folds: 5      Number of repetitions: 1
-----------------------------------------------------------------------
             | Coefficient  Std. err.      t    P>|t|    [95% conf. interval]
-------------+-------------------------------------------------------------
           D |   .4891524   .0252617    19.36   0.000    .4396404   .5386644
-----------------------------------------------------------------------

True theta: 0.5000

DDML estimation results (stacked learners)
=======================================================================
 Y equation: E[Y|X]      learner: pystacked    Cross-fitted: Yes
 D equation: E[D|X]      learner: pystacked    Cross-fitted: Yes
 Number of folds: 5      Number of repetitions: 3
-----------------------------------------------------------------------
             | Coefficient  Std. err.      t    P>|t|    [95% conf. interval]
-------------+-------------------------------------------------------------
           D |   .5013842   .0243119    20.62   0.000    .4537337   .5490347
-----------------------------------------------------------------------</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-2" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== DML with LASSO (DoubleML package) ===
================= DoubleMLPLR Object ==================
       Estimate. Std. Error t value Pr(>|t|)
D       0.49238    0.02487  19.798   <2e-16 ***

=== DML with Random Forest ===
================= DoubleMLPLR Object ==================
       Estimate. Std. Error t value Pr(>|t|)
D       0.50478    0.02541  19.866   <2e-16 ***</pre></div>
        </div>

        <p>Both the LASSO and Random Forest versions recover the true treatment effect (&theta; = 0.50) with tight confidence intervals. The <code>DoubleML</code> package handles cross-fitting, standard error computation, and inference automatically. Notice that the choice of ML learner (LASSO vs. Random Forest) affects the point estimate slightly but both are centered on the truth — this illustrates the double robustness property of DML.</p>

        <p>A key practical advantage of DML is that you can swap in any ML method — gradient boosting, neural networks, ensemble stacking — without changing the overall procedure. The causal validity of the estimate comes from the DML framework (orthogonalization + cross-fitting), not from the specific ML method used. The ML method only affects the <em>efficiency</em> of the estimate: a better ML model produces smaller residuals, which translates into tighter confidence intervals.</p>


        <!-- ===================== CAUSAL FORESTS ===================== -->
        <h2 id="causal-forests">Causal Forests</h2>

        <h3>Heterogeneous Treatment Effects</h3>

        <p>DML gives you a single number: the <strong>average treatment effect</strong> (ATE). But in many applications, the effect of a treatment varies across individuals. A job training program might help low-skilled workers more than high-skilled ones. A drug might work better for younger patients. A policy might benefit urban areas more than rural ones. These differences are called <strong>heterogeneous treatment effects</strong> (HTEs), and discovering them is one of the most important applications of causal ML.</p>

        <p>The quantity of interest is the <strong>Conditional Average Treatment Effect</strong> (CATE), defined as:</p>

        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1.25rem; margin: 1.25rem 0; text-align: center; font-size: 1.1rem;">
          <strong>&tau;(x) = E[Y(1) - Y(0) | X = x]</strong>
        </div>

        <p>where Y(1) and Y(0) are the potential outcomes under treatment and control, and X is a vector of observable characteristics. The CATE &tau;(x) tells you the expected treatment effect for an individual with characteristics x. If &tau;(x) is constant for all x, then the effect is homogeneous and the ATE is sufficient. But if &tau;(x) varies, we want to estimate the entire function &tau;(x), not just its average.</p>

        <p><strong>Causal Forests</strong>, introduced by Wager and Athey (2018), are the leading method for estimating CATEs. A causal forest is a modified random forest where each tree is built to maximize the difference in treatment effects across its splits, rather than to minimize prediction error. The key insight is that the splitting criterion targets treatment effect heterogeneity directly: at each node, the algorithm searches for the covariate split that creates the largest difference in estimated treatment effects between the two child nodes.</p>

        <h3>Honest Estimation</h3>

        <p>A critical innovation in causal forests is <strong>honest estimation</strong>, which addresses the overfitting problem that would arise if the same data were used both to determine the tree structure and to estimate the treatment effects within leaves. Honest estimation splits the training sample for each tree into two halves:</p>

        <ol>
          <li><strong>Splitting sample:</strong> Used to determine the tree structure (which variables to split on and where to place the cutoffs).</li>
          <li><strong>Estimation sample:</strong> Used to estimate the treatment effect within each leaf of the tree.</li>
        </ol>

        <p>This separation ensures that the treatment effect estimates within each leaf are unbiased, because the observations used for estimation played no role in determining which leaf they belong to. Without honesty, the tree would tend to place splits that create spuriously large treatment effect differences, leading to overconfident and biased CATEs.</p>

        <p>The forest aggregates many honest trees, each built on a bootstrap subsample, and the final CATE estimate for a new observation x is a weighted average of the treatment effect estimates from the leaves that x falls into across all trees. Wager and Athey (2018) prove that this estimator is asymptotically normal, which means you can construct valid confidence intervals for individual-level treatment effects — a remarkable result for a tree-based method.</p>


        <!-- Code Block: ml-dml-3 (Causal Forest) -->
        <div class="code-tabs" data-runnable="ml-dml-3">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> econml.dml <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="CausalForestDML combines DML (double residualization) with a causal forest for the final stage. It first residualizes Y and D using any ML method, then fits a causal forest on the residuals to estimate heterogeneous effects.">CausalForestDML</span>
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> RandomForestRegressor

<span class="code-comment"># ---- Simulate data with heterogeneous treatment effects ----</span>
np.random.<span class="code-function">seed</span>(42)
n = 2000
p = 10

X = np.random.<span class="code-function">randn</span>(n, p)

<span class="code-comment"># Treatment effect varies with X[:, 0]: tau(x) = 1 + 2*x0</span>
<span class="code-tooltip" data-tip="The true CATE function. Individuals with X0 > 0 have larger treatment effects (tau > 1), and those with X0 < 0 have smaller effects (tau < 1). The ATE is 1.0.">tau</span> = 1 + 2 * X[:, 0]
D = (np.random.<span class="code-function">randn</span>(n) + 0.5 * X[:, 0] > 0).<span class="code-function">astype</span>(<span class="code-keyword">float</span>)
Y = tau * D + X[:, 0] + 0.5 * X[:, 1]**2 + np.random.<span class="code-function">randn</span>(n)

<span class="code-comment"># ---- Fit Causal Forest via DML ----</span>
cf = <span class="code-function">CausalForestDML</span>(
    <span class="code-tooltip" data-tip="The ML model used to predict Y from X (nuisance model for outcome). Any sklearn regressor works here.">model_y</span>=<span class="code-function">RandomForestRegressor</span>(n_estimators=200, max_depth=5, random_state=42),
    <span class="code-tooltip" data-tip="The ML model used to predict D from X (nuisance model for treatment propensity). For binary D, you can also use a classifier.">model_t</span>=<span class="code-function">RandomForestRegressor</span>(n_estimators=200, max_depth=5, random_state=42),
    <span class="code-tooltip" data-tip="Number of trees in the causal forest. More trees improve the stability of estimates but increase computation time.">n_estimators</span>=1000,
    <span class="code-tooltip" data-tip="Minimum number of observations in each leaf. Larger values produce smoother CATE estimates but may miss fine-grained heterogeneity.">min_samples_leaf</span>=5,
    random_state=42
)

cf.<span class="code-function">fit</span>(Y, D, X=X)

<span class="code-comment"># ---- Estimate CATEs ----</span>
cate_hat = cf.<span class="code-function">effect</span>(X)

<span class="code-comment"># ---- Evaluate: compare estimated vs true CATEs ----</span>
<span class="code-function">print</span>(<span class="code-string">"=== Causal Forest Results ==="</span>)
<span class="code-function">print</span>(<span class="code-string">f"Mean estimated CATE:  {cate_hat.mean():.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Mean true CATE:      {tau.mean():.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Correlation(est, true): {np.corrcoef(cate_hat, tau)[0,1]:.4f}"</span>)

<span class="code-comment"># ---- Inference for a specific point ----</span>
point_x = X[[0], :]
effect_inf = cf.<span class="code-function">effect_inference</span>(point_x)
<span class="code-function">print</span>(<span class="code-string">f"\nCate at X[0]:  est={effect_inf.point_estimate[0]:.4f}, "</span>
      <span class="code-string">f"95% CI=[{effect_inf.conf_int()[0][0]:.4f}, {effect_inf.conf_int()[1][0]:.4f}]"</span>)
<span class="code-function">print</span>(<span class="code-string">f"True tau(X[0]): {tau[0]:.4f}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* ---- Causal Forest via econml (Python integration) ----</span>
<span class="code-comment">* Stata does not have a native causal forest command,</span>
<span class="code-comment">* but you can call Python from Stata 16+ using the python: block.</span>

<span class="code-keyword">clear</span> all
<span class="code-keyword">set</span> seed 42
<span class="code-keyword">set</span> obs 2000

<span class="code-comment">* ---- Simulate data ----</span>
<span class="code-keyword">forvalues</span> j = 1/10 {
    <span class="code-keyword">gen</span> double x`j' = <span class="code-function">rnormal</span>()
}

<span class="code-comment">* Heterogeneous treatment effect: tau(x) = 1 + 2*x1</span>
<span class="code-keyword">gen</span> double tau = 1 + 2 * x1
<span class="code-keyword">gen</span> double D = (<span class="code-function">rnormal</span>() + 0.5 * x1 > 0)
<span class="code-keyword">gen</span> double Y = tau * D + x1 + 0.5 * x2^2 + <span class="code-function">rnormal</span>()

<span class="code-comment">* ---- Call Python for causal forest ----</span>
<span class="code-keyword">python:</span>
<span class="code-keyword">from</span> sfi <span class="code-keyword">import</span> Data
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> econml.dml <span class="code-keyword">import</span> CausalForestDML
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> RandomForestRegressor

<span class="code-comment"># Pull data from Stata</span>
n = Data.getObsTotal()
X = np.column_stack([np.array(Data.getVarFloat(<span class="code-string">f'x{j}'</span>)) <span class="code-keyword">for</span> j <span class="code-keyword">in</span> range(1, 11)])
D = np.array(Data.getVarFloat(<span class="code-string">'D'</span>))
Y = np.array(Data.getVarFloat(<span class="code-string">'Y'</span>))
tau_true = np.array(Data.getVarFloat(<span class="code-string">'tau'</span>))

<span class="code-comment"># Fit causal forest</span>
cf = CausalForestDML(
    model_y=RandomForestRegressor(n_estimators=200, max_depth=5, random_state=42),
    model_t=RandomForestRegressor(n_estimators=200, max_depth=5, random_state=42),
    n_estimators=1000,
    min_samples_leaf=5,
    random_state=42
)
cf.fit(Y, D, X=X)

cate_hat = cf.effect(X)
print(<span class="code-string">f"=== Causal Forest Results ==="</span>)
print(<span class="code-string">f"Mean estimated CATE:    {cate_hat.mean():.4f}"</span>)
print(<span class="code-string">f"Mean true CATE:         {tau_true.mean():.4f}"</span>)
print(<span class="code-string">f"Correlation(est, true): {np.corrcoef(cate_hat, tau_true)[0,1]:.4f}"</span>)

<span class="code-comment"># Push estimated CATEs back to Stata</span>
Data.addVarFloat(<span class="code-string">'cate_hat'</span>)
<span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(n):
    Data.storeVar(<span class="code-string">'cate_hat'</span>, i, cate_hat[i])
<span class="code-keyword">end</span>

<span class="code-keyword">summarize</span> cate_hat tau</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="The grf (Generalized Random Forests) package by Athey, Tibshirani, and Wager implements causal forests, instrumental forests, and more. It provides point estimates and valid confidence intervals for CATEs.">grf</span>)

<span class="code-comment"># ---- Simulate data with heterogeneous treatment effects ----</span>
<span class="code-function">set.seed</span>(42)
n &lt;- 2000
p &lt;- 10

X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * p), n, p)

<span class="code-comment"># Treatment effect varies with X[,1]: tau(x) = 1 + 2*x1</span>
tau &lt;- 1 + 2 * X[, 1]
D &lt;- <span class="code-function">as.numeric</span>(<span class="code-function">rnorm</span>(n) + 0.5 * X[, 1] > 0)
Y &lt;- tau * D + X[, 1] + 0.5 * X[, 2]^2 + <span class="code-function">rnorm</span>(n)

<span class="code-comment"># ---- Fit causal forest ----</span>
cf &lt;- <span class="code-function">causal_forest</span>(
  X, Y, D,
  <span class="code-tooltip" data-tip="Number of trees in the forest. More trees improve precision and stability. Default is 2000.">num.trees</span> = 2000,
  <span class="code-tooltip" data-tip="When honesty = TRUE, each tree uses one half of its subsample for splitting and the other half for estimation. This prevents overfitting of the treatment effect estimates.">honesty</span> = <span class="code-keyword">TRUE</span>,
  <span class="code-tooltip" data-tip="Fraction of observations used for the splitting half in honest estimation. Default 0.5 means an even split.">honesty.fraction</span> = 0.5,
  seed = 42
)

<span class="code-comment"># ---- Estimate CATEs ----</span>
cate_hat &lt;- <span class="code-function">predict</span>(cf)$predictions

<span class="code-comment"># ---- Results ----</span>
<span class="code-function">cat</span>(<span class="code-string">"=== Causal Forest Results ===\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Mean estimated CATE:   "</span>, <span class="code-function">round</span>(<span class="code-function">mean</span>(cate_hat), 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Mean true CATE:        "</span>, <span class="code-function">round</span>(<span class="code-function">mean</span>(tau), 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Correlation(est, true):"</span>, <span class="code-function">round</span>(<span class="code-function">cor</span>(cate_hat, tau), 4), <span class="code-string">"\n"</span>)

<span class="code-comment"># ---- ATE with confidence interval ----</span>
ate &lt;- <span class="code-function">average_treatment_effect</span>(cf)
<span class="code-function">cat</span>(<span class="code-string">"\nATE estimate:"</span>, <span class="code-function">round</span>(ate[1], 4),
    <span class="code-string">"  SE:"</span>, <span class="code-function">round</span>(ate[2], 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"95% CI: ["</span>, <span class="code-function">round</span>(ate[1] - 1.96 * ate[2], 4), <span class="code-string">","</span>,
    <span class="code-function">round</span>(ate[1] + 1.96 * ate[2], 4), <span class="code-string">"]\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"True ATE:"</span>, <span class="code-function">round</span>(<span class="code-function">mean</span>(tau), 4), <span class="code-string">"\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-dml-3 -->
        <div class="output-simulation" data-output="ml-dml-3" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Causal Forest Results ===
Mean estimated CATE:  1.0042
Mean true CATE:      1.0103
Correlation(est, true): 0.8637

Cate at X[0]:  est=1.6821, 95% CI=[0.9348, 2.4294]
True tau(X[0]): 1.9934</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-3" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Causal Forest Results ===
Mean estimated CATE:    1.0076
Mean true CATE:         1.0103
Correlation(est, true): 0.8584

    Variable |        Obs        Mean    Std. dev.       Min        Max
-------------+---------------------------------------------------------
    cate_hat |      2,000    1.007643    1.184726  -2.348912   4.815237
         tau |      2,000    1.010346    1.996041  -5.392147   7.341528</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-3" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Causal Forest Results ===
Mean estimated CATE:    0.9987
Mean true CATE:         1.0103
Correlation(est, true): 0.8712

ATE estimate: 1.0052   SE: 0.0487
95% CI: [ 0.9097 , 1.1007 ]
True ATE: 1.0103</pre></div>
        </div>

        <p>The causal forest recovers the average treatment effect well (close to the true ATE of ~1.0) and, crucially, it captures the <em>heterogeneity</em> in treatment effects — the correlation between estimated and true CATEs is high (~0.87). This means the forest successfully identifies who benefits more and who benefits less from treatment. The R implementation using <code>grf</code> is particularly well-developed and provides built-in functions for ATEs, variable importance, and calibration tests.</p>


        <!-- Code Block: ml-dml-4 (Plot Heterogeneous Effects) -->
        <h4>Visualizing Treatment Effect Heterogeneity</h4>

        <p>One of the main advantages of causal forests is that they produce individual-level treatment effect estimates, which we can plot to understand how the effect varies across the population.</p>

        <div class="code-tabs" data-runnable="ml-dml-4">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> matplotlib.pyplot <span class="code-keyword">as</span> plt
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># ---- (Assumes cf, cate_hat, tau, X from previous block) ----</span>

fig, axes = plt.<span class="code-function">subplots</span>(1, 2, figsize=(12, 5))

<span class="code-comment"># Plot 1: Estimated vs True CATEs</span>
axes[0].<span class="code-function">scatter</span>(tau, cate_hat, alpha=0.3, s=10, color=<span class="code-string">'steelblue'</span>)
axes[0].<span class="code-function">plot</span>([tau.<span class="code-function">min</span>(), tau.<span class="code-function">max</span>()],
             [tau.<span class="code-function">min</span>(), tau.<span class="code-function">max</span>()],
             <span class="code-string">'r--'</span>, lw=2, label=<span class="code-string">'45-degree line'</span>)
axes[0].<span class="code-function">set_xlabel</span>(<span class="code-string">'True CATE'</span>)
axes[0].<span class="code-function">set_ylabel</span>(<span class="code-string">'Estimated CATE'</span>)
axes[0].<span class="code-function">set_title</span>(<span class="code-string">'Estimated vs. True Treatment Effects'</span>)
axes[0].<span class="code-function">legend</span>()

<span class="code-comment"># Plot 2: CATE as a function of X0</span>
<span class="code-tooltip" data-tip="Sorting by X[:,0] lets us see how the estimated CATE varies along the dimension where we built in heterogeneity (tau = 1 + 2*X0).">sort_idx</span> = np.<span class="code-function">argsort</span>(X[:, 0])
axes[1].<span class="code-function">scatter</span>(X[sort_idx, 0], cate_hat[sort_idx], alpha=0.2, s=8, color=<span class="code-string">'steelblue'</span>, label=<span class="code-string">'Estimated'</span>)
axes[1].<span class="code-function">plot</span>(X[sort_idx, 0], tau[sort_idx], <span class="code-string">'r-'</span>, lw=2, label=<span class="code-string">'True tau(x)'</span>)
axes[1].<span class="code-function">set_xlabel</span>(<span class="code-string">'X[0]'</span>)
axes[1].<span class="code-function">set_ylabel</span>(<span class="code-string">'Treatment Effect'</span>)
axes[1].<span class="code-function">set_title</span>(<span class="code-string">'CATE by X[0] (Main Effect Modifier)'</span>)
axes[1].<span class="code-function">legend</span>()

plt.<span class="code-function">tight_layout</span>()
plt.<span class="code-function">savefig</span>(<span class="code-string">'cate_heterogeneity.png'</span>, dpi=150, bbox_inches=<span class="code-string">'tight'</span>)
plt.<span class="code-function">show</span>()
<span class="code-function">print</span>(<span class="code-string">"Plot saved: cate_heterogeneity.png"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* ---- Plot heterogeneous effects (requires cate_hat from previous block) ----</span>

<span class="code-comment">* Scatter: estimated CATE vs true tau</span>
<span class="code-keyword">twoway</span> (<span class="code-keyword">scatter</span> cate_hat tau, msize(tiny) mcolor(blue%30)) ///
       (<span class="code-keyword">function</span> y=x, range(tau) lcolor(red) lwidth(medium)), ///
       <span class="code-tooltip" data-tip="Legend labels for the scatter and the 45-degree line.">legend</span>(label(1 <span class="code-string">"Estimated CATE"</span>) label(2 <span class="code-string">"45-degree line"</span>)) ///
       xtitle(<span class="code-string">"True CATE"</span>) ytitle(<span class="code-string">"Estimated CATE"</span>) ///
       title(<span class="code-string">"Estimated vs. True Treatment Effects"</span>)
<span class="code-keyword">graph export</span> <span class="code-string">"cate_scatter.png"</span>, replace

<span class="code-comment">* CATE by X1</span>
<span class="code-keyword">twoway</span> (<span class="code-keyword">scatter</span> cate_hat x1, msize(tiny) mcolor(blue%30)) ///
       (<span class="code-keyword">line</span> tau x1, sort lcolor(red) lwidth(medium)), ///
       legend(label(1 <span class="code-string">"Estimated"</span>) label(2 <span class="code-string">"True tau(x)"</span>)) ///
       xtitle(<span class="code-string">"X1"</span>) ytitle(<span class="code-string">"Treatment Effect"</span>) ///
       title(<span class="code-string">"CATE by X1 (Main Effect Modifier)"</span>)
<span class="code-keyword">graph export</span> <span class="code-string">"cate_by_x1.png"</span>, replace</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># ---- (Assumes cf, cate_hat, tau, X from previous block) ----</span>

<span class="code-function">par</span>(mfrow = <span class="code-function">c</span>(1, 2))

<span class="code-comment"># Plot 1: Estimated vs True CATEs</span>
<span class="code-function">plot</span>(tau, cate_hat, pch = 16, cex = 0.4,
     col = <span class="code-function">adjustcolor</span>(<span class="code-string">"steelblue"</span>, 0.3),
     xlab = <span class="code-string">"True CATE"</span>, ylab = <span class="code-string">"Estimated CATE"</span>,
     main = <span class="code-string">"Estimated vs. True Treatment Effects"</span>)
<span class="code-function">abline</span>(0, 1, col = <span class="code-string">"red"</span>, lwd = 2, lty = 2)
<span class="code-function">legend</span>(<span class="code-string">"topleft"</span>, <span class="code-string">"45-degree line"</span>, col = <span class="code-string">"red"</span>, lwd = 2, lty = 2)

<span class="code-comment"># Plot 2: CATE by X[,1]</span>
ord &lt;- <span class="code-function">order</span>(X[, 1])
<span class="code-function">plot</span>(X[ord, 1], cate_hat[ord], pch = 16, cex = 0.4,
     col = <span class="code-function">adjustcolor</span>(<span class="code-string">"steelblue"</span>, 0.3),
     xlab = <span class="code-string">"X[,1]"</span>, ylab = <span class="code-string">"Treatment Effect"</span>,
     main = <span class="code-string">"CATE by X[,1] (Main Effect Modifier)"</span>)
<span class="code-function">lines</span>(X[ord, 1], tau[ord], col = <span class="code-string">"red"</span>, lwd = 2)
<span class="code-function">legend</span>(<span class="code-string">"topleft"</span>, <span class="code-function">c</span>(<span class="code-string">"Estimated"</span>, <span class="code-string">"True tau(x)"</span>),
       col = <span class="code-function">c</span>(<span class="code-string">"steelblue"</span>, <span class="code-string">"red"</span>), pch = <span class="code-function">c</span>(16, <span class="code-keyword">NA</span>), lwd = <span class="code-function">c</span>(<span class="code-keyword">NA</span>, 2))

<span class="code-function">cat</span>(<span class="code-string">"Plots generated.\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-dml-4 -->
        <div class="output-simulation" data-output="ml-dml-4" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Plot saved: cate_heterogeneity.png

[Two-panel figure displayed]
Left: Scatter of estimated vs. true CATEs clusters tightly around 45-degree line.
Right: Estimated CATEs track the linear true tau(x) = 1 + 2*X0 pattern closely.</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-4" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>file cate_scatter.png saved
file cate_by_x1.png saved

[Two graphs exported]
Graph 1: Estimated vs. true CATE scatter follows 45-degree line.
Graph 2: Estimated CATEs rise linearly with X1, matching true tau = 1 + 2*X1.</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-4" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Plots generated.

[Two-panel figure displayed]
Left: Scatter of estimated vs. true CATEs — points cluster around 45-degree line.
Right: Estimated CATEs increase linearly with X[,1], tracking true tau(x) = 1 + 2*X1.</pre></div>
        </div>

        <p>The scatter plot of estimated versus true CATEs shows that the causal forest captures the systematic variation in treatment effects well. The plot of CATEs against X<sub>1</sub> (the variable that drives heterogeneity in our simulation) reveals the linear relationship &tau;(x) = 1 + 2x<sub>1</sub> that we built into the data. In practice, where you do not know the true CATE, these plots help identify which covariates are the strongest effect modifiers and guide policy targeting.</p>


        <!-- ===================== POLICY LEARNING ===================== -->
        <h2 id="policy-learning">Policy Learning</h2>

        <p>Once you have estimated heterogeneous treatment effects, a natural next question is: <strong>who should be treated?</strong> If a program has positive effects for some people and negative effects for others, or if the budget constrains you to treat only a subset, you want an <strong>optimal treatment rule</strong> — a function that maps individual characteristics to a treatment recommendation.</p>

        <p>Athey and Wager (2021) formalize this as a <strong>policy learning</strong> problem. The idea is to find a treatment assignment policy &pi;(x) that maximizes the expected welfare of the population. If &tau;(x) is the CATE, then the optimal unconstrained policy is simply "treat if &tau;(x) > 0." But in practice, you may want interpretable rules (e.g., shallow decision trees) or may face constraints (e.g., treat at most 30% of the population). The <code>policytree</code> package in R and the <code>econml</code> package in Python implement these methods.</p>


        <!-- Code Block: ml-dml-5 (Policy Learning) -->
        <div class="code-tabs" data-runnable="ml-dml-5">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> econml.dml <span class="code-keyword">import</span> CausalForestDML
<span class="code-keyword">from</span> sklearn.ensemble <span class="code-keyword">import</span> RandomForestRegressor
<span class="code-keyword">from</span> sklearn.tree <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="DecisionTreeClassifier is used here to learn a simple, interpretable policy rule. The tree maps covariates X to treatment decisions {0, 1}.">DecisionTreeClassifier</span>

<span class="code-comment"># ---- Simulate data (same DGP) ----</span>
np.random.<span class="code-function">seed</span>(42)
n = 2000; p = 10
X = np.random.<span class="code-function">randn</span>(n, p)
tau = 1 + 2 * X[:, 0]
D = (np.random.<span class="code-function">randn</span>(n) + 0.5 * X[:, 0] > 0).<span class="code-function">astype</span>(<span class="code-keyword">float</span>)
Y = tau * D + X[:, 0] + 0.5 * X[:, 1]**2 + np.random.<span class="code-function">randn</span>(n)

<span class="code-comment"># ---- Fit Causal Forest ----</span>
cf = <span class="code-function">CausalForestDML</span>(
    model_y=<span class="code-function">RandomForestRegressor</span>(n_estimators=200, max_depth=5, random_state=42),
    model_t=<span class="code-function">RandomForestRegressor</span>(n_estimators=200, max_depth=5, random_state=42),
    n_estimators=1000, min_samples_leaf=5, random_state=42
)
cf.<span class="code-function">fit</span>(Y, D, X=X)
cate_hat = cf.<span class="code-function">effect</span>(X)

<span class="code-comment"># ---- Learn a simple policy (depth-2 tree) ----</span>
<span class="code-tooltip" data-tip="The policy labels: treat if estimated CATE > 0. We then train a shallow decision tree to approximate this rule with interpretable splits.">policy_labels</span> = (cate_hat > 0).<span class="code-function">astype</span>(<span class="code-keyword">int</span>)
policy_tree = <span class="code-function">DecisionTreeClassifier</span>(max_depth=2, random_state=42)
policy_tree.<span class="code-function">fit</span>(X, policy_labels)

<span class="code-comment"># ---- Evaluate policy ----</span>
recommended = policy_tree.<span class="code-function">predict</span>(X)
<span class="code-function">print</span>(<span class="code-string">"=== Policy Learning Results ==="</span>)
<span class="code-function">print</span>(<span class="code-string">f"Share recommended for treatment: {recommended.mean():.3f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Mean CATE among treated group:   {cate_hat[recommended == 1].mean():.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Mean CATE among untreated group: {cate_hat[recommended == 0].mean():.4f}"</span>)

<span class="code-comment"># ---- Policy tree splits ----</span>
<span class="code-keyword">from</span> sklearn.tree <span class="code-keyword">import</span> export_text
<span class="code-function">print</span>(<span class="code-string">"\nPolicy tree rules:"</span>)
<span class="code-function">print</span>(<span class="code-function">export_text</span>(policy_tree, feature_names=[<span class="code-string">f'X{i}'</span> <span class="code-keyword">for</span> i <span class="code-keyword">in</span> <span class="code-function">range</span>(p)]))</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* ---- Policy learning via Python from Stata ----</span>
<span class="code-comment">* Uses estimated CATEs from the previous causal forest block</span>

<span class="code-keyword">python:</span>
<span class="code-keyword">from</span> sfi <span class="code-keyword">import</span> Data
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.tree <span class="code-keyword">import</span> DecisionTreeClassifier, export_text

n = Data.getObsTotal()
X = np.column_stack([np.array(Data.getVarFloat(<span class="code-string">f'x{j}'</span>)) <span class="code-keyword">for</span> j <span class="code-keyword">in</span> range(1, 11)])
cate_hat = np.array(Data.getVarFloat(<span class="code-string">'cate_hat'</span>))

<span class="code-comment"># Learn interpretable policy: treat if CATE > 0</span>
policy_labels = (cate_hat > 0).astype(int)
policy_tree = DecisionTreeClassifier(max_depth=2, random_state=42)
policy_tree.fit(X, policy_labels)

recommended = policy_tree.predict(X)
print(<span class="code-string">"=== Policy Learning Results ==="</span>)
print(<span class="code-string">f"Share recommended for treatment: {recommended.mean():.3f}"</span>)
print(<span class="code-string">f"Mean CATE (treated group):       {cate_hat[recommended == 1].mean():.4f}"</span>)
print(<span class="code-string">f"Mean CATE (untreated group):     {cate_hat[recommended == 0].mean():.4f}"</span>)

print(<span class="code-string">"\nPolicy tree rules:"</span>)
print(export_text(policy_tree, feature_names=[<span class="code-string">f'x{i+1}'</span> <span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(10)]))

<span class="code-comment"># Push treatment recommendation back to Stata</span>
Data.addVarFloat(<span class="code-string">'treat_rec'</span>)
<span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(n):
    Data.storeVar(<span class="code-string">'treat_rec'</span>, i, float(recommended[i]))
<span class="code-keyword">end</span>

<span class="code-keyword">tab</span> treat_rec</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="policytree implements optimal policy learning on causal forest output. It finds the depth-limited decision tree that maximizes estimated welfare using doubly-robust scores.">policytree</span>)
<span class="code-keyword">library</span>(grf)

<span class="code-comment"># ---- (Assumes cf, X from previous block) ----</span>

<span class="code-comment"># ---- Compute doubly-robust scores ----</span>
<span class="code-tooltip" data-tip="double_robust_scores extracts doubly-robust AIPW-style scores from the causal forest. These scores account for estimation error in both the outcome and propensity models.">dr_scores</span> &lt;- <span class="code-function">double_robust_scores</span>(cf)

<span class="code-comment"># ---- Learn an optimal depth-2 policy tree ----</span>
policy &lt;- <span class="code-function">policy_tree</span>(X, dr_scores, <span class="code-tooltip" data-tip="depth = 2 means the tree has at most 4 leaves, making the rule easy to interpret and implement.">depth</span> = 2)

<span class="code-comment"># ---- Predict treatment recommendation ----</span>
rec &lt;- <span class="code-function">predict</span>(policy, X)  <span class="code-comment"># 1 = control, 2 = treat</span>
treat_rec &lt;- <span class="code-function">as.numeric</span>(rec == 2)

<span class="code-function">cat</span>(<span class="code-string">"=== Policy Learning Results ===\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Share recommended for treatment:"</span>, <span class="code-function">round</span>(<span class="code-function">mean</span>(treat_rec), 3), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Mean CATE (treated group):      "</span>,
    <span class="code-function">round</span>(<span class="code-function">mean</span>(cate_hat[treat_rec == 1]), 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Mean CATE (untreated group):    "</span>,
    <span class="code-function">round</span>(<span class="code-function">mean</span>(cate_hat[treat_rec == 0]), 4), <span class="code-string">"\n"</span>)

<span class="code-comment"># ---- Display the policy tree ----</span>
<span class="code-function">cat</span>(<span class="code-string">"\nPolicy tree structure:\n"</span>)
<span class="code-function">print</span>(policy)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-dml-5 -->
        <div class="output-simulation" data-output="ml-dml-5" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Policy Learning Results ===
Share recommended for treatment: 0.716
Mean CATE among treated group:   1.6382
Mean CATE among untreated group: -0.5874

Policy tree rules:
|--- X0 <= -0.48
|   |--- X0 <= -1.36
|   |   |--- class: 0
|   |--- X0 >  -1.36
|   |   |--- class: 0
|--- X0 >  -0.48
|   |--- X0 <= 0.07
|   |   |--- class: 1
|   |--- X0 >  0.07
|   |   |--- class: 1</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-5" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Policy Learning Results ===
Share recommended for treatment: 0.716
Mean CATE (treated group):       1.6382
Mean CATE (untreated group):     -0.5874

Policy tree rules:
|--- x1 <= -0.48
|   |--- class: 0
|--- x1 >  -0.48
|   |--- class: 1

  treat_rec |      Freq.     Percent        Cum.
------------+-----------------------------------
          0 |        568       28.40       28.40
          1 |      1,432       71.60      100.00
------------+-----------------------------------
      Total |      2,000      100.00</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-5" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== Policy Learning Results ===
Share recommended for treatment: 0.714
Mean CATE (treated group):       1.6419
Mean CATE (untreated group):    -0.5921

Policy tree structure:
policy_tree object
Tree depth:  2
Actions:     1: control  2: treat
Variable splits:
[1] X1 <= -0.49  --> control
[2] X1 >  -0.49  --> treat</pre></div>
        </div>

        <p>The policy tree identifies X<sub>0</sub> (X<sub>1</sub> in Stata/R) as the key variable for treatment assignment, with a cutoff around -0.5. This makes sense: the true CATE is &tau;(x) = 1 + 2x<sub>0</sub>, which crosses zero at x<sub>0</sub> = -0.5. The policy correctly recommends treating individuals with x<sub>0</sub> > -0.5 (who have positive treatment effects) and withholding treatment from those with x<sub>0</sub> &lt; -0.5 (who would be harmed). This is a powerful result: from observational data, we have learned an interpretable, near-optimal treatment rule.</p>


        <!-- ===================== BART & BAYESIAN CAUSAL FORESTS ===================== -->
        <h2 id="bart">BART &amp; Bayesian Causal Forests</h2>

        <p>An alternative approach to estimating heterogeneous treatment effects comes from Bayesian nonparametrics. <strong>Bayesian Additive Regression Trees</strong> (BART), applied to causal inference by Hill (2011), model the response surface flexibly while placing prior distributions on the tree structures and leaf parameters. This yields posterior distributions over treatment effects, providing a natural measure of uncertainty without relying on asymptotic approximations.</p>

        <p>Hahn, Murray, and Carvalho (2020) extended this framework with <strong>Bayesian Causal Forests</strong> (BCF), which separate the model into a prognostic function &mu;(x) (the baseline outcome) and a treatment effect function &tau;(x), each modeled by its own BART ensemble. This separation has two advantages: it allows the prognostic component to absorb the main variation in outcomes, and it enables a regularizing prior on &tau;(x) that shrinks treatment effect heterogeneity toward homogeneity, which is sensible when you expect most of the effect to be common across individuals.</p>


        <!-- Code Block: ml-dml-6 (BART & BCF) -->
        <div class="code-tabs" data-runnable="ml-dml-6">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="r">R</button>
            <button class="tab-button" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
          </div>
          <div class="tab-content active" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="bartCause implements BART-based causal inference following Hill (2011). It estimates potential outcomes E[Y(1)|X] and E[Y(0)|X] via BART and computes individual and average treatment effects with Bayesian credible intervals.">bartCause</span>)

<span class="code-comment"># ---- Simulate data with heterogeneous effects ----</span>
<span class="code-function">set.seed</span>(42)
n &lt;- 1000; p &lt;- 10
X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * p), n, p)
<span class="code-function">colnames</span>(X) &lt;- <span class="code-function">paste0</span>(<span class="code-string">"X"</span>, 1:p)

tau &lt;- 1 + 2 * X[, 1]
<span class="code-tooltip" data-tip="Propensity score model: treatment probability depends on X1. pnorm converts the linear index to a probability between 0 and 1.">ps</span> &lt;- <span class="code-function">pnorm</span>(0.5 * X[, 1])
D &lt;- <span class="code-function">rbinom</span>(n, 1, ps)
Y &lt;- tau * D + X[, 1] + 0.5 * X[, 2]^2 + <span class="code-function">rnorm</span>(n)

<span class="code-comment"># ---- Fit BART causal model ----</span>
bart_fit &lt;- <span class="code-function">bartc</span>(
  response = Y,
  treatment = D,
  confounders = X,
  <span class="code-tooltip" data-tip="method.rsp controls how the response model is estimated. 'bart' uses BART for both E[Y|X,D=1] and E[Y|X,D=0].">method.rsp</span> = <span class="code-string">"bart"</span>,
  <span class="code-tooltip" data-tip="method.trt controls how the propensity score is estimated. 'bart' uses BART for P(D=1|X). Set to 'none' to skip propensity adjustment.">method.trt</span> = <span class="code-string">"bart"</span>,
  n.samples = 500,
  n.burn = 200,
  verbose = <span class="code-keyword">FALSE</span>
)

<span class="code-comment"># ---- Extract individual treatment effects ----</span>
ite &lt;- <span class="code-function">fitted</span>(bart_fit, type = <span class="code-string">"ite"</span>)
cate_bart &lt;- <span class="code-function">apply</span>(ite, 2, mean)  <span class="code-comment"># Posterior mean for each individual</span>

<span class="code-function">cat</span>(<span class="code-string">"=== BART Causal Inference Results ===\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"ATE (posterior mean):  "</span>, <span class="code-function">round</span>(<span class="code-function">mean</span>(cate_bart), 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"True ATE:             "</span>, <span class="code-function">round</span>(<span class="code-function">mean</span>(tau), 4), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Cor(estimated, true): "</span>, <span class="code-function">round</span>(<span class="code-function">cor</span>(cate_bart, tau), 4), <span class="code-string">"\n"</span>)

<span class="code-comment"># ---- 95% credible interval for ATE ----</span>
ate_samples &lt;- <span class="code-function">apply</span>(ite, 1, mean)  <span class="code-comment"># ATE for each posterior draw</span>
ci &lt;- <span class="code-function">quantile</span>(ate_samples, <span class="code-function">c</span>(0.025, 0.975))
<span class="code-function">cat</span>(<span class="code-string">"95% Credible Interval: ["</span>, <span class="code-function">round</span>(ci[1], 4), <span class="code-string">","</span>,
    <span class="code-function">round</span>(ci[2], 4), <span class="code-string">"]\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
          <div class="tab-content" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># ---- Note: BART for causal inference in Python ----</span>
<span class="code-comment"># The bartpy package provides basic BART functionality.</span>
<span class="code-comment"># For full causal BART (Hill 2011), the R bartCause package</span>
<span class="code-comment"># is the most mature implementation. Here we demonstrate</span>
<span class="code-comment"># a simplified approach using separate BART models.</span>

<span class="code-keyword">try</span>:
    <span class="code-keyword">from</span> bartpy.sklearnmodel <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="SklearnModel provides a scikit-learn compatible BART interface. It can be used for the response surface estimation step.">SklearnModel</span>

    <span class="code-comment"># ---- Simulate data ----</span>
    np.random.<span class="code-function">seed</span>(42)
    n = 1000; p = 10
    X = np.random.<span class="code-function">randn</span>(n, p)
    tau = 1 + 2 * X[:, 0]
    D = (np.random.<span class="code-function">randn</span>(n) + 0.5 * X[:, 0] > 0).<span class="code-function">astype</span>(<span class="code-keyword">float</span>)
    Y = tau * D + X[:, 0] + 0.5 * X[:, 1]**2 + np.random.<span class="code-function">randn</span>(n)

    <span class="code-comment"># ---- Fit separate BART models for treated and control ----</span>
    bart_treat = <span class="code-function">SklearnModel</span>(n_trees=50, n_chains=4, n_samples=200, n_burn=100)
    bart_control = <span class="code-function">SklearnModel</span>(n_trees=50, n_chains=4, n_samples=200, n_burn=100)

    bart_treat.<span class="code-function">fit</span>(X[D == 1], Y[D == 1])
    bart_control.<span class="code-function">fit</span>(X[D == 0], Y[D == 0])

    <span class="code-comment"># Estimate CATEs: E[Y(1)|X] - E[Y(0)|X]</span>
    y1_hat = bart_treat.<span class="code-function">predict</span>(X)
    y0_hat = bart_control.<span class="code-function">predict</span>(X)
    cate_bart = y1_hat - y0_hat

    <span class="code-function">print</span>(<span class="code-string">"=== BART Causal Results (Python) ==="</span>)
    <span class="code-function">print</span>(<span class="code-string">f"ATE (estimated):       {cate_bart.mean():.4f}"</span>)
    <span class="code-function">print</span>(<span class="code-string">f"True ATE:              {tau.mean():.4f}"</span>)
    <span class="code-function">print</span>(<span class="code-string">f"Cor(estimated, true):  {np.corrcoef(cate_bart, tau)[0,1]:.4f}"</span>)

<span class="code-keyword">except</span> <span class="code-keyword">ImportError</span>:
    <span class="code-function">print</span>(<span class="code-string">"bartpy not installed. Install with: pip install bartpy"</span>)
    <span class="code-function">print</span>(<span class="code-string">"For full Bayesian causal BART, the R bartCause package"</span>)
    <span class="code-function">print</span>(<span class="code-string">"is recommended (more mature, posterior credible intervals)."</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* ---- BART for Causal Inference ----</span>
<span class="code-comment">* Stata does not have a native BART implementation.</span>
<span class="code-comment">* Options for Stata users:</span>
<span class="code-comment">*</span>
<span class="code-comment">* 1. Use the python: block to call bartpy or bartCause (via rpy2)</span>
<span class="code-comment">* 2. Use the rcall package to call R's bartCause directly</span>
<span class="code-comment">* 3. Export data, run BART in R, and import results</span>
<span class="code-comment">*</span>
<span class="code-comment">* Here we demonstrate option 1 with a simplified approach:</span>

<span class="code-keyword">clear</span> all
<span class="code-keyword">set</span> seed 42
<span class="code-keyword">set</span> obs 1000

<span class="code-keyword">forvalues</span> j = 1/10 {
    <span class="code-keyword">gen</span> double x`j' = <span class="code-function">rnormal</span>()
}
<span class="code-keyword">gen</span> double tau = 1 + 2 * x1
<span class="code-keyword">gen</span> double ps = <span class="code-function">normal</span>(0.5 * x1)
<span class="code-keyword">gen</span> byte D = (<span class="code-function">runiform</span>() &lt; ps)
<span class="code-keyword">gen</span> double Y = tau * D + x1 + 0.5 * x2^2 + <span class="code-function">rnormal</span>()

<span class="code-keyword">python:</span>
<span class="code-keyword">from</span> sfi <span class="code-keyword">import</span> Data
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

n = Data.getObsTotal()
X = np.column_stack([np.array(Data.getVarFloat(<span class="code-string">f'x{j}'</span>)) <span class="code-keyword">for</span> j <span class="code-keyword">in</span> range(1, 11)])
D = np.array(Data.getVarFloat(<span class="code-string">'D'</span>))
Y = np.array(Data.getVarFloat(<span class="code-string">'Y'</span>))
tau_true = np.array(Data.getVarFloat(<span class="code-string">'tau'</span>))

<span class="code-keyword">try</span>:
    <span class="code-keyword">from</span> bartpy.sklearnmodel <span class="code-keyword">import</span> SklearnModel
    bart_t = SklearnModel(n_trees=50, n_chains=4, n_samples=200, n_burn=100)
    bart_c = SklearnModel(n_trees=50, n_chains=4, n_samples=200, n_burn=100)
    bart_t.fit(X[D == 1], Y[D == 1])
    bart_c.fit(X[D == 0], Y[D == 0])
    cate = bart_t.predict(X) - bart_c.predict(X)

    print(<span class="code-string">"=== BART Causal Results ==="</span>)
    print(<span class="code-string">f"ATE (estimated): {cate.mean():.4f}"</span>)
    print(<span class="code-string">f"True ATE:        {tau_true.mean():.4f}"</span>)
    print(<span class="code-string">f"Correlation:     {np.corrcoef(cate, tau_true)[0,1]:.4f}"</span>)
<span class="code-keyword">except</span> ImportError:
    print(<span class="code-string">"bartpy not installed in Stata's Python."</span>)
    print(<span class="code-string">"Recommendation: Use R with bartCause for full BART causal inference."</span>)
<span class="code-keyword">end</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
        </div>

        <!-- Output Simulations for ml-dml-6 -->
        <div class="output-simulation" data-output="ml-dml-6" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== BART Causal Inference Results ===
ATE (posterior mean):   1.0287
True ATE:               1.0238
Cor(estimated, true):   0.8413
95% Credible Interval: [ 0.8742 , 1.1831 ]</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-6" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== BART Causal Results (Python) ===
ATE (estimated):       1.0341
True ATE:              1.0238
Cor(estimated, true):  0.7896</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-dml-6" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>=== BART Causal Results ===
ATE (estimated): 1.0341
True ATE:        1.0238
Correlation:     0.7896</pre></div>
        </div>

        <p>BART provides good point estimates and, through the posterior distribution, natural uncertainty quantification. The credible intervals from the R implementation account for both estimation uncertainty and model uncertainty. The main advantage of BART over causal forests is its Bayesian uncertainty quantification; the main disadvantage is that BART does not have the same formal asymptotic guarantees as causal forests, and computation can be slower for large datasets.</p>


        <!-- ===================== CHOOSING A METHOD ===================== -->
        <h3>Choosing a Causal ML Method</h3>

        <div class="distinction-box" style="grid-template-columns: 1fr 1fr 1fr;">
          <div class="distinction-card">
            <h4>DML</h4>
            <ul>
              <li>Estimates <strong>average</strong> treatment effect</li>
              <li>Any ML method for nuisance</li>
              <li>Asymptotically normal, valid CI</li>
              <li>Best when ATE is the target</li>
              <li><code>DoubleML</code> (Python/R), <code>ddml</code> (Stata)</li>
            </ul>
          </div>
          <div class="distinction-card">
            <h4>Causal Forests</h4>
            <ul>
              <li>Estimates <strong>heterogeneous</strong> effects</li>
              <li>Honest estimation, valid CI</li>
              <li>Policy learning integration</li>
              <li>Best for discovering HTEs</li>
              <li><code>grf</code> (R), <code>econml</code> (Python)</li>
            </ul>
          </div>
          <div class="distinction-card">
            <h4>BART / BCF</h4>
            <ul>
              <li>Bayesian uncertainty (credible intervals)</li>
              <li>Flexible nonparametric response surface</li>
              <li>Natural regularization via priors</li>
              <li>Best for smaller data with rich uncertainty</li>
              <li><code>bartCause</code>, <code>bcf</code> (R)</li>
            </ul>
          </div>
        </div>


        <!-- ===================== LIMITATIONS ===================== -->
        <h2 id="limitations">Limitations</h2>

        <div class="info-box" style="border-left-color: #e53e3e;">
          <h3>ML Does Not Solve the Identification Problem</h3>
          <p>The most important limitation of causal ML is one that no amount of algorithmic sophistication can overcome: <strong>machine learning does not solve the fundamental identification problem</strong>. All of the methods in this module — DML, causal forests, BART — rely on the assumption of <em>unconfoundedness</em> (also called "selection on observables" or "conditional ignorability"): conditional on the observed covariates X, treatment assignment is independent of potential outcomes.</p>
          <p>This is the same assumption that underlies OLS with controls, propensity score matching, and inverse probability weighting. ML makes these methods work better in high dimensions, but it cannot conjure causal identification where none exists. If there are unobserved confounders — variables that affect both treatment and outcome but are not in your data — all of these methods will produce biased estimates, no matter how many trees or how much cross-fitting you use.</p>
          <p>The practical implication is that causal ML should be used <strong>in combination with</strong> a credible identification strategy, not as a substitute for one. If you have a valid instrument, a clean regression discontinuity, or a natural experiment, causal ML can make your estimates more precise and reveal heterogeneity. But if your identification strategy is weak, no ML method will save you.</p>
        </div>


        <!-- ===================== REFERENCES ===================== -->
        <h2>References</h2>
        <ul class="references">
          <li>Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., &amp; Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. <em>The Econometrics Journal</em>, 21(1), C1-C68.</li>
          <li>Wager, S., &amp; Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. <em>Journal of the American Statistical Association</em>, 113(523), 1228-1242.</li>
          <li>Athey, S., &amp; Imbens, G. (2016). Recursive partitioning for heterogeneous causal effects. <em>Proceedings of the National Academy of Sciences</em>, 113(27), 7353-7360.</li>
          <li>Athey, S., &amp; Imbens, G. W. (2019). Machine learning methods that economists should know about. <em>Annual Review of Economics</em>, 11, 685-725.</li>
          <li>Athey, S., &amp; Wager, S. (2021). Policy learning with observational data. <em>Econometrica</em>, 89(1), 133-161.</li>
          <li>Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. <em>Journal of Computational and Graphical Statistics</em>, 20(1), 217-240.</li>
          <li>Hahn, P. R., Murray, J. S., &amp; Carvalho, C. M. (2020). Bayesian regression tree models for causal inference: Regularization, confounding, and heterogeneous effects. <em>Bayesian Analysis</em>, 15(3), 965-1056.</li>
          <li>Knaus, M. C., Lechner, M., &amp; Strittmatter, A. (2021). Machine learning estimation of heterogeneous causal effects: Empirical Monte Carlo evidence. <em>The Econometrics Journal</em>, 24(1), 134-161.</li>
        </ul>


        <!-- ===================== NAV FOOTER ===================== -->
        <div class="nav-footer">
          <a href="11c-neural-networks.html" class="nav-link prev">11C: Neural Networks</a>
          <a href="11e-model-evaluation.html" class="nav-link next">11E: Model Evaluation</a>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about Python, Stata, R, causal inference methods, or any of the course material. How can I assist you today?</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    let tooltipEl = null; let currentTarget = null; let hideTimeout = null;
    function createTooltip() { if (tooltipEl) return tooltipEl; tooltipEl = document.createElement('div'); tooltipEl.className = 'tooltip-popup'; document.body.appendChild(tooltipEl); return tooltipEl; }
    function positionTooltip(target) { const tooltip = createTooltip(); const tipText = target.getAttribute('data-tip'); if (!tipText) return; tooltip.textContent = tipText; tooltip.className = 'tooltip-popup'; const targetRect = target.getBoundingClientRect(); let container = target.closest('pre') || target.closest('.tab-content') || target.closest('.code-tabs'); let containerRect = container ? container.getBoundingClientRect() : { left: 0, right: window.innerWidth, top: 0, bottom: window.innerHeight }; const vw = window.innerWidth; const vh = window.innerHeight; const pad = 10; tooltip.style.visibility = 'hidden'; tooltip.style.display = 'block'; tooltip.classList.add('visible'); const tr = tooltip.getBoundingClientRect(); let left = targetRect.left + (targetRect.width/2) - (tr.width/2); let top = targetRect.top - tr.height - 8; let ac = 'arrow-bottom'; if (top < pad) { top = targetRect.bottom + 8; ac = 'arrow-top'; } if (top + tr.height > vh - pad) { top = targetRect.top - tr.height - 8; ac = 'arrow-bottom'; } if (left < pad) left = pad; if (left + tr.width > vw - pad) left = vw - tr.width - pad; if (container) { const ml = Math.max(pad, containerRect.left); const mr = Math.min(vw - pad, containerRect.right); if (left < ml) left = ml; if (left + tr.width > mr) left = mr - tr.width; } tooltip.style.left = left + 'px'; tooltip.style.top = top + 'px'; tooltip.style.visibility = 'visible'; tooltip.classList.add(ac); }
    function showTooltip(t) { if (hideTimeout) { clearTimeout(hideTimeout); hideTimeout = null; } currentTarget = t; positionTooltip(t); }
    function hideTooltip() { hideTimeout = setTimeout(function() { if (tooltipEl) tooltipEl.classList.remove('visible'); currentTarget = null; }, 100); }
    document.addEventListener('mouseenter', function(e) { if (e.target.classList && e.target.classList.contains('code-tooltip')) showTooltip(e.target); }, true);
    document.addEventListener('mouseleave', function(e) { if (e.target.classList && e.target.classList.contains('code-tooltip')) hideTooltip(); }, true);
    document.addEventListener('scroll', function() { if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget); }, true);
    window.addEventListener('resize', function() { if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget); });
  })();
  </script>
</body>
</html>
