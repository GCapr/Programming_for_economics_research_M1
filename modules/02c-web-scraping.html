<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 2c: Web Scraping | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content { -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; }
    .protected-content pre, .protected-content code, .protected-content .code-block, .protected-content .code-tabs { -webkit-user-select: text; -moz-user-select: text; -ms-user-select: text; user-select: text; }
    .output-simulation { background: #1e1e1e; border-radius: 8px; margin: 1rem 0; overflow: hidden; font-family: 'Fira Code', monospace; font-size: 0.8rem; display: none; }
    .output-simulation.visible { display: block; }
    .output-header { background: #333; padding: 0.5rem 1rem; display: flex; justify-content: space-between; align-items: center; color: #ccc; font-size: 0.75rem; }
    .output-body { padding: 1rem; color: #d4d4d4; white-space: pre-wrap; overflow-x: auto; max-height: 400px; overflow-y: auto; }
    .output-body .out-green { color: #4ec9b0; }
    .output-body .out-yellow { color: #dcdcaa; }
    .output-body .out-blue { color: #569cd6; }
    .output-body .out-num { color: #b5cea8; }
    .output-body .out-str { color: #ce9178; }
    .output-body .out-red { color: #f14c4c; }
    .research-banner { background: #4a5568; color: white; padding: 1.5rem; border-radius: 12px; margin: 2rem 0; border-left: 5px solid #ed8936; }
    .research-banner h3 { color: white; margin-top: 0; }
    .concept-box { background: #f0f9ff; border-left: 4px solid #2563eb; padding: 1.5rem; margin: 1.5rem 0; border-radius: 0 8px 8px 0; }
    .concept-box h4 { color: #1e40af; margin-top: 0; }
    .legal-box { background: #fef3c7; border-left: 4px solid #d97706; padding: 1.5rem; margin: 1.5rem 0; border-radius: 0 8px 8px 0; }
    .legal-box h4 { color: #92400e; margin-top: 0; }
    .danger-box { background: #fee2e2; border-left: 4px solid #dc2626; padding: 1.5rem; margin: 1.5rem 0; border-radius: 0 8px 8px 0; }
    .danger-box h4 { color: #991b1b; margin-top: 0; }
    .run-btn { background: #22c55e; color: white; border: none; padding: 0.5rem 1rem; border-radius: 6px; cursor: pointer; font-size: 0.85rem; font-weight: 600; margin-top: 0.5rem; display: inline-flex; align-items: center; gap: 0.5rem; }
    .run-btn:hover { background: #16a34a; }
    .run-btn::before { content: '‚ñ∂'; font-size: 0.7rem; }
    .close-output { background: transparent; border: none; color: #9ca3af; cursor: pointer; font-size: 1.2rem; }
    .close-output:hover { color: white; }
    /* Code tooltips - hover explanations (JavaScript-powered for boundary detection) */
    .code-tooltip {
      position: relative;
      cursor: help;
      border-bottom: 1px dotted #888;
      text-decoration: none;
    }

    /* Tooltip element created by JavaScript */
    .tooltip-popup {
      position: fixed;
      background: #1f2937;
      color: white;
      padding: 0.5rem 0.75rem;
      border-radius: 6px;
      font-size: 0.75rem;
      white-space: normal;
      max-width: 300px;
      opacity: 0;
      pointer-events: none;
      transition: opacity 0.15s ease-in-out;
      z-index: 10000;
      line-height: 1.4;
      text-align: left;
      font-family: var(--font-body);
      font-style: normal;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    .tooltip-popup.visible {
      opacity: 1;
    }

    /* Small arrow pointer */
    .tooltip-popup::after {
      content: '';
      position: absolute;
      border: 6px solid transparent;
    }
    .tooltip-popup.arrow-bottom::after {
      top: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-top-color: #1f2937;
    }
    .tooltip-popup.arrow-top::after {
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-bottom-color: #1f2937;
    }
    .tooltip-popup.arrow-left::after {
      top: 50%;
      right: 100%;
      transform: translateY(-50%);
      border-right-color: #1f2937;
    }
    .tooltip-popup.arrow-right::after {
      top: 50%;
      left: 100%;
      transform: translateY(-50%);
      border-left-color: #1f2937;
    }
    .html-diagram { background: #1e1e1e; border-radius: 8px; padding: 1.5rem; margin: 1.5rem 0; font-family: 'Fira Code', monospace; font-size: 0.85rem; color: #d4d4d4; overflow-x: auto; }
    .html-diagram .tag { color: #569cd6; }
    .html-diagram .attr { color: #9cdcfe; }
    .html-diagram .value { color: #ce9178; }
    .html-diagram .content { color: #d4d4d4; }
    .html-diagram .comment { color: #6a9955; }
    .checklist { background: #f0fdf4; border: 1px solid #86efac; border-radius: 8px; padding: 1.5rem; margin: 1.5rem 0; }
    .checklist h4 { color: #166534; margin-top: 0; }
    .checklist ul { margin-bottom: 0; }
    .checklist li { margin-bottom: 0.5rem; }

    /* ========================================
       CONSISTENT TYPOGRAPHY HIERARCHY
       (Matches site-wide conventions)
       ======================================== */

    /* Section numbers in headings */
    .section-num {
      color: var(--color-accent);
      font-weight: 700;
    }

    /* Section divider between major sections */
    .section-divider {
      height: 2px;
      background: linear-gradient(90deg, transparent 0%, var(--color-border) 15%, var(--color-border) 85%, transparent 100%);
      margin: 3rem 0 2rem 0;
    }

    /* ========================================
       COLLAPSIBLE/ACCORDION SECTIONS
       ======================================== */

    .accordion-section {
      border: 1px solid var(--color-border);
      border-radius: 8px;
      margin: 1rem 0;
      overflow: hidden;
      background: white;
    }

    .accordion-header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 1rem 1.25rem;
      background: var(--color-bg-alt);
      cursor: pointer;
      transition: background 0.2s ease;
      border: none;
      width: 100%;
      text-align: left;
      font-family: var(--font-heading);
      font-size: 1.1rem;
      font-weight: 600;
      color: var(--color-primary);
    }

    .accordion-header:hover {
      background: #e2e8f0;
    }

    .accordion-header::after {
      content: '+';
      font-size: 1.4rem;
      font-weight: 400;
      color: var(--color-accent);
      transition: transform 0.3s ease;
    }

    .accordion-section.open .accordion-header::after {
      content: '‚àí';
    }

    .accordion-content {
      max-height: 0;
      overflow: hidden;
      transition: max-height 0.3s ease-out;
      padding: 0 1.25rem;
    }

    .accordion-section.open .accordion-content {
      max-height: 2000px;
      padding: 1rem 1.25rem 1.25rem;
    }

    .accordion-content p:last-child,
    .accordion-content ul:last-child {
      margin-bottom: 0;
    }

    /* ========================================
       TABLE OF CONTENTS (Simpler, site-consistent)
       ======================================== */

    .toc {
      background: var(--color-bg-alt);
      border: 1px solid var(--color-border);
      border-radius: 8px;
      padding: 1.25rem 1.5rem;
      margin: 1.5rem 0 2rem 0;
    }

    .toc h3 {
      font-size: 1rem;
      color: var(--color-primary);
      margin: 0 0 0.75rem 0;
      padding: 0;
    }

    .toc ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }

    .toc li {
      margin: 0.4rem 0;
    }

    .toc a {
      color: var(--color-text);
      font-size: 0.95rem;
      padding: 0.25rem 0;
      display: inline-block;
    }

    .toc a:hover {
      color: var(--color-accent);
    }

    .toc .toc-num {
      color: var(--color-accent);
      font-weight: 600;
      margin-right: 0.25rem;
    }
  </style>
</head>
<body>
    <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>

      <div class="course-description">
        <h3>Course Modules</h3>
        <ul class="module-list">
          <li><strong>Module 0:</strong> Languages & Platforms ‚Äî Python, Stata, R setup; IDEs (RStudio, VS Code, Jupyter)</li>
          <li><strong>Module 1:</strong> Getting Started ‚Äî Installation, basic syntax, packages</li>
          <li><strong>Module 2:</strong> Data Harnessing ‚Äî File import, APIs, web scraping</li>
          <li><strong>Module 3:</strong> Data Exploration ‚Äî Inspection, summary statistics, visualization</li>
          <li><strong>Module 4:</strong> Data Cleaning ‚Äî Data quality, transformation, validation</li>
          <li><strong>Module 5:</strong> Data Analysis ‚Äî Statistical analysis, simulation, experimental design</li>
          <li><strong>Module 6:</strong> Causal Inference ‚Äî Matching, DiD, RDD, IV, Synthetic Control</li>
          <li><strong>Module 7:</strong> Estimation Methods ‚Äî Standard errors, panel data, MLE/GMM</li>
          <li><strong>Module 8:</strong> Replicability ‚Äî Project organization, documentation, replication packages</li>
          <li><strong>Module 9:</strong> Git & GitHub ‚Äî Version control, collaboration, branching</li>
          <li><strong>Module 10:</strong> History of NLP ‚Äî From ELIZA to Transformers</li>
          <li><strong>Module 11:</strong> Machine Learning ‚Äî Prediction, regularization, neural networks</li>
          <li><strong>Module 12:</strong> Large Language Models ‚Äî How LLMs work, prompting, APIs</li>
        </ul>
      </div>

      <div class="access-note">
        This course is currently open to <strong>students at Sciences Po</strong>. If you are not a Sciences Po student but would like access, please <a href="mailto:giulia.caprini@sciencespo.fr">email me</a> to request an invite token.
      </div>

      <div class="password-form">
        <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
        <button id="password-submit">Access Course</button>
        <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
      </div>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav active">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li class="active"><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li class="has-subnav">
            <a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a>
            <ul class="sub-nav">
              <li><a href="10a-text-analysis-today.html">Text Analysis Today</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a>
            <ul class="sub-nav">
              <li><a href="11a-regularization.html">Regularization</a></li>
              <li><a href="11b-trees.html">Tree-Based Methods</a></li>
              <li><a href="11c-neural-networks.html">Neural Networks</a></li>
              <li><a href="11d-causal-ml.html">Causal ML</a></li>
              <li><a href="11e-model-evaluation.html">Model Evaluation</a></li>
            </ul>
          </li>          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <div class="module-header">
          <h1>2c &nbsp;Web Scraping</h1>
          <div class="module-meta">
            <span>~5 hours</span>
            <span>HTML, BeautifulSoup, Legal Considerations</span>
            <span>Intermediate</span>
          </div>
        </div>

        <div class="learning-objectives">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Understand when web scraping is appropriate vs. using APIs</li>
            <li>Navigate the legal and ethical landscape of web scraping</li>
            <li>Parse HTML structure and extract data using BeautifulSoup/rvest</li>
            <li>Handle pagination, rate limiting, and dynamic content</li>
            <li>Build robust, respectful scrapers for research data collection</li>
          </ul>
        </div>

        <!-- Critical Legal Warning -->
        <div class="danger-box">
          <h4>Before You Scrape: Legal and Ethical Obligations</h4>
          <p><strong>Web scraping exists in a legal gray area.</strong> What's technically possible isn't always legal or ethical. Before scraping any website, you must understand the legal framework and respect website owners' rights.</p>
          <p style="margin-bottom: 0;"><strong>This module teaches responsible scraping for legitimate research purposes only.</strong></p>
        </div>

        <!-- Research Project Scenario -->
        <div class="research-banner">
          <h3>üìä Course Research Project: Climate Vulnerability & Economic Growth</h3>
          <p style="margin-bottom: 1rem;">Throughout this course, we're building a research project on <strong>climate vulnerability and economic growth</strong>. In <a href="02a-file-import.html" style="color: #fbd38d;">Module 2a</a>, we loaded GDP data from files. In <a href="02b-apis.html" style="color: #fbd38d;">Module 2b</a>, we fetched CO2 emissions via the World Bank API. Now we'll <strong>scrape additional climate data</strong> from Wikipedia.</p>
          <p style="margin-bottom: 1rem;"><strong>Our target:</strong> <a href="https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions" style="color: #fbd38d;" target="_blank">Wikipedia's "List of countries by carbon dioxide emissions"</a> page. This table contains historical emissions data that we can merge with our World Bank data for a richer analysis.</p>
          <p style="margin-bottom: 1rem;"><strong>Why Wikipedia?</strong> It's public, legal to scrape (under their <a href="https://en.wikipedia.org/wiki/Wikipedia:Copyrights" style="color: #fbd38d;" target="_blank">CC license</a>), has well-structured HTML tables, and lets you verify your results by looking at the actual page.</p>
          <p style="margin-bottom: 0; background: rgba(255,255,255,0.1); padding: 0.75rem 1rem; border-radius: 6px;"><strong>Our goal:</strong> Extract the CO2 emissions table into a pandas DataFrame / R tibble, clean the data, and merge it with our World Bank dataset. This demonstrates a realistic research workflow.</p>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#when-to-scrape"><span class="toc-num">2c.1</span> When to Scrape (and When Not To)</a></li>
            <li><a href="#legal-framework"><span class="toc-num">2c.2</span> Legal Framework</a></li>
            <li><a href="#html-basics"><span class="toc-num">2c.3</span> Understanding HTML Structure</a></li>
            <li><a href="#basic-scraping"><span class="toc-num">2c.4</span> Basic Scraping Techniques</a></li>
            <li><a href="#advanced-techniques"><span class="toc-num">2c.5</span> Advanced Techniques</a></li>
            <li><a href="#best-practices"><span class="toc-num">2c.6</span> Best Practices for Research</a></li>
          </ul>
        </div>

        <!-- Section 1: When to Scrape -->
        <h2 id="when-to-scrape"><span class="section-num">2c.1</span> When to Scrape (and When Not To)</h2>

        <p>Before writing a single line of scraping code, you need to ask: <strong>is scraping actually the right approach?</strong> Scraping should be your last resort, not your first instinct. Let's see why.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p>For our CO2 emissions data, let's check alternatives first:</p>
          <ol style="margin-bottom: 0;">
            <li><strong>Check for existing datasets:</strong> We already used the World Bank API for CO2 data in <a href="02b-apis.html">Module 2b</a>. Wikipedia offers additional historical data and different aggregations we can complement our dataset with.</li>
            <li><strong>Check for an API:</strong> Wikipedia has a <a href="https://www.mediawiki.org/wiki/API:Main_page" target="_blank">MediaWiki API</a> that can retrieve page content. However, parsing HTML tables is often easier than parsing wikitext.</li>
            <li><strong>Why we scrape anyway:</strong> This is a <em>learning exercise</em>. Wikipedia's tables are well-structured, legal to scrape, and let you practice techniques you'll need for sites that <em>don't</em> have APIs.</li>
          </ol>
        </div>

        <div class="concept-box">
          <h4>The Hierarchy of Data Acquisition</h4>
          <p style="margin-bottom: 0.5rem;">Always prefer these options <strong>in order</strong>:</p>
          <ol style="margin-bottom: 0;">
            <li><strong>Official datasets</strong> &mdash; Published data files from the source</li>
            <li><strong>APIs</strong> &mdash; Structured, sanctioned data access (see <a href="02b-apis.html">Module 2b</a>)</li>
            <li><strong>Data request</strong> &mdash; Contact the organization directly</li>
            <li><strong>Web scraping</strong> &mdash; Last resort when above options fail</li>
          </ol>
        </div>

        <div class="info-box tip">
          <div class="info-box-title">Why Scraping Should Be a Last Resort</div>
          <p style="margin-bottom: 0.5rem;">Scraping has real costs:</p>
          <ul style="margin-bottom: 0;">
            <li><strong>It's fragile:</strong> If a website changes its layout, your scraper breaks</li>
            <li><strong>It's slow:</strong> Polite scraping requires delays between requests</li>
            <li><strong>It's legally murky:</strong> APIs and official datasets have clear usage terms</li>
            <li><strong>It requires maintenance:</strong> You'll spend time fixing broken scrapers</li>
          </ul>
        </div>

        <!-- Section 2: Legal Framework -->
        <div class="section-divider"></div>
        <h2 id="legal-framework"><span class="section-num">2c.2</span> Legal Framework</h2>

        <p>You've decided that scraping is necessary for your project. Before writing any code, you need to understand <strong>what you're legally allowed to do</strong>. This section might seem like a detour from the technical content, but skipping it could put your research‚Äîand potentially your institution‚Äîat risk.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">For our Wikipedia CO2 emissions scraper, we're in an excellent legal position: Wikipedia content is published under a <strong>Creative Commons Attribution-ShareAlike license</strong>, which explicitly permits reuse. Their <a href="https://en.wikipedia.org/robots.txt" target="_blank">robots.txt</a> allows scraping of article pages. We just need to be polite (rate limit our requests) and provide attribution if we publish the data.</p>
        </div>

        <div class="legal-box">
          <h4>I Am Not a Lawyer</h4>
          <p style="margin-bottom: 0;">This section provides educational information about legal concepts related to web scraping. <strong>It is not legal advice.</strong> Consult with your institution's legal counsel or a qualified attorney for guidance on specific situations.</p>
        </div>

        <h3>Key Legal Considerations</h3>

        <p>The legal landscape for web scraping involves several overlapping frameworks. <strong>Click each topic below</strong> to expand the details.</p>

        <!-- Accordion 1: Terms of Service -->
        <div class="accordion-section">
          <button class="accordion-header">1. Terms of Service (ToS)</button>
          <div class="accordion-content">
            <p>Most websites have Terms of Service that may prohibit automated data collection. Violating ToS can result in:</p>
            <ul>
              <li>Being blocked from the website</li>
              <li>Civil lawsuits for breach of contract</li>
              <li>In some jurisdictions, criminal charges under computer fraud laws</li>
            </ul>
            <div class="info-box warning" style="margin-top: 1rem;">
              <div class="info-box-title">Always Check the Terms of Service</div>
              <p style="margin-bottom: 0;">Before scraping, find and read the website's ToS. Look for keywords like "automated", "scraping", "crawling", "bot", "data collection".</p>
            </div>
          </div>
        </div>

        <!-- Accordion 2: robots.txt -->
        <div class="accordion-section">
          <button class="accordion-header">2. robots.txt</button>
          <div class="accordion-content">
            <p>The <code>robots.txt</code> file tells automated systems which parts of a site they can access. It's located at the root of every website (e.g., <code>example.com/robots.txt</code>).</p>
            <div class="html-diagram">
<span class="comment"># Example robots.txt file</span>

User-agent: *          <span class="comment"># Rules for all bots</span>
Disallow: /private/    <span class="comment"># Don't access /private/</span>
Disallow: /admin/      <span class="comment"># Don't access /admin/</span>
Allow: /public/        <span class="comment"># Explicitly allowed</span>

Crawl-delay: 10        <span class="comment"># Wait 10 seconds between requests</span>
            </div>
            <div class="info-box note" style="margin-top: 1rem;">
              <div class="info-box-title">robots.txt is Not Legally Binding</div>
              <p style="margin-bottom: 0;">Technically, robots.txt is a voluntary standard. However, ignoring it demonstrates bad faith and may be used against you in legal proceedings. <strong>Always respect robots.txt.</strong></p>
            </div>
          </div>
        </div>

        <!-- Accordion 3: Copyright -->
        <div class="accordion-section">
          <button class="accordion-header">3. Copyright and Database Rights</button>
          <div class="accordion-content">
            <p>Even if you can legally <em>access</em> data, you may not have the right to <em>use</em> or <em>republish</em> it:</p>
            <ul>
              <li><strong>Copyright</strong> protects creative works (articles, images, unique descriptions)</li>
              <li><strong>Database rights</strong> (EU) protect substantial investments in compiling data</li>
              <li><strong>Facts themselves</strong> are generally not copyrightable, but their presentation may be</li>
            </ul>
          </div>
        </div>

        <!-- Accordion 4: CFAA -->
        <div class="accordion-section">
          <button class="accordion-header">4. US: Computer Fraud and Abuse Act (CFAA)</button>
          <div class="accordion-content">
            <p>The CFAA prohibits "unauthorized access" to computer systems. Recent court decisions have <strong>clarified researchers' rights</strong>:</p>
            <ul>
              <li><strong>hiQ Labs v. LinkedIn (2022)</strong>: Scraping <em>publicly accessible</em> data is not "unauthorized access." Landmark decision for researchers.</li>
              <li><strong>Van Buren v. United States (Supreme Court, 2021)</strong>: Violating Terms of Service alone does not trigger criminal CFAA liability.</li>
              <li><strong>Sandvig v. Barr (2020)</strong>: CFAA does not criminalize mere ToS violations for research purposes.</li>
            </ul>
          </div>
        </div>

        <!-- Accordion 5: EU Regulation -->
        <div class="accordion-section">
          <button class="accordion-header">5. EU: Text and Data Mining Exceptions</button>
          <div class="accordion-content">
            <p>The EU <strong>Digital Single Market Directive 2019/790</strong> created explicit protections for research scraping:</p>
            <ul>
              <li><strong>Article 3 (Research Exception)</strong>: Research organizations may perform text and data mining <em>regardless of contractual terms</em>. This overrides ToS restrictions for legitimate research.</li>
              <li><strong>Article 4 (General Exception)</strong>: Any lawful access holder may perform TDM unless the rightsholder has <em>explicitly</em> reserved this right in machine-readable format.</li>
            </ul>
            <p><strong>GDPR considerations:</strong> Article 89 allows processing personal data for research with appropriate safeguards (data minimization, pseudonymization).</p>
          </div>
        </div>

        <!-- Accordion 6: Recent Developments -->
        <div class="accordion-section">
          <button class="accordion-header">6. Recent Legal Developments (2023-2025)</button>
          <div class="accordion-content">
            <ul>
              <li><strong>AI Training Data Debates</strong>: Academic research maintains stronger fair use protections than commercial AI training</li>
              <li><strong>US Copyright Office (2023)</strong>: Factual data extraction generally does not constitute copyright infringement</li>
              <li><strong>EU AI Act (2024)</strong>: Research scraping for AI development has explicit carve-outs</li>
              <li><strong>Meta v. Bright Data (2024)</strong>: Reinforced that scraping public data does not violate the CFAA</li>
            </ul>
          </div>
        </div>

        <div class="info-box warning" style="margin-top: 1.5rem;">
          <div class="info-box-title">Jurisdiction Matters</div>
          <p style="margin-bottom: 0;">Legal protections vary by country. EU researchers generally have stronger statutory protections. Consult with legal counsel about which framework applies to your specific project.</p>
        </div>

        <h3>Research-Specific Considerations</h3>

        <div class="checklist">
          <h4>Pre-Scraping Checklist for Researchers</h4>
          <ul>
            <li><strong>Check for existing datasets</strong> &mdash; ICPSR, Harvard Dataverse, Kaggle, data.gov</li>
            <li><strong>Check for an API</strong> &mdash; Even if undocumented, try adding /api/ to the URL</li>
            <li><strong>Read the Terms of Service</strong> &mdash; Document that you checked and your interpretation</li>
            <li><strong>Check robots.txt</strong> &mdash; Respect all directives and document compliance</li>
            <li><strong>Consider fair use/TDM exceptions</strong> &mdash; Is your use transformative? Non-commercial? Does EU Article 3 apply?</li>
            <li><strong>Contact your IRB</strong> &mdash; Human subjects data may require approval</li>
            <li><strong>Consult your institution</strong> &mdash; Many universities have policies on scraping</li>
          </ul>
        </div>

        <div class="concept-box" style="background: #f0fdf4; border-left-color: #22c55e;">
          <h4 style="color: #166534;">Documentation Template for Research Scraping</h4>
          <p>It's a good idea to create a dated memo that includes:</p>
          <ol>
            <li><strong>Research purpose</strong>: What question are you investigating? Why is web data necessary?</li>
            <li><strong>Data source assessment</strong>: Did you check for official datasets, APIs, or data request options?</li>
            <li><strong>Legal analysis</strong>:
              <ul>
                <li>ToS review: What do the terms say? How does your use comply or qualify for exceptions?</li>
                <li>robots.txt review: What does it permit/prohibit? How will you comply?</li>
                <li>Applicable law: Which jurisdiction applies? What protections exist (CFAA/hiQ, EU TDM)?</li>
              </ul>
            </li>
            <li><strong>Ethical considerations</strong>: Is the data sensitive? Could individuals be harmed? What safeguards will you implement?</li>
            <li><strong>Technical safeguards</strong>: Rate limiting, caching, minimal data collection, secure storage</li>
            <li><strong>Institutional approvals</strong>: IRB status, departmental sign-off, legal consultation</li>
          </ol>
          <p style="margin-bottom: 0;"><strong>Keep this document with your project files.</strong> If questions arise years later during peer review or legal inquiry, this contemporaneous record demonstrates good faith.</p>
        </div>

        <!-- Section 3: HTML Basics -->
        <div class="section-divider"></div>
        <h2 id="html-basics"><span class="section-num">2c.3</span> Understanding HTML Structure</h2>

        <p>Now that you understand when and whether to scrape, it's time to learn <strong>how web pages are structured</strong>. This is the foundation of all scraping work. You can't extract data from a page if you don't understand where that data lives.</p>

        <p>Think of a web page like a document with a very precise organizational system. Just as a research paper has headings, paragraphs, tables, and footnotes, a web page has HTML elements that organize its content. Your scraper's job is to navigate this structure and extract exactly what you need.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">Visit the <a href="https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions" target="_blank">Wikipedia CO2 emissions page</a> and right-click ‚Üí Inspect on the data table. You'll see it's a <code>&lt;table&gt;</code> element with class <code>wikitable</code>. Each row (<code>&lt;tr&gt;</code>) contains cells (<code>&lt;td&gt;</code>) with country names and emissions data. This structure is what we'll target with our scraper.</p>
        </div>

        <div class="concept-box">
          <h4>What is HTML?</h4>
          <p style="margin-bottom: 0;"><strong>HTML (HyperText Markup Language)</strong> is the structure of web pages. It uses nested <strong>tags</strong> to organize content. To scrape data, you need to understand this structure so you can tell your code where to find the information you want.</p>
        </div>

        <h3>HTML Elements</h3>
        <p>HTML is made of <strong>elements</strong>‚Äîbuilding blocks that contain content. Every element has the same basic structure:</p>

        <div class="html-diagram">
<span class="tag">&lt;tagname</span> <span class="attr">attribute</span>=<span class="value">"value"</span><span class="tag">&gt;</span><span class="content">Content goes here</span><span class="tag">&lt;/tagname&gt;</span>

<span class="comment">&lt;!-- Common examples: --&gt;</span>
<span class="tag">&lt;p&gt;</span>This is a paragraph<span class="tag">&lt;/p&gt;</span>
<span class="tag">&lt;a</span> <span class="attr">href</span>=<span class="value">"https://example.com"</span><span class="tag">&gt;</span>This is a link<span class="tag">&lt;/a&gt;</span>
<span class="tag">&lt;div</span> <span class="attr">class</span>=<span class="value">"container"</span><span class="tag">&gt;</span>This is a division/section<span class="tag">&lt;/div&gt;</span>
<span class="tag">&lt;span</span> <span class="attr">id</span>=<span class="value">"price"</span><span class="tag">&gt;</span>$99.99<span class="tag">&lt;/span&gt;</span>
<span class="tag">&lt;table&gt;</span>...<span class="tag">&lt;/table&gt;</span>              <span class="comment">&lt;!-- Tables --&gt;</span>
<span class="tag">&lt;ul&gt;&lt;li&gt;</span>...<span class="tag">&lt;/li&gt;&lt;/ul&gt;</span>         <span class="comment">&lt;!-- Lists --&gt;</span>
        </div>

        <h3>Attributes: class and id</h3>
        <p>Attributes help identify specific elements. The most important for scraping are:</p>
        <ul>
          <li><strong>class</strong> &mdash; Shared by multiple elements (e.g., all prices might have <code>class="price"</code>)</li>
          <li><strong>id</strong> &mdash; Unique identifier for a single element (e.g., <code>id="main-content"</code>)</li>
        </ul>

        <h3>Viewing Page Source</h3>
        <p>To see a page's HTML structure:</p>
        <ol>
          <li>Right-click anywhere on the page</li>
          <li>Select "View Page Source" (sees original HTML) or "Inspect" (interactive developer tools)</li>
          <li>In developer tools, you can hover over elements to see their HTML</li>
        </ol>

        <!-- Step-by-step visual guide with screenshots -->
        <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; margin: 1.5rem 0;">
          <!-- Step 1: Right-click -->
          <div style="background: #f8fafc; border-radius: 12px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
            <div style="background: #1a365d; color: white; padding: 0.5rem 1rem; font-weight: 600; font-size: 0.85rem;">
              Step 1: Right-click
            </div>
            <div style="background: #e2e8f0; min-height: 160px;">
              <img src="../images/inspect-step1-rightclick.png" alt="Right-click on a webpage element to open context menu" style="width: 100%; height: 160px; object-fit: cover; display: block;">
            </div>
            <div style="padding: 0.75rem 1rem; font-size: 0.8rem; color: #475569; border-top: 1px solid #e2e8f0;">
              Right-click on any element ‚Üí Select "Inspect"
            </div>
          </div>

          <!-- Step 2: DevTools opens -->
          <div style="background: #f8fafc; border-radius: 12px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
            <div style="background: #2c5282; color: white; padding: 0.5rem 1rem; font-weight: 600; font-size: 0.85rem;">
              Step 2: DevTools Opens
            </div>
            <div style="background: #1e1e1e; min-height: 160px;">
              <img src="../images/inspect-step2-devtools.png" alt="Chrome DevTools panel showing HTML structure" style="width: 100%; height: 160px; object-fit: cover; display: block;">
            </div>
            <div style="padding: 0.75rem 1rem; font-size: 0.8rem; color: #475569; border-top: 1px solid #e2e8f0;">
              The Elements panel shows the HTML structure
            </div>
          </div>

          <!-- Step 3: Element highlighting -->
          <div style="background: #f8fafc; border-radius: 12px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08);">
            <div style="background: #ed8936; color: white; padding: 0.5rem 1rem; font-weight: 600; font-size: 0.85rem;">
              Step 3: Hover to Highlight
            </div>
            <div style="background: #e2e8f0; min-height: 160px;">
              <img src="../images/inspect-step3-highlight.png" alt="Hovering over HTML highlights the element on the page" style="width: 100%; height: 160px; object-fit: cover; display: block;">
            </div>
            <div style="padding: 0.75rem 1rem; font-size: 0.8rem; color: #475569; border-top: 1px solid #e2e8f0;">
              Hover over HTML ‚Üí element lights up on page!
            </div>
          </div>
        </div>
        <p style="font-size: 0.85rem; color: #666; text-align: center; margin-top: 0.5rem;"><em>This is how you find the HTML selectors needed for your scraper</em></p>

        <div class="info-box tip">
          <div class="info-box-title">Pro Tip: Use the Element Picker</div>
          <p style="margin-bottom: 0;">In Chrome/Firefox developer tools, click the element picker icon (cursor in box) then click any element on the page. The HTML for that element will be highlighted in the developer panel.</p>
        </div>

        <h3>Example: A Simple Web Page</h3>
        <div class="html-diagram">
<span class="tag">&lt;html&gt;</span>
  <span class="tag">&lt;head&gt;</span>
    <span class="tag">&lt;title&gt;</span>Country GDP Data<span class="tag">&lt;/title&gt;</span>
  <span class="tag">&lt;/head&gt;</span>
  <span class="tag">&lt;body&gt;</span>
    <span class="tag">&lt;h1&gt;</span>GDP by Country<span class="tag">&lt;/h1&gt;</span>

    <span class="tag">&lt;table</span> <span class="attr">class</span>=<span class="value">"data-table"</span><span class="tag">&gt;</span>
      <span class="tag">&lt;tr&gt;</span>
        <span class="tag">&lt;th&gt;</span>Country<span class="tag">&lt;/th&gt;</span>
        <span class="tag">&lt;th&gt;</span>GDP (Billion USD)<span class="tag">&lt;/th&gt;</span>
      <span class="tag">&lt;/tr&gt;</span>
      <span class="tag">&lt;tr&gt;</span>
        <span class="tag">&lt;td&gt;</span>United States<span class="tag">&lt;/td&gt;</span>
        <span class="tag">&lt;td</span> <span class="attr">class</span>=<span class="value">"gdp-value"</span><span class="tag">&gt;</span>21,000<span class="tag">&lt;/td&gt;</span>
      <span class="tag">&lt;/tr&gt;</span>
      <span class="tag">&lt;tr&gt;</span>
        <span class="tag">&lt;td&gt;</span>China<span class="tag">&lt;/td&gt;</span>
        <span class="tag">&lt;td</span> <span class="attr">class</span>=<span class="value">"gdp-value"</span><span class="tag">&gt;</span>14,700<span class="tag">&lt;/td&gt;</span>
      <span class="tag">&lt;/tr&gt;</span>
    <span class="tag">&lt;/table&gt;</span>
  <span class="tag">&lt;/body&gt;</span>
<span class="tag">&lt;/html&gt;</span>
        </div>

        <!-- Section 4: Basic Scraping -->
        <div class="section-divider"></div>
        <h2 id="basic-scraping"><span class="section-num">2c.4</span> Basic Scraping Techniques</h2>

        <p>Now comes the practical part: <strong>actually writing code to extract data</strong>. We'll start with the fundamentals and build up to more complex techniques. By the end of this section, you'll be able to fetch a web page, find the data you need, and extract it into a usable format.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">For our CO2 emissions scraper, we need to: (1) fetch the page at <code>en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions</code>, (2) locate the table with class <code>wikitable</code>, and (3) extract each row into a DataFrame. The code examples below show exactly how to do this.</p>
        </div>

        <p>We'll use <strong>Python with BeautifulSoup</strong> (most common) and <strong>R with rvest</strong>. Stata doesn't have native scraping capabilities, but you can call Python from Stata (not covered here, but you can ask the chatbot if you're interested!).</p>

        <h3>Step 1: Fetch the Page</h3>

        <p><strong>The problem:</strong> Before you can extract data, you need to get the HTML content of the page into your program. This is like downloading the page's source code.</p>

        <p><strong>The solution:</strong> Use a library that sends HTTP requests (the same protocol your browser uses) and receives the HTML back.</p>

        <div class="code-tabs" data-runnable="fetch-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Fetch a web page</span>
<span class="code-comment"># First, install required packages (run once in your terminal):</span>
<span class="code-comment"># pip install requests beautifulsoup4</span>

<span class="code-tooltip" data-tip="IMPORT: This keyword loads external code libraries into your script. Think of it like adding tools to your toolbox - you need to import them before you can use them."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: Python's most popular library for fetching web pages. It handles all the complex HTTP communication so you can simply say 'get this URL' and receive the page content.">requests</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: This loads just ONE specific tool from a larger library. Instead of loading everything from bs4, we only load BeautifulSoup - this is more efficient."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="BS4: Short for BeautifulSoup4, this library parses HTML code and lets you search through it. The name 'beautiful soup' refers to the messy HTML code you often find on web pages.">bs4</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP: The main class that turns raw HTML text into a searchable structure. Once you create a BeautifulSoup object, you can use methods like find() and find_all() to locate specific elements.">BeautifulSoup</span>

<span class="code-comment"># Define a User-Agent (identifies your scraper)</span>
<span class="code-comment"># Be honest - some sites block generic Python requests</span>
<span class="code-tooltip" data-tip="HEADERS: A Python dictionary containing metadata about your request. Websites use this information to know who is visiting. Being transparent about your scraper helps build trust and avoids getting blocked.">headers</span> <span class="code-tooltip" data-tip="EQUALS SIGN: In Python, = assigns a value to a variable. The variable on the left (headers) will now contain the value on the right (the dictionary).">= </span><span class="code-tooltip" data-tip="DICTIONARY (curly braces): A Python data structure that stores key-value pairs. Here, 'User-Agent' is the key and the long string is its value. Dictionaries are perfect for storing labeled information.">{
    <span class="code-tooltip" data-tip="USER-AGENT KEY: This tells the server what browser or program is making the request. Including your contact email is good etiquette - it lets website owners reach you if there's a problem.">'User-Agent'</span>: <span class="code-tooltip" data-tip="USER-AGENT VALUE: This string identifies your scraper. The Mozilla/5.0 prefix makes it look like a browser, followed by a description and your contact info. Always include real contact information for research projects.">'Mozilla/5.0 (Research scraper for academic project; contact@university.edu)'</span>
}</span>

<span class="code-comment"># Fetch the page</span>
<span class="code-tooltip" data-tip="URL VARIABLE: Stores the web address you want to scrape. Using a variable makes your code cleaner and easier to modify - you only need to change this one line to scrape a different page.">url</span> = <span class="code-tooltip" data-tip="URL STRING: The complete web address of the page you want to fetch. Always use https:// (secure) when available. This is the exact address you would type in a browser.">"https://quotes.toscrape.com/"</span>
<span class="code-tooltip" data-tip="RESPONSE VARIABLE: Stores everything the server sends back - the page content, status codes, headers, etc. We call it 'response' because it's the server's response to our request.">response</span> = <span class="code-tooltip" data-tip="REQUESTS.GET(): This function sends an HTTP GET request to fetch a webpage. GET is the standard way to retrieve data (as opposed to POST which sends data). This is like typing a URL in your browser and pressing Enter.">requests.get</span>(<span class="code-tooltip" data-tip="URL PARAMETER: The first argument tells requests.get() which webpage to fetch. This is the address you want to visit.">url</span>, <span class="code-tooltip" data-tip="HEADERS PARAMETER: Passes your custom headers (including User-Agent) to the request. This identifies your scraper to the website server.">headers=headers</span>, <span class="code-tooltip" data-tip="TIMEOUT PARAMETER: Maximum seconds to wait for a response before giving up. Without this, your script could hang forever if a server doesn't respond. 30 seconds is a reasonable default.">timeout=<span class="code-number">30</span></span>)

<span class="code-comment"># Check if request was successful</span>
<span class="code-tooltip" data-tip="IF STATEMENT: Checks a condition and only runs the indented code below if the condition is true. Here we check if the request succeeded before trying to process the response."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS_CODE: HTTP responses include a number indicating success or failure. 200 means 'OK - everything worked'. 404 means 'not found', 500 means 'server error'. Always check this before processing!">response.status_code</span> <span class="code-tooltip" data-tip="EQUALS COMPARISON (==): Two equals signs check if two values are the same. This is different from single = which assigns values. Here we're asking 'is the status code equal to 200?'">==</span> <span class="code-tooltip" data-tip="200: The HTTP status code for success. When a server returns 200, it means your request was received, understood, and processed correctly. The page content should be available."><span class="code-number">200</span></span>:
    <span class="code-tooltip" data-tip="PRINT(): Displays text in the console/terminal. Useful for seeing what your code is doing, debugging problems, and confirming that operations succeeded."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="SUCCESS MESSAGE: This string confirms the page was fetched. Adding status messages helps you track what your scraper is doing, especially when processing many pages.">"Success! Page fetched."</span>)
    <span class="code-comment"># Parse the HTML</span>
    <span class="code-tooltip" data-tip="SOUP VARIABLE: A common name for BeautifulSoup objects. Once created, this variable lets you search through the HTML using methods like find(), find_all(), and select(). The name 'soup' is a convention in the Python community.">soup</span> = <span class="code-tooltip" data-tip="BEAUTIFULSOUP CONSTRUCTOR: Creates a new BeautifulSoup object from HTML text. The first argument is the HTML content, the second specifies which parser to use. This transforms raw text into a searchable tree structure.">BeautifulSoup</span>(<span class="code-tooltip" data-tip="RESPONSE.TEXT: The actual HTML content of the webpage as a string. This is what you see when you 'View Page Source' in a browser. The .text property extracts just the content, not headers or metadata.">response.text</span>, <span class="code-tooltip" data-tip="HTML.PARSER: Python's built-in HTML parser. It's reliable and doesn't require extra installation. Other options include 'lxml' (faster) or 'html5lib' (more lenient with broken HTML).">'html.parser'</span>)
<span class="code-tooltip" data-tip="ELSE: Runs this code block when the IF condition is false. Here, if the status code is NOT 200 (something went wrong), we print an error message instead of trying to parse the response."><span class="code-keyword">else</span></span>:
    <span class="code-keyword">print</span>(<span class="code-tooltip" data-tip="F-STRING (f prefix): A Python feature that lets you embed variables directly in strings using curly braces. The {response.status_code} will be replaced with the actual number (like 404 or 500).">f"Error: {response.status_code}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Fetch a web page</span>
<span class="code-comment"># First, install required packages (run once in your R console):</span>
<span class="code-comment"># install.packages(c("rvest", "httr"))</span>

<span class="code-tooltip" data-tip="LIBRARY(): This R function loads a package (library) so you can use its functions. You must call library() before using any functions from that package. It's like opening a toolbox."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="RVEST: R's main web scraping package, created by Hadley Wickham as part of the tidyverse. The name is a play on 'harvest' - you're harvesting data from the web. It provides simple functions for extracting data from HTML.">rvest</span>)
<span class="code-tooltip" data-tip="LIBRARY() for httr: Loading a second package. R lets you use multiple packages together."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="HTTR: R package for working with HTTP (web requests). It handles the low-level details of communicating with web servers. The name comes from 'HTTP' + 'R'. Used here to set custom headers.">httr</span>)

<span class="code-comment"># Define User-Agent</span>
<span class="code-tooltip" data-tip="SET_CONFIG(): Sets global options that apply to ALL subsequent web requests in your R session. Here we're setting a User-Agent header that identifies our scraper. This is polite and helps avoid being blocked.">set_config</span>(<span class="code-tooltip" data-tip="USER_AGENT(): Creates a User-Agent header that tells websites who is making the request. Always include contact info so website owners can reach you if there's an issue with your scraper."><span class="code-function">user_agent</span></span>(<span class="code-tooltip" data-tip="USER-AGENT STRING: Your scraper's identification. Include a brief description of your project and a real contact email. This transparency is professional and helps you avoid being mistaken for a malicious bot."><span class="code-string">"Research scraper for academic project; contact@university.edu"</span></span>))

<span class="code-comment"># Fetch and parse the page in one step</span>
<span class="code-tooltip" data-tip="URL VARIABLE: Stores the web address to scrape. In R, variable names can contain dots and underscores. Using a variable makes code easier to read and modify.">url</span> <span class="code-tooltip" data-tip="ASSIGNMENT OPERATOR (<-): R's way of assigning values to variables. The arrow points from the value to the variable name. You can also use = but <- is the traditional R style.">&lt;-</span> <span class="code-tooltip" data-tip="URL STRING: The complete web address you want to fetch. R strings can use single or double quotes. This is where your target webpage lives."><span class="code-string">"https://quotes.toscrape.com/"</span></span>
<span class="code-tooltip" data-tip="PAGE VARIABLE: Stores the parsed HTML document. We call it 'page' because it represents the entire webpage. This object can then be searched using html_element() and html_elements().">page</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result of read_html() in the 'page' variable for later use.">&lt;-</span> <span class="code-tooltip" data-tip="READ_HTML(): The rvest function that does TWO things at once: (1) fetches the webpage from the URL, and (2) parses the HTML into a searchable structure. This is rvest's most important function - it's your starting point for any scraping task."><span class="code-function">read_html</span></span>(<span class="code-tooltip" data-tip="URL ARGUMENT: Passing the URL to read_html(). The function will fetch this webpage and return the parsed HTML content.">url</span>)

<span class="code-comment"># Page is now ready for extraction</span>
<span class="code-tooltip" data-tip="PRINT(): Displays output in the R console. Useful for confirming your code is working and for debugging. In RStudio, you'll see this message in the Console pane."><span class="code-function">print</span></span>(<span class="code-tooltip" data-tip="STATUS MESSAGE: A simple confirmation that the operation succeeded. Adding these messages helps you track what your script is doing, especially when processing multiple pages."><span class="code-string">"Page fetched successfully"</span></span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="fetch-1" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-green">Success! Page fetched.</span>

<span class="out-blue">Response details:</span>
Status: <span class="out-num">200</span> OK
Content-Type: text/html; charset=utf-8
Content-Length: <span class="out-num">45,231</span> bytes</div>
        </div>

        <div class="output-simulation" data-output="fetch-1" data-lang="r">
          <div class="output-header">
            <span>R Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-green">Page fetched successfully</span>

{html_document}
&lt;html&gt;
[1] &lt;head&gt;...
[2] &lt;body&gt;...</div>
        </div>

        <h3>Step 2: Extract Data</h3>

        <div class="code-tabs" data-runnable="extract-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Extract data from HTML</span>
<span class="code-comment"># pip install requests beautifulsoup4</span>
<span class="code-keyword">import</span> requests
<span class="code-keyword">from</span> bs4 <span class="code-keyword">import</span> BeautifulSoup

<span class="code-comment"># Fetch a real page to work with</span>
<span class="code-tooltip" data-tip="FETCH PAGE: We need an HTML page to practice extraction on. quotes.toscrape.com is a website built specifically for learning web scraping.">response</span> = requests.get(<span class="code-string">"https://quotes.toscrape.com/"</span>, timeout=<span class="code-number">30</span>)
<span class="code-tooltip" data-tip="BEAUTIFULSOUP: Parses the HTML into a searchable tree. Now we can use find() and find_all() to extract data.">soup</span> = BeautifulSoup(response.text, <span class="code-string">'html.parser'</span>)

<span class="code-comment"># Find a single element by tag name</span>
<span class="code-tooltip" data-tip="SOUP.FIND(): Finds the FIRST element matching your criteria. Think of it like 'Ctrl+F' - it stops at the first match. Returns None if nothing found.">title</span> = soup.find(<span class="code-tooltip" data-tip="TAG NAME 'h1': HTML heading level 1 - typically the main page title.">'h1'</span>)
<span class="code-tooltip" data-tip="TITLE.TEXT.STRIP(): .text extracts the text content (removing HTML tags), .strip() removes extra whitespace."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="F-STRING: The f prefix lets you embed variables with {curly braces}. Here we show the extracted title.">f"Page title: {title.text.strip()}"</span>)

<span class="code-comment"># Find by class name</span>
<span class="code-tooltip" data-tip="SOUP.FIND() WITH CLASS: Finds the first element matching BOTH the tag AND the class. Note the underscore in class_ (because 'class' is a reserved Python keyword).">first_quote</span> = soup.find(<span class="code-tooltip" data-tip="TAG NAME 'span': A generic inline HTML element often used to wrap text.">'span'</span>, <span class="code-tooltip" data-tip="CLASS_ PARAMETER: The underscore is required because 'class' is a reserved keyword in Python. This filters to elements with class='text'.">class_=<span class="code-string">'text'</span></span>)
<span class="code-keyword">print</span>(f<span class="code-string">"First quote: {first_quote.text}"</span>)

<span class="code-comment"># Find ALL matching elements (returns a list)</span>
<span class="code-tooltip" data-tip="SOUP.FIND_ALL(): Unlike find() which returns ONE element, find_all() returns a LIST of ALL matches. Use this when you need multiple items.">all_quotes</span> = soup.find_all(<span class="code-tooltip" data-tip="DIV TAG: A container element. On this page, each quote is wrapped in a div with class='quote'.">'div'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: Only finds divs that have class='quote'. Each one contains the quote text, author, and tags.">class_=<span class="code-string">'quote'</span></span>)
<span class="code-keyword">print</span>(f<span class="code-string">"\nFound {len(all_quotes)} quotes on this page:"</span>)

<span class="code-comment"># Loop through results and extract nested data</span>
<span class="code-tooltip" data-tip="FOR LOOP: Iterates through each quote element. Inside each quote div, we can search for sub-elements."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="QUOTE VARIABLE: On each iteration, holds one quote div element.">quote</span> <span class="code-keyword">in</span> all_quotes:
    <span class="code-tooltip" data-tip="NESTED FIND: Searching WITHIN the current quote element (not the whole page). This finds the span with class 'text' inside this specific quote.">text</span> = quote.find(<span class="code-string">'span'</span>, class_=<span class="code-string">'text'</span>).text
    <span class="code-tooltip" data-tip="CHAINED FIND: Finding the author name inside this quote. The 'small' tag with class 'author' contains the author's name.">author</span> = quote.find(<span class="code-string">'small'</span>, class_=<span class="code-string">'author'</span>).text
    <span class="code-keyword">print</span>(f<span class="code-string">"  {author}: {text[:50]}..."</span>)

<span class="code-comment"># Extract attribute values (like href from links)</span>
<span class="code-tooltip" data-tip="FIND_ALL BY CLASS: Finding all 'a' (anchor/link) elements with class 'tag'. These are the topic tags under each quote.">tag_links</span> = soup.find_all(<span class="code-tooltip" data-tip="ANCHOR TAG 'a': HTML hyperlinks. The URL is in the href attribute, the clickable text is between the tags.">'a'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: Only links with class='tag' - the topic tags like 'love', 'life', 'inspirational'.">class_=<span class="code-string">'tag'</span></span>)
<span class="code-keyword">print</span>(<span class="code-string">"\nTags found:"</span>)
<span class="code-tooltip" data-tip="FOR LOOP: Iterating through each tag link to extract both the text and the URL."><span class="code-keyword">for</span></span> tag <span class="code-keyword">in</span> tag_links[:<span class="code-number">5</span>]:  <span class="code-comment"># First 5 tags</span>
    <span class="code-tooltip" data-tip="TAG.GET('href'): Safely extracts the href attribute. Using .get() returns None if missing (instead of crashing).">href</span> = tag.get(<span class="code-string">'href'</span>)
    <span class="code-keyword">print</span>(f<span class="code-string">"  {tag.text}: {href}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Extract data from HTML using rvest</span>
<span class="code-comment"># install.packages("rvest")</span>
<span class="code-function">library</span>(rvest)

<span class="code-comment"># Fetch a real page to work with</span>
<span class="code-tooltip" data-tip="READ_HTML(): Fetches the webpage and parses the HTML into a searchable structure in one step. This is rvest's starting point for any scraping task.">page</span> &lt;- <span class="code-function">read_html</span>(<span class="code-string">"https://quotes.toscrape.com/"</span>)

<span class="code-comment"># Find a single element by tag name</span>
<span class="code-tooltip" data-tip="HTML_ELEMENT(): rvest function that finds the FIRST element matching a CSS selector (singular). For multiple matches, use html_elements() with an 's'.">title</span> &lt;- page <span class="code-tooltip" data-tip="PIPE OPERATOR (%>%): Passes the result from the left as input to the function on the right. Think of it as 'then do this'.">%>%</span>
  <span class="code-function">html_element</span>(<span class="code-tooltip" data-tip="CSS SELECTOR 'h1': Selects the first h1 (heading 1) element.">"h1"</span>) %&gt;%
  <span class="code-tooltip" data-tip="HTML_TEXT(): Extracts the text content from an HTML element, removing all tags."><span class="code-function">html_text</span></span>()
<span class="code-function">print</span>(<span class="code-function">paste</span>(<span class="code-string">"Page title:"</span>, title))

<span class="code-comment"># Find by class (use . prefix for class in CSS selectors)</span>
<span class="code-tooltip" data-tip="CSS SELECTOR 'span.text': The DOT prefix means 'with this class'. So span.text finds span elements with class='text'.">first_quote</span> &lt;- page %&gt;%
  <span class="code-function">html_element</span>(<span class="code-string">"span.text"</span>) %&gt;%
  <span class="code-function">html_text</span>()
<span class="code-function">print</span>(<span class="code-function">paste</span>(<span class="code-string">"First quote:"</span>, first_quote))

<span class="code-comment"># Find ALL matching elements (use plural html_elements)</span>
<span class="code-tooltip" data-tip="HTML_ELEMENTS() (with 's'): Returns ALL matching elements, not just the first one. Use the plural form when you expect multiple matches.">quotes</span> &lt;- page %&gt;%
  <span class="code-function">html_elements</span>(<span class="code-tooltip" data-tip="CSS SELECTOR 'div.quote': Finds all div elements with class='quote'. Each one contains the text, author, and tags.">"div.quote"</span>)

<span class="code-comment"># Extract nested data from each quote</span>
<span class="code-tooltip" data-tip="HTML_ELEMENT() inside HTML_ELEMENTS(): First we found all quote divs, now we extract the author from WITHIN each one.">authors</span> &lt;- quotes %&gt;%
  <span class="code-function">html_element</span>(<span class="code-string">"small.author"</span>) %&gt;%
  <span class="code-function">html_text</span>()
<span class="code-function">print</span>(<span class="code-function">paste</span>(<span class="code-string">"Authors:"</span>))
<span class="code-function">print</span>(authors)

<span class="code-comment"># Extract attribute values (like href from links)</span>
<span class="code-tooltip" data-tip="HTML_ATTR(): Extracts a specific attribute value. While html_text() gets content between tags, html_attr() gets attribute values like href, src, class, etc.">tag_links</span> &lt;- page %&gt;%
  <span class="code-function">html_elements</span>(<span class="code-tooltip" data-tip="CSS SELECTOR 'a.tag': Finds all anchor elements with class='tag' - the topic tags under each quote.">"a.tag"</span>) %&gt;%
  <span class="code-function">html_attr</span>(<span class="code-tooltip" data-tip="HREF ATTRIBUTE: The URL that the link points to.">"href"</span>)
<span class="code-function">print</span>(<span class="code-string">"Tag links:"</span>)
<span class="code-function">print</span>(<span class="code-function">head</span>(tag_links, <span class="code-number">5</span>))</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="extract-1" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">Page title: <span class="out-str">Quotes to Scrape</span>
First quote: <span class="out-str">&ldquo;The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.&rdquo;</span>

Found <span class="out-num">10</span> quotes on this page:
  <span class="out-str">Albert Einstein</span>: &ldquo;The world as we have created it is a process...
  <span class="out-str">J.K. Rowling</span>: &ldquo;It is our choices, Harry, that show what we t...
  <span class="out-str">Albert Einstein</span>: &ldquo;There are only two ways to live your life. On...
  <span class="out-str">Jane Austen</span>: &ldquo;The person, be it gentleman or lady, who has n...
  <span class="out-str">Marilyn Monroe</span>: &ldquo;Imperfection is beauty, madness is genius and...
  ...

Tags found:
  change: <span class="out-blue">/tag/change/page/1/</span>
  deep-thoughts: <span class="out-blue">/tag/deep-thoughts/page/1/</span>
  thinking: <span class="out-blue">/tag/thinking/page/1/</span>
  world: <span class="out-blue">/tag/world/page/1/</span>
  abilities: <span class="out-blue">/tag/abilities/page/1/</span></div>
        </div>

        <div class="output-simulation" data-output="extract-1" data-lang="r">
          <div class="output-header">
            <span>R Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">[1] <span class="out-str">"Page title: Quotes to Scrape"</span>
[1] <span class="out-str">"First quote: \u201cThe world as we have created it is a process of our thinking..."</span>

[1] <span class="out-str">"Authors:"</span>
 [1] <span class="out-str">"Albert Einstein"</span>   <span class="out-str">"J.K. Rowling"</span>
 [3] <span class="out-str">"Albert Einstein"</span>   <span class="out-str">"Jane Austen"</span>
 [5] <span class="out-str">"Marilyn Monroe"</span>    <span class="out-str">"Albert Einstein"</span>
 [7] <span class="out-str">"Andr&eacute; Gide"</span>       <span class="out-str">"Thomas A. Edison"</span>
 [9] <span class="out-str">"Eleanor Roosevelt"</span> <span class="out-str">"Steve Martin"</span>

[1] <span class="out-str">"Tag links:"</span>
[1] <span class="out-str">"/tag/change/page/1/"</span>       <span class="out-str">"/tag/deep-thoughts/page/1/"</span>
[3] <span class="out-str">"/tag/thinking/page/1/"</span>     <span class="out-str">"/tag/world/page/1/"</span>
[5] <span class="out-str">"/tag/abilities/page/1/"</span></div>
        </div>

        <h3>CSS Selectors Quick Reference</h3>
        <table style="width: 100%; margin: 1rem 0;">
          <thead>
            <tr>
              <th>Selector</th>
              <th>Meaning</th>
              <th>Example</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>tag</code></td>
              <td>Element by tag name</td>
              <td><code>div</code>, <code>table</code>, <code>a</code></td>
            </tr>
            <tr>
              <td><code>.class</code></td>
              <td>Element by class</td>
              <td><code>.price</code>, <code>.data-table</code></td>
            </tr>
            <tr>
              <td><code>#id</code></td>
              <td>Element by id</td>
              <td><code>#main-content</code></td>
            </tr>
            <tr>
              <td><code>tag.class</code></td>
              <td>Tag with specific class</td>
              <td><code>span.price</code></td>
            </tr>
            <tr>
              <td><code>parent child</code></td>
              <td>Nested elements</td>
              <td><code>table tr td</code></td>
            </tr>
            <tr>
              <td><code>[attr=value]</code></td>
              <td>By attribute</td>
              <td><code>[data-country="USA"]</code></td>
            </tr>
          </tbody>
        </table>

        <!-- Section 5: Advanced Techniques -->
        <div class="section-divider"></div>
        <h2 id="advanced-techniques"><span class="section-num">2c.5</span> Advanced Techniques</h2>

        <p>The basic techniques work great for simple pages. But real-world scraping often hits complications. In this section, we'll tackle the <strong>common challenges you'll encounter</strong> and show you how to solve them.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Beyond Our CO2 Example: Real-World Challenges</h4>
          <p style="margin-bottom: 0.75rem;">Our Wikipedia example is intentionally simple. In real research projects, you'll face additional challenges:</p>
          <ul style="margin-bottom: 0;">
            <li><strong>Pagination:</strong> Data spread across multiple pages (e.g., search results showing "Page 1 of 50")</li>
            <li><strong>Dynamic content:</strong> Modern sites load data with JavaScript‚Äîyour basic scraper sees an empty page</li>
            <li><strong>Rate limiting:</strong> Hitting a server too fast will get your IP blocked</li>
            <li><strong>Authentication:</strong> Some data requires login (not covered here‚Äîask the chatbot if you need this)</li>
          </ul>
        </div>

        <h3>Handling Pagination</h3>

        <p><strong>The problem:</strong> You've successfully scraped the first page of results, but the website shows "Page 1 of 10"‚Äîthe data you need is spread across multiple pages.</p>

        <p><strong>Why this happens:</strong> Websites break large datasets into pages to improve load times and user experience. A municipal budget might list hundreds of line items across dozens of pages.</p>

        <p><strong>The solution:</strong> Write a loop that visits each page in sequence, extracting data from each one and combining the results.</p>

        <div class="code-tabs" data-runnable="pagination">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Handle pagination</span>
<span class="code-comment"># First, install required packages (run once in your terminal):</span>
<span class="code-comment"># pip install requests beautifulsoup4</span>

<span class="code-tooltip" data-tip="IMPORT: Loading the requests library for fetching web pages."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: Python's HTTP library. We've seen this before - it handles fetching web pages.">requests</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading just BeautifulSoup from the bs4 package."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="BS4: The BeautifulSoup package for parsing HTML.">bs4</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP: The HTML parsing class we'll use to search through page content.">BeautifulSoup</span>
<span class="code-tooltip" data-tip="IMPORT TIME: Loading Python's time module, which provides functions for working with time, including pausing execution."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="TIME MODULE: Python's built-in module for time-related functions. The key function here is time.sleep() which pauses your code - essential for polite scraping that doesn't overwhelm servers.">time</span>

<span class="code-comment"># Identify your scraper</span>
<span class="code-tooltip" data-tip="HEADERS: A dictionary with our User-Agent. This identifies our scraper to the website server. Always include contact info for research projects.">headers</span> = {<span class="code-string">'User-Agent'</span>: <span class="code-string">'Mozilla/5.0 (Research scraper; contact@university.edu)'</span>}

<span class="code-comment"># Store all results</span>
<span class="code-tooltip" data-tip="ALL_DATA LIST: An empty list that will collect all items from all pages. We start empty and append items as we find them. This is our 'shopping cart' for scraped data.">all_data</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating an empty list.">=</span> <span class="code-tooltip" data-tip="EMPTY LIST []: Square brackets with nothing inside create an empty list. As we scrape each page, we'll add (append) items to this list.">[]</span>

<span class="code-comment"># Loop through pages</span>
<span class="code-tooltip" data-tip="FOR LOOP: This will repeat the indented code below once for each page number. Each iteration, page_num takes the next value in the sequence."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="PAGE_NUM VARIABLE: Holds the current page number (1, then 2, then 3, etc.). We use this to build the URL for each page.">page_num</span> <span class="code-tooltip" data-tip="IN KEYWORD: Part of the for loop syntax."><span class="code-keyword">in</span></span> <span class="code-tooltip" data-tip="RANGE(1, 11): Generates numbers from 1 to 10 (the second number is exclusive). So this creates: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. Perfect for pagination!"><span class="code-function">range</span>(<span class="code-number">1</span>, <span class="code-number">11</span>)</span>:  <span class="code-comment"># Pages 1-10</span>
    <span class="code-tooltip" data-tip="URL VARIABLE: Builds the URL for the current page by inserting the page number. This site uses /page/N/ for pagination.">url</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the constructed URL.">=</span> <span class="code-tooltip" data-tip="F-STRING WITH VARIABLE: The {page_num} gets replaced with the actual number. So when page_num is 3, this becomes 'https://quotes.toscrape.com/page/3/'.">f"https://quotes.toscrape.com/page/{page_num}/"</span>

    <span class="code-comment"># Be polite: wait between requests</span>
    <span class="code-tooltip" data-tip="TIME.SLEEP(): Pauses your code for the specified number of seconds. CRITICAL for responsible scraping! Without delays, you could overwhelm the server with too many requests per second.">time.sleep</span>(<span class="code-tooltip" data-tip="DELAY VALUE (2): Wait 2 seconds between each request. For research, 2-5 seconds is typically polite."><span class="code-number">2</span></span>)  <span class="code-comment"># Wait 2 seconds</span>

    <span class="code-tooltip" data-tip="RESPONSE: Fetching the current page.">response</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the server's response.">=</span> <span class="code-tooltip" data-tip="REQUESTS.GET(): Fetching the page at the current URL.">requests.get</span>(<span class="code-tooltip" data-tip="URL: The page address we constructed above.">url</span>, <span class="code-tooltip" data-tip="HEADERS: Including our User-Agent to identify our scraper.">headers=headers</span>, <span class="code-tooltip" data-tip="TIMEOUT: Maximum seconds to wait for a response.">timeout=<span class="code-number">30</span></span>)
    <span class="code-tooltip" data-tip="IF STATEMENT: Checking for errors before processing."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS CODE CHECK: If NOT 200 (success), something went wrong.">response.status_code</span> <span class="code-tooltip" data-tip="NOT EQUALS (!=): Checks if two values are different.">!=</span> <span class="code-tooltip" data-tip="200: The success status code."><span class="code-number">200</span></span>:
        <span class="code-keyword">print</span>(f"Error on page {page_num}, stopping")
        <span class="code-keyword">break</span>

    <span class="code-tooltip" data-tip="SOUP: Creating a BeautifulSoup object from this page's HTML.">soup</span> = BeautifulSoup(response.text, <span class="code-string">'html.parser'</span>)

    <span class="code-comment"># Extract quotes from this page</span>
    <span class="code-tooltip" data-tip="QUOTES: A list of all quote divs on this page. Each page has up to 10 quotes.">quotes</span> = soup.find_all(<span class="code-string">'div'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: Only finding divs that have class='quote'. Each quote div contains the text, author, and tags.">class_=<span class="code-string">'quote'</span></span>)

    <span class="code-comment"># Check if page is empty (end of data)</span>
    <span class="code-keyword">if</span> <span class="code-keyword">not</span> <span class="code-tooltip" data-tip="EMPTY CHECK: If quotes is an empty list, we've gone past the last page.">quotes</span>:
        <span class="code-keyword">print</span>(f"No more data after page {page_num-1}")
        <span class="code-keyword">break</span>

    <span class="code-tooltip" data-tip="NESTED FOR LOOP: Loop through each quote on this page to extract its data."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="QUOTE VARIABLE: Holds one quote div at a time.">quote</span> <span class="code-keyword">in</span> quotes:
        <span class="code-tooltip" data-tip="ALL_DATA.APPEND(): Adds a new dictionary to our collection. Each dictionary represents one scraped quote.">all_data.append</span>({
            <span class="code-tooltip" data-tip="TEXT KEY: The quote text, found in a span with class 'text'."><span class="code-string">'text'</span></span>: quote.find(<span class="code-string">'span'</span>, class_=<span class="code-string">'text'</span>).text,
            <span class="code-tooltip" data-tip="AUTHOR KEY: The author name, found in a small element with class 'author'."><span class="code-string">'author'</span></span>: quote.find(<span class="code-string">'small'</span>, class_=<span class="code-string">'author'</span>).text
        })

    <span class="code-keyword">print</span>(f"Page {page_num}: {len(quotes)} quotes")

<span class="code-keyword">print</span>(f"Total: {len(all_data)} quotes collected")</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Handle pagination</span>
<span class="code-comment"># First, install required packages (run once in your R console):</span>
<span class="code-comment"># install.packages(c("rvest", "purrr"))</span>

<span class="code-tooltip" data-tip="LIBRARY(): Loading the rvest package for web scraping."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="RVEST: R's web scraping package for parsing HTML and extracting data.">rvest</span>)
<span class="code-tooltip" data-tip="LIBRARY(): Loading another package."><span class="code-function">library</span></span>(<span class="code-tooltip" data-tip="PURRR: Part of the tidyverse, provides functional programming tools. We use it here for map_dfr() which applies a function to multiple inputs and combines results.">purrr</span>)

<span class="code-comment"># Function to scrape one page</span>
<span class="code-tooltip" data-tip="SCRAPE_PAGE: We're creating a custom function that handles scraping a single page. This makes our code reusable and cleaner - we can call it for any page number.">scrape_page</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Assigning our function definition to the scrape_page variable.">&lt;-</span> <span class="code-tooltip" data-tip="FUNCTION(): R's way of creating a new function. Everything inside the curly braces {} is the function body."><span class="code-keyword">function</span></span>(<span class="code-tooltip" data-tip="PAGE_NUM PARAMETER: The input to our function. When we call scrape_page(3), page_num becomes 3 inside the function.">page_num</span>) {
  <span class="code-comment"># Be polite: wait between requests</span>
  <span class="code-tooltip" data-tip="SYS.SLEEP(): R's function to pause execution. CRITICAL for polite scraping."><span class="code-function">Sys.sleep</span></span>(<span class="code-number">2</span>)

  <span class="code-tooltip" data-tip="URL VARIABLE: Constructing the URL for this specific page.">url</span> &lt;- <span class="code-tooltip" data-tip="PASTE0(): Concatenates strings without any separator. Creates URLs like 'https://quotes.toscrape.com/page/3/'."><span class="code-function">paste0</span></span>(<span class="code-string">"https://quotes.toscrape.com/page/"</span>, page_num, <span class="code-string">"/"</span>)
  <span class="code-tooltip" data-tip="PAGE VARIABLE: Storing the fetched and parsed HTML.">page</span> &lt;- <span class="code-function">read_html</span>(url)

  <span class="code-comment"># Extract quotes from this page</span>
  <span class="code-tooltip" data-tip="QUOTES: Finding all quote div elements on the page.">quotes</span> &lt;- page %&gt;% <span class="code-function">html_elements</span>(<span class="code-string">"div.quote"</span>)

  <span class="code-comment"># Return a data frame with text and author columns</span>
  <span class="code-tooltip" data-tip="DATA.FRAME(): Creates a structured table from the extracted data. The last expression in a function is automatically returned.">data.frame</span>(
    text = quotes %&gt;% <span class="code-function">html_element</span>(<span class="code-string">"span.text"</span>) %&gt;% <span class="code-function">html_text</span>(),
    author = quotes %&gt;% <span class="code-function">html_element</span>(<span class="code-string">"small.author"</span>) %&gt;% <span class="code-function">html_text</span>(),
    stringsAsFactors = <span class="code-keyword">FALSE</span>
  )
}

<span class="code-comment"># Scrape pages 1-10</span>
<span class="code-tooltip" data-tip="MAP_DFR(): Applies scrape_page to each number 1-10, collects all data frames, and row-binds them into one big data frame.">all_data</span> &lt;- <span class="code-function">map_dfr</span>(<span class="code-number">1</span>:<span class="code-number">10</span>, scrape_page)

<span class="code-function">print</span>(<span class="code-function">paste</span>(<span class="code-string">"Total quotes collected:"</span>, <span class="code-function">nrow</span>(all_data)))</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="pagination" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">Page <span class="out-num">1</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">2</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">3</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">4</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">5</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">6</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">7</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">8</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">9</span>: <span class="out-num">10</span> quotes
Page <span class="out-num">10</span>: <span class="out-num">10</span> quotes

<span class="out-green">Total: 100 quotes collected</span></div>
        </div>

        <div class="output-simulation" data-output="pagination" data-lang="r">
          <div class="output-header">
            <span>R Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body">Scraping page 1...
Scraping page 2...
Scraping page 3...
...
Scraping page 10...
[1] <span class="out-green">"Total quotes collected: 100"</span></div>
        </div>

        <h3>Handling Dynamic Content (JavaScript)</h3>

        <p><strong>The problem:</strong> You run your scraper but get an empty result‚Äîor the HTML you receive doesn't contain the data you can clearly see on the page. What's happening?</p>

        <p><strong>Why this happens:</strong> Modern websites often load data <em>after</em> the initial page loads, using JavaScript. When you visit the page in a browser, JavaScript runs and fetches the data. But when your scraper fetches the page, it only gets the initial HTML‚Äî<em>before</em> JavaScript has run. The data simply isn't there yet.</p>

        <p><strong>How to diagnose this:</strong> In your browser, right-click and select "View Page Source" (not "Inspect"). If you can't find your data in the source but you can see it on the rendered page, JavaScript is loading it dynamically.</p>

        <div class="info-box warning">
          <div class="info-box-title">Solutions for JavaScript-Loaded Content</div>
          <p>You have three options, in order of preference:</p>
          <ol style="margin-bottom: 0;">
            <li><strong>Find the hidden API:</strong> Open Developer Tools ‚Üí Network tab ‚Üí reload the page ‚Üí look for XHR/Fetch requests. The data often comes from a JSON API that's much easier to scrape directly!</li>
            <li><strong>Use Selenium/Playwright:</strong> These tools control a real browser that executes JavaScript. More complex, but works for any site.</li>
            <li><strong>Check for a static version:</strong> Some sites offer non-JS versions for accessibility or older browsers.</li>
          </ol>
        </div>

        <div class="code-tabs" data-runnable="selenium">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Handle JavaScript with Selenium</span>
<span class="code-comment"># First, install required packages (run once in your terminal):</span>
<span class="code-comment"># pip install selenium webdriver-manager beautifulsoup4</span>

<span class="code-tooltip" data-tip="FROM...IMPORT: Loading specific modules from the selenium package. Selenium is different from requests - it controls a real web browser!"><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="SELENIUM: A library that controls web browsers programmatically. Unlike requests which just fetches HTML, Selenium opens a real browser that can execute JavaScript, click buttons, fill forms, etc.">selenium</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="WEBDRIVER: The main Selenium module that provides browser control. It can drive Chrome, Firefox, Safari, and other browsers.">webdriver</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading the Service class for managing the browser driver."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="CHROME.SERVICE: Module for managing the ChromeDriver service that connects Selenium to Chrome.">selenium.webdriver.chrome.service</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="SERVICE: A class that manages the ChromeDriver process. It handles starting and stopping the browser driver.">Service</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading the By class for specifying how to find elements."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="COMMON.BY: Module containing constants for different ways to locate elements.">selenium.webdriver.common.by</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BY: A class with constants for locating elements - BY.CLASS_NAME, BY.ID, BY.CSS_SELECTOR, etc. Similar to BeautifulSoup's find methods but for Selenium.">By</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading WebDriverWait for waiting until elements appear."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="SUPPORT.UI: Module with utilities for waiting and user interaction.">selenium.webdriver.support.ui</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="WEBDRIVERWAIT: A class that waits for certain conditions before continuing. Essential for JavaScript-heavy pages where content loads after the initial page load.">WebDriverWait</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading expected_conditions for defining what to wait for."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="SUPPORT: Module with waiting utilities.">selenium.webdriver.support</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="EXPECTED_CONDITIONS: A collection of common conditions to wait for - element present, element clickable, text appears, etc. Aliased as 'EC' for brevity.">expected_conditions</span> <span class="code-tooltip" data-tip="AS KEYWORD: Creates an alias. Now we can write EC instead of expected_conditions."><span class="code-keyword">as</span></span> <span class="code-tooltip" data-tip="EC: Short alias for expected_conditions. Makes code cleaner and easier to type.">EC</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading webdriver-manager for automatic driver installation."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="WEBDRIVER_MANAGER: A helpful library that automatically downloads and manages the correct ChromeDriver version for your Chrome browser. Saves you from manual driver management!">webdriver_manager.chrome</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="CHROMEDRIVERMANAGER: Automatically finds and downloads the correct ChromeDriver version. Without this, you'd need to manually download the matching driver version.">ChromeDriverManager</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Also loading BeautifulSoup because we'll use it to parse the final HTML."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="BS4: BeautifulSoup library.">bs4</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP: After Selenium loads the page (with JavaScript executed), we can hand the HTML to BeautifulSoup for easier parsing.">BeautifulSoup</span>

<span class="code-comment"># Set up Chrome in headless mode (no visible window)</span>
<span class="code-tooltip" data-tip="OPTIONS: A configuration object for Chrome. We can set various browser preferences and command-line arguments.">options</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating a new options object.">=</span> <span class="code-tooltip" data-tip="WEBDRIVER.CHROMEOPTIONS(): Creates a Chrome-specific options object. You can configure things like headless mode, window size, user preferences, etc.">webdriver.ChromeOptions()</span>
<span class="code-tooltip" data-tip="OPTIONS.ADD_ARGUMENT(): Adds a command-line argument to Chrome. This is how you configure browser behavior.">options.add_argument</span>(<span class="code-tooltip" data-tip="HEADLESS FLAG: Runs Chrome without opening a visible window. Perfect for servers or automated scripts where you don't need to see the browser. The browser still works normally, just invisibly.">'--headless'</span>)
<span class="code-tooltip" data-tip="DRIVER: The WebDriver instance that controls the browser. This is your main interface for browser automation - you can load pages, click elements, fill forms, etc.">driver</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating and configuring the Chrome browser.">=</span> <span class="code-tooltip" data-tip="WEBDRIVER.CHROME(): Creates a new Chrome browser instance controlled by Selenium.">webdriver.Chrome</span>(
    <span class="code-tooltip" data-tip="SERVICE PARAMETER: Provides the ChromeDriver service. ChromeDriverManager().install() automatically downloads the correct driver version for your Chrome browser.">service=Service(ChromeDriverManager().install())</span>,
    <span class="code-tooltip" data-tip="OPTIONS PARAMETER: Passes our configuration (headless mode) to the browser.">options=options</span>
)

<span class="code-tooltip" data-tip="TRY BLOCK: Wraps code that might fail. This is important because if an error occurs, we still want to close the browser (in the finally block)."><span class="code-keyword">try</span></span>:
    <span class="code-comment"># Load the JavaScript-rendered version of quotes.toscrape.com</span>
    <span class="code-tooltip" data-tip="DRIVER.GET(): Tells the browser to navigate to a URL. The browser loads the page and executes JavaScript. The /js/ version of this site loads quotes dynamically with JavaScript.">driver.get</span>(<span class="code-tooltip" data-tip="URL: The JS-rendered version of quotes.toscrape.com. If you View Page Source, the quotes aren't there - JavaScript loads them after the page loads.">"https://quotes.toscrape.com/js/"</span>)

    <span class="code-comment"># Wait for quotes to load via JavaScript (max 10 seconds)</span>
    <span class="code-tooltip" data-tip="WAIT VARIABLE: Creates a wait object that will pause execution until certain conditions are met. The 10 is the maximum seconds to wait before giving up.">wait</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating the WebDriverWait object.">=</span> <span class="code-tooltip" data-tip="WEBDRIVERWAIT(): Creates a wait object. First argument is the driver, second is max wait time in seconds.">WebDriverWait</span>(<span class="code-tooltip" data-tip="DRIVER: Our browser instance.">driver</span>, <span class="code-tooltip" data-tip="TIMEOUT (10): Maximum seconds to wait."><span class="code-number">10</span></span>)
    <span class="code-tooltip" data-tip="WAIT.UNTIL(): Pauses execution until the specified condition is true OR timeout is reached. This is how we wait for JavaScript-loaded content to appear.">wait.until</span>(<span class="code-tooltip" data-tip="EC.PRESENCE_OF_ELEMENT_LOCATED(): Checks if an element exists in the DOM. Once an element with class 'quote' appears, the wait ends.">EC.presence_of_element_located</span>((<span class="code-tooltip" data-tip="BY.CLASS_NAME: Tells Selenium to look for an element by its CSS class name.">By.CLASS_NAME</span>, <span class="code-tooltip" data-tip="CLASS NAME STRING: The class we're waiting for. When JavaScript renders the quotes, div.quote elements appear in the DOM.">"quote"</span>)))

    <span class="code-comment"># Now page is fully loaded - get the HTML</span>
    <span class="code-tooltip" data-tip="HTML VARIABLE: Contains the complete page HTML, including all content that JavaScript added after the initial load.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the page HTML.">=</span> <span class="code-tooltip" data-tip="DRIVER.PAGE_SOURCE: Gets the current HTML of the page. Unlike requests.get(), this includes everything JavaScript has added. This is the 'after JavaScript' version of the page.">driver.page_source</span>
    <span class="code-tooltip" data-tip="SOUP: Now we can use BeautifulSoup to parse this JavaScript-rendered HTML.">soup</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating the BeautifulSoup object.">=</span> <span class="code-tooltip" data-tip="BEAUTIFULSOUP(): Parsing the HTML for easy searching.">BeautifulSoup</span>(<span class="code-tooltip" data-tip="HTML: The page source including JavaScript-loaded content.">html</span>, <span class="code-tooltip" data-tip="PARSER: Using Python's built-in parser.">'html.parser'</span>)

    <span class="code-comment"># Extract data as usual with BeautifulSoup</span>
    <span class="code-tooltip" data-tip="QUOTES: Finding all quote divs. These were loaded by JavaScript - a plain requests.get() would not have found them!">quotes</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the found quotes.">=</span> <span class="code-tooltip" data-tip="SOUP.FIND_ALL(): Standard BeautifulSoup search, same as before.">soup.find_all</span>(<span class="code-tooltip" data-tip="DIV TAG: Finding div elements.">'div'</span>, <span class="code-tooltip" data-tip="CLASS FILTER: With class 'quote'.">class_=<span class="code-string">'quote'</span></span>)
    <span class="code-tooltip" data-tip="SUCCESS MESSAGE: Confirming the extraction worked."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="STATUS: Shows how many quotes were found after JavaScript loaded them.">f"Found {len(quotes)} quotes loaded by JavaScript!"</span>)

<span class="code-tooltip" data-tip="FINALLY BLOCK: This code ALWAYS runs, whether try succeeded or raised an error. Perfect for cleanup tasks like closing the browser."><span class="code-keyword">finally</span></span>:
    <span class="code-comment"># Always close the browser</span>
    <span class="code-tooltip" data-tip="DRIVER.QUIT(): Closes the browser and ends the ChromeDriver process. CRITICAL! Without this, you'll have zombie Chrome processes running in the background, eating up memory. Always close your browsers!">driver.quit()</span></code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="selenium" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-blue">Starting Chrome in headless mode...</span>
<span class="out-blue">Navigating to https://quotes.toscrape.com/js/...</span>
<span class="out-blue">Waiting for JavaScript to load quotes...</span>
<span class="out-green">Element found after 1.8 seconds</span>
<span class="out-green">Found 10 quotes loaded by JavaScript!</span></div>
        </div>

        <!-- Section 6: Best Practices -->
        <div class="section-divider"></div>
        <h2 id="best-practices"><span class="section-num">2c.6</span> Best Practices for Research</h2>

        <p>You now know <em>how</em> to scrape. This section is about <strong>how to scrape responsibly</strong>‚Äîboth for ethical reasons and practical ones. A well-designed scraper is more likely to succeed and less likely to get you in trouble.</p>

        <div class="concept-box" style="background: #fef3c7; border-left-color: #d97706;">
          <h4 style="color: #92400e;">üîç Applying This to Our Project</h4>
          <p style="margin-bottom: 0;">For our CO2 emissions scraper, best practices mean: (1) adding a 1-2 second delay between requests if scraping multiple pages, (2) setting a User-Agent that identifies our scraper, (3) caching pages locally so we don't re-download during development, and (4) handling errors gracefully if Wikipedia is temporarily unavailable.</p>
        </div>

        <div class="checklist">
          <h4>The Gentleman-Scraper's Checklist</h4>
          <ul>
            <li><strong>Identify yourself:</strong> Use a clear User-Agent with contact info</li>
            <li><strong>Respect robots.txt:</strong> Don't access disallowed pages</li>
            <li><strong>Rate limit:</strong> Add delays (2+ seconds) between requests</li>
            <li><strong>Cache responses:</strong> Don't re-fetch pages you've already downloaded</li>
            <li><strong>Handle errors gracefully:</strong> Don't hammer a server if it returns errors</li>
            <li><strong>Scrape during off-hours:</strong> Minimize impact on the server</li>
            <li><strong>Only take what you need:</strong> Don't download entire websites</li>
            <li><strong>Store data securely:</strong> Especially if it contains personal information</li>
            <li><strong>Document your methodology:</strong> For reproducibility and ethics review</li>
            <li><strong>Consider reaching out:</strong> Website owners may provide data directly</li>
          </ul>
        </div>

        <h3>Rate Limiting and Caching</h3>

        <p>These two techniques solve different problems, but they're both essential for any serious scraping project.</p>

        <div class="concept-box">
          <h4>üö¶ Rate Limiting: Why You Need It</h4>
          <p><strong>The problem:</strong> Your scraper can request pages much faster than a human would‚Äîpotentially hundreds per second. This can overwhelm the server, slow down the website for other users, or trigger security measures that block you.</p>
          <p style="margin-bottom: 0;"><strong>The solution:</strong> Add a deliberate delay between requests. A 2-3 second delay is polite. Some sites specify a <code>Crawl-delay</code> in their robots.txt‚Äîalways respect it.</p>
        </div>

        <div class="concept-box">
          <h4>üíæ Caching: Why You Need It</h4>
          <p><strong>The problem:</strong> While developing your scraper, you'll run it many times‚Äîtesting, fixing bugs, adjusting selectors. Each run re-downloads pages you already have, wasting time and putting unnecessary load on the server.</p>
          <p><strong>The solution:</strong> Save each page's HTML to a local file. Before fetching a URL, check if you already have it cached. This makes development faster and reduces server load.</p>
          <p style="margin-bottom: 0;"><strong>Bonus benefit:</strong> If your scraper crashes halfway through, you don't lose progress‚Äîcached pages don't need to be re-downloaded.</p>
        </div>

        <div class="code-tabs" data-runnable="polite">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Polite scraping with rate limiting and caching</span>
<span class="code-comment"># First, install required packages (run once in your terminal):</span>
<span class="code-comment"># pip install requests</span>

<span class="code-tooltip" data-tip="IMPORT: Loading the requests library."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: For fetching web pages.">requests</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the time module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="TIME: For adding delays and tracking request timing.">time</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the hashlib module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="HASHLIB: Provides hashing functions like MD5. We use it to create unique cache filenames from URLs.">hashlib</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the os module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="OS: Operating system utilities. Used for file/folder operations.">os</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading Path from pathlib."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="PATHLIB: A modern way to work with file paths in Python. More readable than os.path.">pathlib</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="PATH: A class representing filesystem paths. Makes working with directories and files much cleaner.">Path</span>

<span class="code-tooltip" data-tip="CLASS DEFINITION: Creating a new class called PoliteScraper. A class is like a blueprint - it bundles related data and functions together. This class handles rate limiting and caching automatically."><span class="code-keyword">class</span></span> <span class="code-tooltip" data-tip="POLITESCRAPER: Our custom scraper class name. The name reflects its purpose - being polite to servers by rate limiting and caching."><span class="code-function">PoliteScraper</span></span>:
    <span class="code-tooltip" data-tip="__INIT__ METHOD (Constructor): A special method that runs when you create a new PoliteScraper object. It sets up all the initial values the scraper needs."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="__INIT__: The initialization method. Double underscores indicate special Python methods."><span class="code-function">__init__</span></span>(<span class="code-tooltip" data-tip="SELF: A reference to the object being created. Required as the first parameter of all methods. Through 'self', methods can access and modify the object's data.">self</span>, <span class="code-tooltip" data-tip="DELAY PARAMETER: How many seconds to wait between requests. Default is 2. You can change this when creating a scraper.">delay=<span class="code-number">2</span></span>, <span class="code-tooltip" data-tip="CACHE_DIR PARAMETER: The folder name for storing cached pages. Default is 'scraper_cache'.">cache_dir=<span class="code-string">"scraper_cache"</span></span>):
        <span class="code-tooltip" data-tip="SELF.DELAY: Stores the delay value on the object. Using 'self.' makes it accessible to other methods in the class.">self.delay</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the delay value.">=</span> <span class="code-tooltip" data-tip="DELAY: The value passed to the constructor.">delay</span>
        <span class="code-tooltip" data-tip="SELF.CACHE_DIR: Stores the cache directory path. Path() converts the string to a Path object for easier file operations.">self.cache_dir</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating and storing the Path object.">=</span> <span class="code-tooltip" data-tip="PATH(): Creates a Path object from the directory name string.">Path(cache_dir)</span>
        <span class="code-tooltip" data-tip="MKDIR(): Creates the directory if it doesn't exist. exist_ok=True means it won't raise an error if the directory already exists.">self.cache_dir.mkdir</span>(<span class="code-tooltip" data-tip="EXIST_OK: If True, don't raise an error when the directory exists. Perfect for setup code that might run multiple times.">exist_ok=<span class="code-keyword">True</span></span>)
        <span class="code-tooltip" data-tip="SELF.LAST_REQUEST: Tracks when we last made a request (Unix timestamp). Starts at 0, meaning 'never'. Used for rate limiting.">self.last_request</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Initializing to 0.">=</span> <span class="code-tooltip" data-tip="ZERO: No requests made yet."><span class="code-number">0</span></span>
        <span class="code-tooltip" data-tip="SELF.SESSION: A requests Session object. Sessions persist settings like headers and cookies across multiple requests, which is more efficient and polite.">self.session</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Creating a new Session.">=</span> <span class="code-tooltip" data-tip="REQUESTS.SESSION(): Creates a session that maintains settings across requests. More efficient than creating new connections each time.">requests.Session()</span>
        <span class="code-tooltip" data-tip="SESSION.HEADERS.UPDATE(): Sets default headers for all requests made through this session. The User-Agent identifies our scraper.">self.session.headers.update</span>({
            <span class="code-tooltip" data-tip="USER-AGENT HEADER: Identifies our scraper to websites. Include your contact info so website owners can reach you.">'User-Agent'</span>: <span class="code-tooltip" data-tip="USER-AGENT VALUE: Our scraper identification string.">'Research Scraper (contact@university.edu)'</span>
        })

    <span class="code-tooltip" data-tip="METHOD DEFINITION: Creating a helper method (the underscore prefix _ suggests it's for internal use)."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="_GET_CACHE_PATH: A private helper method (underscore = internal). Converts a URL to a cache file path. Each URL gets a unique filename based on its hash."><span class="code-function">_get_cache_path</span></span>(<span class="code-tooltip" data-tip="SELF: Reference to the scraper object.">self</span>, <span class="code-tooltip" data-tip="URL PARAMETER: The URL we want to cache.">url</span>):
        <span class="code-comment"># Create unique filename from URL</span>
        <span class="code-tooltip" data-tip="URL_HASH: A unique 32-character string generated from the URL. Even slightly different URLs produce completely different hashes.">url_hash</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the hash.">=</span> <span class="code-tooltip" data-tip="HASHLIB.MD5().HEXDIGEST(): Creates an MD5 hash of the URL and converts it to a hexadecimal string. This creates a unique, filesystem-safe filename from any URL.">hashlib.md5(url.encode()).hexdigest()</span>
        <span class="code-tooltip" data-tip="RETURN: Sends a value back to the caller. The method returns the full path to the cache file."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="PATH CONCATENATION: Using / operator with Path objects to build file paths. Creates something like 'scraper_cache/a1b2c3d4.html'.">self.cache_dir / f"{url_hash}.html"</span>

    <span class="code-tooltip" data-tip="METHOD DEFINITION: The main fetch method that users will call."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="FETCH METHOD: The main function to get a webpage. Handles caching and rate limiting automatically."><span class="code-function">fetch</span></span>(<span class="code-tooltip" data-tip="SELF: Reference to the scraper.">self</span>, <span class="code-tooltip" data-tip="URL PARAMETER: The webpage to fetch.">url</span>, <span class="code-tooltip" data-tip="USE_CACHE PARAMETER: If True, use cached version if available. If False, always fetch fresh. Default is True for efficiency.">use_cache=<span class="code-keyword">True</span></span>):
        <span class="code-tooltip" data-tip="CACHE_PATH: Get the path where this URL's cache file would be stored.">cache_path</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the cache path.">=</span> <span class="code-tooltip" data-tip="CALLING _GET_CACHE_PATH: Using our helper method to generate the cache filename.">self._get_cache_path(url)</span>

        <span class="code-comment"># Check cache first</span>
        <span class="code-tooltip" data-tip="IF STATEMENT: Checking if we should use the cache."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="USE_CACHE: Only check cache if caching is enabled.">use_cache</span> <span class="code-tooltip" data-tip="AND: Both conditions must be true."><span class="code-keyword">and</span></span> <span class="code-tooltip" data-tip="CACHE_PATH.EXISTS(): Checks if the cache file exists on disk. If so, we can skip the network request!">cache_path.exists()</span>:
            <span class="code-tooltip" data-tip="PRINT CACHE HIT: Informing the user we're using cached data."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="CACHE MESSAGE: Shows which URL is being loaded from cache.">f"Using cached version of {url}"</span>)
            <span class="code-tooltip" data-tip="RETURN: Send the cached content back immediately, skipping the network request."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="CACHE_PATH.READ_TEXT(): Reads the entire cache file as a string. Fast and no network needed!">cache_path.read_text()</span>

        <span class="code-comment"># Rate limiting</span>
        <span class="code-tooltip" data-tip="ELAPSED: Calculate how long since our last request. time.time() returns the current time in seconds.">elapsed</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the elapsed time.">=</span> <span class="code-tooltip" data-tip="TIME DIFFERENCE: Current time minus last request time gives seconds elapsed.">time.time() - self.last_request</span>
        <span class="code-tooltip" data-tip="IF STATEMENT: Checking if we need to wait."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="RATE LIMIT CHECK: If less time has passed than our required delay, we need to wait.">elapsed < self.delay</span>:
            <span class="code-tooltip" data-tip="TIME.SLEEP(): Pause for just the right amount of time. If delay is 2 seconds and 0.5 seconds passed, sleep for 1.5 seconds. Smart rate limiting!">time.sleep</span>(<span class="code-tooltip" data-tip="WAIT TIME: Only wait the remaining time needed, not the full delay.">self.delay - elapsed</span>)

        <span class="code-comment"># Make request</span>
        <span class="code-tooltip" data-tip="RESPONSE: Fetching the page through our session (which includes our headers).">response</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the response.">=</span> <span class="code-tooltip" data-tip="SESSION.GET(): Making the request through our session, which automatically includes our User-Agent header.">self.session.get</span>(<span class="code-tooltip" data-tip="URL: The page to fetch.">url</span>, <span class="code-tooltip" data-tip="TIMEOUT: Maximum wait time for response.">timeout=<span class="code-number">30</span></span>)
        <span class="code-tooltip" data-tip="UPDATE LAST_REQUEST: Recording when we made this request for future rate limiting.">self.last_request</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the current timestamp.">=</span> <span class="code-tooltip" data-tip="TIME.TIME(): Current Unix timestamp.">time.time()</span>

        <span class="code-tooltip" data-tip="IF STATEMENT: Checking for success."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS CHECK: 200 means success.">response.status_code == <span class="code-number">200</span></span>:
            <span class="code-comment"># Cache the response</span>
            <span class="code-tooltip" data-tip="WRITE_TEXT(): Saves the HTML content to the cache file. Next time we request this URL, we'll use this cached version.">cache_path.write_text</span>(<span class="code-tooltip" data-tip="RESPONSE.TEXT: The HTML content to cache.">response.text</span>)
            <span class="code-tooltip" data-tip="RETURN: Send the HTML back to the caller."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="RESPONSE.TEXT: The fetched HTML content.">response.text</span>
        <span class="code-tooltip" data-tip="ELSE: Handle errors."><span class="code-keyword">else</span></span>:
            <span class="code-tooltip" data-tip="RAISE: Throws an error that stops execution. This alerts the caller that something went wrong."><span class="code-keyword">raise</span></span> <span class="code-tooltip" data-tip="EXCEPTION: Creates an error with a descriptive message."><span class="code-function">Exception</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Includes the status code for debugging.">f"HTTP {response.status_code}"</span>)

<span class="code-comment"># Usage</span>
<span class="code-tooltip" data-tip="SCRAPER INSTANCE: Creating a new PoliteScraper object with a 3-second delay between requests.">scraper</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing our scraper object.">=</span> <span class="code-tooltip" data-tip="POLITESCRAPER(): Creates a new scraper. We're customizing the delay to 3 seconds.">PoliteScraper</span>(<span class="code-tooltip" data-tip="DELAY ARGUMENT: Setting a 3-second minimum wait between requests.">delay=<span class="code-number">3</span></span>)
<span class="code-tooltip" data-tip="HTML VARIABLE: Will store the fetched/cached page content.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result.">=</span> <span class="code-tooltip" data-tip="SCRAPER.FETCH(): Fetching a page. First call will make a network request and cache it.">scraper.fetch</span>(<span class="code-tooltip" data-tip="URL: The first page to fetch.">"https://quotes.toscrape.com/page/1/"</span>)
<span class="code-tooltip" data-tip="HTML: Fetching a second page. The scraper automatically waits 3 seconds between requests.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the second page.">=</span> <span class="code-tooltip" data-tip="SCRAPER.FETCH(): Fetching another page. Rate limiting ensures we wait before requesting.">scraper.fetch</span>(<span class="code-tooltip" data-tip="URL: The second page to fetch.">"https://quotes.toscrape.com/page/2/"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="polite" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-blue">Fetching https://quotes.toscrape.com/page/1/...</span>
<span class="out-green">Cached to scraper_cache/a1b2c3d4.html</span>

<span class="out-blue">Waiting 3 seconds...</span>

<span class="out-blue">Fetching https://quotes.toscrape.com/page/2/...</span>
<span class="out-green">Cached to scraper_cache/e5f6g7h8.html</span>

<span class="out-str">Next run will use cached versions!</span></div>
        </div>

        <h3>Error Handling and Retries</h3>

        <p><strong>The problem:</strong> When scraping 200+ sites, things <em>will</em> go wrong. Servers will be temporarily unavailable, your internet connection will hiccup, some pages will return errors. If your scraper crashes at error #50, you lose all progress.</p>

        <p><strong>The solution:</strong> Build resilience into your scraper. When a request fails, wait and try again (this is called "retrying with exponential backoff"). Only give up after multiple failures.</p>

        <div class="info-box note">
          <div class="info-box-title">What is Exponential Backoff?</div>
          <p style="margin-bottom: 0;">Instead of retrying immediately (which might fail again), you wait progressively longer: 5 seconds, then 10 seconds, then 20 seconds. This gives the server time to recover and avoids hammering it when it's already struggling.</p>
        </div>

        <div class="code-tabs" data-runnable="errors">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Robust error handling</span>
<span class="code-comment"># First, install required packages (run once in your terminal):</span>
<span class="code-comment"># pip install requests</span>

<span class="code-tooltip" data-tip="IMPORT: Loading the requests library."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="REQUESTS: HTTP library for making web requests.">requests</span>
<span class="code-tooltip" data-tip="IMPORT: Loading the time module."><span class="code-keyword">import</span></span> <span class="code-tooltip" data-tip="TIME: For adding delays between retry attempts.">time</span>
<span class="code-tooltip" data-tip="FROM...IMPORT: Loading specific exception type."><span class="code-keyword">from</span></span> <span class="code-tooltip" data-tip="REQUESTS.EXCEPTIONS: Module containing all exception types that requests can raise.">requests.exceptions</span> <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="REQUESTEXCEPTION: The base exception class for all requests errors - connection failures, timeouts, etc. Catching this handles all network-related errors.">RequestException</span>

<span class="code-tooltip" data-tip="FUNCTION DEFINITION: Creating a reusable function for fetching URLs with automatic retry logic."><span class="code-keyword">def</span></span> <span class="code-tooltip" data-tip="FETCH_WITH_RETRY: A function that tries multiple times to fetch a URL. If the first attempt fails, it waits and tries again. This makes your scraper resilient to temporary network issues."><span class="code-function">fetch_with_retry</span></span>(<span class="code-tooltip" data-tip="URL PARAMETER: The webpage to fetch.">url</span>, <span class="code-tooltip" data-tip="MAX_RETRIES: How many times to try before giving up. Default is 3 attempts total.">max_retries=<span class="code-number">3</span></span>, <span class="code-tooltip" data-tip="BASE_DELAY: Starting wait time in seconds. This gets multiplied for exponential backoff (5s, then 10s, then 20s...).">base_delay=<span class="code-number">5</span></span>):
    <span class="code-tooltip" data-tip="DOCSTRING: A documentation string explaining what this function does. Triple quotes allow multi-line strings. Good practice for documenting your code."><span class="code-string">"""Fetch URL with exponential backoff on failure."""</span></span>

    <span class="code-tooltip" data-tip="FOR LOOP: Try up to max_retries times. 'attempt' will be 0, 1, 2 (for 3 retries)."><span class="code-keyword">for</span></span> <span class="code-tooltip" data-tip="ATTEMPT VARIABLE: Tracks which attempt we're on (0-based). Used to calculate exponential backoff.">attempt</span> <span class="code-tooltip" data-tip="IN KEYWORD: Part of the for loop."><span class="code-keyword">in</span></span> <span class="code-tooltip" data-tip="RANGE(MAX_RETRIES): Creates sequence 0, 1, 2 for 3 retries. Each iteration is one attempt."><span class="code-function">range</span>(max_retries)</span>:
        <span class="code-tooltip" data-tip="TRY BLOCK: Wraps code that might raise an exception. If an error occurs, execution jumps to the except block instead of crashing."><span class="code-keyword">try</span></span>:
            <span class="code-tooltip" data-tip="RESPONSE: Making the actual request.">response</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the response.">=</span> <span class="code-tooltip" data-tip="REQUESTS.GET(): Fetching the URL with a 30-second timeout.">requests.get</span>(<span class="code-tooltip" data-tip="URL: The page to fetch.">url</span>, <span class="code-tooltip" data-tip="TIMEOUT: Maximum wait time before giving up on this request.">timeout=<span class="code-number">30</span></span>)

            <span class="code-comment"># Success</span>
            <span class="code-tooltip" data-tip="IF STATEMENT: Checking for success status."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="STATUS CODE 200: Success! Everything worked.">response.status_code == <span class="code-number">200</span></span>:
                <span class="code-tooltip" data-tip="RETURN: Exit the function immediately and send back the HTML. Success means we don't need to retry."><span class="code-keyword">return</span></span> <span class="code-tooltip" data-tip="RESPONSE.TEXT: The HTML content.">response.text</span>

            <span class="code-comment"># Rate limited - wait and retry</span>
            <span class="code-tooltip" data-tip="ELIF (else if): Another condition to check if the previous one was false."><span class="code-keyword">elif</span></span> <span class="code-tooltip" data-tip="STATUS CODE 429: 'Too Many Requests' - the server is telling us to slow down. This is rate limiting in action.">response.status_code == <span class="code-number">429</span></span>:
                <span class="code-tooltip" data-tip="WAIT_TIME: How long to wait. We check the Retry-After header first (the server tells us), or fall back to base_delay.">wait_time</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the wait time.">=</span> <span class="code-tooltip" data-tip="INT(): Converts to integer since the header might be a string.">int</span>(<span class="code-tooltip" data-tip="HEADERS.GET(): Safely gets a header value. Retry-After is a standard header telling you how long to wait. If missing, we use base_delay as default.">response.headers.get</span>(<span class="code-tooltip" data-tip="RETRY-AFTER HEADER: A standard HTTP header that tells you how many seconds to wait before trying again.">'Retry-After'</span>, <span class="code-tooltip" data-tip="DEFAULT VALUE: If the header doesn't exist, use base_delay instead.">base_delay</span>))
                <span class="code-tooltip" data-tip="PRINT STATUS: Informing the user about the rate limit."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="RATE LIMIT MESSAGE: Shows how long we'll wait.">f"Rate limited. Waiting {wait_time}s..."</span>)
                <span class="code-tooltip" data-tip="TIME.SLEEP(): Pausing for the specified duration before the next attempt.">time.sleep</span>(<span class="code-tooltip" data-tip="WAIT_TIME: The duration to pause.">wait_time</span>)

            <span class="code-comment"># Server error - wait and retry</span>
            <span class="code-tooltip" data-tip="ELIF: Checking for server errors."><span class="code-keyword">elif</span></span> <span class="code-tooltip" data-tip="STATUS >= 500: Any 5xx status code indicates a server error (500 Internal Error, 502 Bad Gateway, 503 Service Unavailable, etc.). These are usually temporary - worth retrying.">response.status_code >= <span class="code-number">500</span></span>:
                <span class="code-tooltip" data-tip="DELAY CALCULATION: Exponential backoff - each retry waits longer. First retry: 5s, second: 10s, third: 20s. This gives the server time to recover.">delay</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Calculating the delay.">=</span> <span class="code-tooltip" data-tip="EXPONENTIAL BACKOFF FORMULA: base_delay * (2 ^ attempt). When attempt=0: 5*1=5s. attempt=1: 5*2=10s. attempt=2: 5*4=20s. This prevents hammering a struggling server.">base_delay * (<span class="code-number">2</span> ** attempt)</span>
                <span class="code-tooltip" data-tip="PRINT STATUS: Informing about the server error."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Shows the wait time.">f"Server error. Retry in {delay}s..."</span>)
                <span class="code-tooltip" data-tip="TIME.SLEEP(): Waiting before retry.">time.sleep</span>(<span class="code-tooltip" data-tip="DELAY: The calculated exponential backoff time.">delay</span>)

            <span class="code-comment"># Client error - don't retry</span>
            <span class="code-tooltip" data-tip="ELSE: Handles other status codes (4xx client errors like 404 Not Found, 403 Forbidden). These usually won't be fixed by retrying - the problem is with our request."><span class="code-keyword">else</span></span>:
                <span class="code-tooltip" data-tip="RAISE: Immediately stop and report the error. Client errors (like 404) won't be fixed by retrying, so we give up."><span class="code-keyword">raise</span></span> <span class="code-tooltip" data-tip="EXCEPTION: Creating an error with the status code."><span class="code-function">Exception</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Includes the status code for debugging.">f"HTTP {response.status_code}"</span>)

        <span class="code-tooltip" data-tip="EXCEPT BLOCK: Catches network-level errors like connection refused, timeout, DNS failure. These are different from HTTP status codes - the request didn't even complete."><span class="code-keyword">except</span></span> <span class="code-tooltip" data-tip="REQUESTEXCEPTION: Catches all requests-related errors - connection failed, timed out, DNS error, etc.">RequestException</span> <span class="code-tooltip" data-tip="AS KEYWORD: Captures the exception object in variable 'e' so we can examine it."><span class="code-keyword">as</span></span> <span class="code-tooltip" data-tip="E: The exception object containing error details.">e</span>:
            <span class="code-tooltip" data-tip="PRINT ERROR: Showing what went wrong."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="ERROR MESSAGE: Shows the exception details.">f"Request failed: {e}"</span>)
            <span class="code-tooltip" data-tip="IF STATEMENT: Check if we have retries remaining."><span class="code-keyword">if</span></span> <span class="code-tooltip" data-tip="RETRIES REMAINING CHECK: If attempt is less than max_retries-1, we still have more tries. For 3 max_retries, we retry when attempt is 0 or 1.">attempt < max_retries - <span class="code-number">1</span></span>:
                <span class="code-tooltip" data-tip="DELAY: Calculate exponential backoff for this retry.">delay</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the delay.">=</span> <span class="code-tooltip" data-tip="EXPONENTIAL BACKOFF: Same formula as before.">base_delay * (<span class="code-number">2</span> ** attempt)</span>
                <span class="code-tooltip" data-tip="PRINT RETRY MESSAGE: Informing about the upcoming retry."><span class="code-keyword">print</span></span>(<span class="code-tooltip" data-tip="RETRY MESSAGE: Shows when we'll try again.">f"Retrying in {delay}s..."</span>)
                <span class="code-tooltip" data-tip="TIME.SLEEP(): Wait before retrying.">time.sleep</span>(<span class="code-tooltip" data-tip="DELAY: The backoff duration.">delay</span>)

    <span class="code-tooltip" data-tip="RAISE AFTER LOOP: If we exit the for loop, all retries have been exhausted without success. Give up and report failure."><span class="code-keyword">raise</span></span> <span class="code-tooltip" data-tip="EXCEPTION: Final failure after all retries."><span class="code-function">Exception</span></span>(<span class="code-tooltip" data-tip="FAILURE MESSAGE: Reports how many attempts were made before giving up.">f"Failed after {max_retries} attempts"</span>)

<span class="code-comment"># Usage</span>
<span class="code-tooltip" data-tip="HTML VARIABLE: Will store the successfully fetched content.">html</span> <span class="code-tooltip" data-tip="ASSIGNMENT: Storing the result.">=</span> <span class="code-tooltip" data-tip="FETCH_WITH_RETRY(): Calling our robust fetching function. It will automatically retry on failures.">fetch_with_retry</span>(<span class="code-tooltip" data-tip="URL: The page to fetch. The function handles errors automatically.">"https://quotes.toscrape.com/"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="errors" data-lang="python">
          <div class="output-header">
            <span>Python Output</span>
            <button class="close-output">&times;</button>
          </div>
          <div class="output-body"><span class="out-str">Fetching https://quotes.toscrape.com/...</span>
<span class="out-green">Success! (status 200)</span>
<span class="out-str">
<i>(If the server were temporarily down, you'd see:)</i></span>
<span class="out-yellow">Server error. Retry in 5s...</span>
<span class="out-yellow">Server error. Retry in 10s...</span>
<span class="out-green">Success on attempt 3!</span></div>
        </div>

        <div class="info-box tip">
          <div class="info-box-title">Save Your Work Incrementally</div>
          <p style="margin-bottom: 0;">When scraping large amounts of data, save to disk after each page. If your script crashes after 2 hours, you don't want to lose everything. Use caching (shown above) or append to a CSV/database as you go.</p>
        </div>

        <!-- Common Challenges Quick Reference -->
        <div class="section-divider"></div>
        <h3 style="color: #1e3a5f; font-size: 1.4rem; margin-top: 2rem;">üõ†Ô∏è Troubleshooting: Common Problems & Solutions</h3>

        <p>Here's a quick reference for problems you'll likely encounter. Bookmark this section!</p>

        <div style="display: grid; gap: 1rem; margin: 1.5rem 0;">
          <!-- Problem 1 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "My scraper returns empty results"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> JavaScript is loading the content. <strong>Fix:</strong> Check if View Page Source shows the data. If not, use Selenium or find the underlying API in the Network tab.</p>
          </div>

          <!-- Problem 2 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "I'm getting blocked (403 Forbidden)"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> You're hitting the server too fast, or missing a User-Agent header. <strong>Fix:</strong> Add longer delays (5+ seconds), set a proper User-Agent, and respect robots.txt.</p>
          </div>

          <!-- Problem 3 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "My selector worked yesterday but fails today"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> The website changed its HTML structure. <strong>Fix:</strong> Re-inspect the page and update your selectors. This is why scraping requires ongoing maintenance.</p>
          </div>

          <!-- Problem 4 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "I'm getting encoding errors (weird characters)"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> Character encoding mismatch. <strong>Fix:</strong> Check the page's encoding in the response headers or meta tag. Use <code>response.encoding = 'utf-8'</code> (Python) before accessing <code>response.text</code>.</p>
          </div>

          <!-- Problem 5 -->
          <div style="background: #fff; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem 1.25rem;">
            <p style="margin: 0 0 0.5rem 0; font-weight: 600; color: #dc2626;">‚ùå "My scraper works but is painfully slow"</p>
            <p style="margin: 0; color: #475569; font-size: 0.95rem;"><strong>Likely cause:</strong> You're re-downloading pages unnecessarily. <strong>Fix:</strong> Implement caching! Also consider if you really need every page‚Äîmaybe you can sample or prioritize.</p>
          </div>
        </div>

        <!-- Project Completion Summary -->
        <div class="research-banner" style="background: linear-gradient(135deg, #059669 0%, #10b981 100%); margin-top: 2rem;">
          <h3>‚úÖ What You've Learned: CO2 Emissions Scraper</h3>
          <p style="margin-bottom: 1rem;">You can now build a complete scraper for the Wikipedia CO2 emissions table‚Äîand merge it with your World Bank data for a richer climate analysis:</p>
          <ol style="margin-bottom: 0; padding-left: 1.5rem;">
            <li><strong>Legal check:</strong> Verified Wikipedia's CC license and robots.txt permit scraping</li>
            <li><strong>HTML inspection:</strong> Used browser DevTools to find the <code>table.wikitable</code> selector</li>
            <li><strong>Data extraction:</strong> Fetched the page and parsed the table with BeautifulSoup/rvest</li>
            <li><strong>Error handling:</strong> Built resilience with retries and exponential backoff</li>
            <li><strong>Best practices:</strong> Rate limiting, caching, and respectful User-Agent headers</li>
          </ol>
          <p style="margin-top: 1rem; margin-bottom: 0; font-size: 0.95rem; opacity: 0.9;"><strong>Next steps:</strong> In <a href="03-data-exploration.html" style="color: #fbd38d;">Module 3: Data Exploration</a>, we'll continue with our Climate Vulnerability project, exploring the combined World Bank and scraped datasets. Try extending this scraper to also grab <a href="https://en.wikipedia.org/wiki/List_of_countries_by_greenhouse_gas_emissions" style="color: #fbd38d;" target="_blank">greenhouse gas emissions</a> data!</p>
        </div>

        <!-- ‚ïê‚ïê‚ïê MODULE 02c GRAMMAR PRIMER ‚ïê‚ïê‚ïê -->
        <section class="grammar-primer-section">
          <h2 id="code-grammar">Code Grammar</h2>
          <p>Practice the syntax patterns used in this module. In <strong>Build</strong> mode, read the descriptions and figure out the code. In <strong>Read</strong> mode, look at the code and identify what each piece does. Click cards to check your answers.</p>
          <div class="grammar-primer">
            <script type="application/json">
            {
              "exercises": [
                {
                  "instruction": "Import BeautifulSoup from the bs4 package to parse HTML",
                  "pattern": "from package import ClassName",
                  "structureNote": "Python's <code>from...import</code> syntax loads a specific class from a larger package. We import only <code>BeautifulSoup</code> from <code>bs4</code> rather than the entire package -- this is more efficient and makes your code clearer about what tools it uses.",
                  "languages": {
                    "python": [
                      {"code": "from", "role": "Keyword", "tip": "Tells Python you want to import a specific item from within a package, rather than the whole package", "color": "keyword"},
                      {"code": " ", "role": "Space", "tip": "Separates the keyword from the package name", "color": "syntax"},
                      {"code": "bs4", "role": "Package", "tip": "Short for BeautifulSoup4 -- the package that contains HTML/XML parsing tools. The name 'beautiful soup' refers to the messy HTML you find on web pages", "color": "library"},
                      {"code": " ", "role": "Space", "tip": "Separates the package from the import keyword", "color": "syntax"},
                      {"code": "import", "role": "Keyword", "tip": "Specifies which specific item to load from the package", "color": "keyword"},
                      {"code": " ", "role": "Space", "tip": "Separates import from the class name", "color": "syntax"},
                      {"code": "BeautifulSoup", "role": "Class name", "tip": "The main class that turns raw HTML text into a searchable tree structure. You'll use this to create 'soup' objects that let you find and extract data", "color": "function"}
                    ],
                    "r": [
                      {"code": "library", "role": "Function", "tip": "R's function to load a package -- makes all its functions available in your session", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument to library()", "color": "syntax"},
                      {"code": "rvest", "role": "Package", "tip": "R's main web scraping package (a play on 'harvest'). Created by Hadley Wickham as part of the tidyverse. Provides html_element(), html_elements(), html_text(), and html_attr()", "color": "library"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the library() call -- rvest functions are now available", "color": "syntax"}
                    ]
                  }
                },
                {
                  "instruction": "Fetch a web page and parse its HTML into a searchable object",
                  "pattern": "soup = BeautifulSoup(response.text, 'parser')",
                  "structureNote": "This is the two-step pattern at the heart of web scraping: first <strong>fetch</strong> the raw HTML (with requests.get), then <strong>parse</strong> it into a searchable tree (with BeautifulSoup). The parser converts messy text into a structured object you can query with find() and find_all().",
                  "languages": {
                    "python": [
                      {"code": "soup", "role": "Variable", "tip": "The conventional name for a BeautifulSoup object -- stores the parsed HTML tree that you can search through", "color": "variable"},
                      {"code": " = ", "role": "Assignment", "tip": "Stores the parsed HTML object so you can search it with find() and find_all()", "color": "operator"},
                      {"code": "BeautifulSoup", "role": "Constructor", "tip": "Creates a new BeautifulSoup object that parses HTML text into a searchable tree of elements, tags, and attributes", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the arguments -- two are required: the HTML text and the parser name", "color": "syntax"},
                      {"code": "response.text", "role": "HTML content", "tip": "The raw HTML string from the server response. The .text property extracts just the page content from the Response object", "color": "variable"},
                      {"code": ", ", "role": "Separator", "tip": "Separates the HTML content from the parser argument", "color": "syntax"},
                      {"code": "'html.parser'", "role": "Parser name", "tip": "Python's built-in HTML parser -- reliable and requires no extra installation. Alternatives: 'lxml' (faster) or 'html5lib' (more tolerant of broken HTML)", "color": "string"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the constructor call -- soup is now a searchable HTML tree", "color": "syntax"}
                    ],
                    "r": [
                      {"code": "page", "role": "Variable", "tip": "Stores the parsed HTML document -- the conventional name in R for a scraped page object", "color": "variable"},
                      {"code": " <- ", "role": "Assignment", "tip": "R's assignment arrow -- stores the result of read_html()", "color": "operator"},
                      {"code": "read_html", "role": "Function", "tip": "From rvest -- does both steps in one call: fetches the URL AND parses the HTML into a searchable document", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument", "color": "syntax"},
                      {"code": "url", "role": "URL argument", "tip": "The web address to fetch and parse -- a string like 'https://quotes.toscrape.com/'", "color": "variable"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the read_html() call -- page is now a searchable HTML document", "color": "syntax"}
                    ]
                  }
                },
                {
                  "instruction": "Find the first HTML element matching a tag name and CSS class",
                  "pattern": "variable = soup.find('tag', class_='classname')",
                  "structureNote": "The <code>find()</code> method returns the <strong>first</strong> element matching your criteria. It searches the entire parsed HTML tree. The <code>class_</code> parameter has an underscore because <code>class</code> is a reserved word in Python. This is your surgical tool for extracting a single specific element.",
                  "languages": {
                    "python": [
                      {"code": "first_quote", "role": "Variable", "tip": "Will hold a single BeautifulSoup Tag object -- the first element matching both the tag type and class name", "color": "variable"},
                      {"code": " = ", "role": "Assignment", "tip": "Stores the found element for further extraction (e.g., .text to get its content)", "color": "operator"},
                      {"code": "soup", "role": "Parsed HTML", "tip": "The BeautifulSoup object containing the entire parsed HTML document tree", "color": "variable"},
                      {"code": ".", "role": "Dot accessor", "tip": "Accesses the find() method on the soup object", "color": "syntax"},
                      {"code": "find", "role": "Method", "tip": "Searches the HTML tree and returns the FIRST element matching all specified criteria. Returns None if nothing found", "color": "method"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the search criteria arguments", "color": "syntax"},
                      {"code": "'span'", "role": "Tag name", "tip": "The HTML tag type to search for -- 'span' is a generic inline container often used to wrap text content", "color": "string"},
                      {"code": ", ", "role": "Separator", "tip": "Separates the tag argument from the class filter", "color": "syntax"},
                      {"code": "class_", "role": "Class filter", "tip": "Filters by CSS class attribute. The trailing underscore avoids conflicting with Python's reserved 'class' keyword", "color": "argument"},
                      {"code": "=", "role": "Binding", "tip": "Connects the parameter name to its value", "color": "operator"},
                      {"code": "'text'", "role": "Class value", "tip": "The CSS class to match -- only elements with class='text' will be found. You discover these classes using browser Inspect/DevTools", "color": "string"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the find() call -- returns the first matching element or None", "color": "syntax"}
                    ],
                    "r": [
                      {"code": "first_quote", "role": "Variable", "tip": "Will hold a single HTML element node", "color": "variable"},
                      {"code": " <- ", "role": "Assignment", "tip": "Stores the found element", "color": "operator"},
                      {"code": "page", "role": "Parsed HTML", "tip": "The HTML document object created by read_html()", "color": "variable"},
                      {"code": " %>% ", "role": "Pipe", "tip": "Passes the page to the next function -- read as 'then search for'", "color": "operator"},
                      {"code": "html_element", "role": "Function", "tip": "From rvest -- finds the FIRST element matching a CSS selector. Note: singular 'element' for one result, plural 'elements' for all matches", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the CSS selector argument", "color": "syntax"},
                      {"code": "\"span.text\"", "role": "CSS selector", "tip": "A CSS selector combining tag and class: 'span.text' means 'a span element with class text'. The dot is CSS syntax for class matching", "color": "string"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the html_element() call", "color": "syntax"},
                      {"code": " %>% ", "role": "Pipe", "tip": "Passes the found element to html_text() to extract its content", "color": "operator"},
                      {"code": "html_text", "role": "Function", "tip": "Extracts the text content from an HTML element, stripping all tags", "color": "function"},
                      {"code": "()", "role": "Call parens", "tip": "Calls html_text() with no extra arguments -- returns the plain text content", "color": "syntax"}
                    ]
                  }
                },
                {
                  "instruction": "Find ALL elements matching a tag and class, returning a list of results",
                  "pattern": "results = soup.find_all('tag', class_='classname')",
                  "structureNote": "While <code>find()</code> returns one element, <code>find_all()</code> returns a <strong>list</strong> of ALL matching elements. This is your workhorse for scraping multiple items (all quotes, all rows, all prices). You then loop through the list to extract data from each element.",
                  "languages": {
                    "python": [
                      {"code": "all_quotes", "role": "Variable", "tip": "Will hold a list (ResultSet) of all matching elements -- you can loop through it or check its length with len()", "color": "variable"},
                      {"code": " = ", "role": "Assignment", "tip": "Stores the list of found elements", "color": "operator"},
                      {"code": "soup", "role": "Parsed HTML", "tip": "The BeautifulSoup object containing the parsed page", "color": "variable"},
                      {"code": ".", "role": "Dot accessor", "tip": "Accesses the find_all() method", "color": "syntax"},
                      {"code": "find_all", "role": "Method", "tip": "Returns a LIST of ALL elements matching the criteria -- unlike find() which returns only the first. Returns an empty list [] if no matches", "color": "method"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the search criteria", "color": "syntax"},
                      {"code": "'div'", "role": "Tag name", "tip": "The HTML tag to search for -- 'div' is a container element. On this page, each quote is wrapped in its own div", "color": "string"},
                      {"code": ", ", "role": "Separator", "tip": "Separates the tag from the class filter", "color": "syntax"},
                      {"code": "class_", "role": "Class filter", "tip": "Filters by CSS class -- the underscore avoids Python's reserved 'class' keyword", "color": "argument"},
                      {"code": "=", "role": "Binding", "tip": "Connects the parameter to its value", "color": "operator"},
                      {"code": "'quote'", "role": "Class value", "tip": "Match elements with class='quote'. Each quote div contains nested elements (text, author, tags) that you can extract", "color": "string"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the search -- returns a list of all matching div.quote elements", "color": "syntax"}
                    ],
                    "r": [
                      {"code": "quotes", "role": "Variable", "tip": "Will hold a node set (list) of all matching elements", "color": "variable"},
                      {"code": " <- ", "role": "Assignment", "tip": "Stores the list of elements", "color": "operator"},
                      {"code": "page", "role": "Parsed HTML", "tip": "The parsed HTML document", "color": "variable"},
                      {"code": " %>% ", "role": "Pipe", "tip": "Passes the page to the search function", "color": "operator"},
                      {"code": "html_elements", "role": "Function", "tip": "Note the plural 's' -- html_elements() returns ALL matches (a node set), while html_element() returns only the first", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the CSS selector argument", "color": "syntax"},
                      {"code": "\"div.quote\"", "role": "CSS selector", "tip": "Selects all div elements with class='quote'. The dot syntax (tag.class) is standard CSS selector notation", "color": "string"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the call -- returns all matching elements as a list you can iterate over", "color": "syntax"}
                    ]
                  }
                },
                {
                  "instruction": "Extract the href attribute from a link element (get the URL a link points to)",
                  "pattern": "url = element.get('attribute_name')",
                  "structureNote": "HTML elements have two kinds of data: <strong>text content</strong> (between tags) and <strong>attributes</strong> (inside the opening tag). To get text, use <code>.text</code>. To get attributes like <code>href</code>, <code>src</code>, or <code>class</code>, use <code>.get('attr')</code>. This is essential for extracting URLs from links and image sources.",
                  "languages": {
                    "python": [
                      {"code": "href", "role": "Variable", "tip": "Will hold the URL string that the link points to (e.g., '/tag/change/page/1/')", "color": "variable"},
                      {"code": " = ", "role": "Assignment", "tip": "Stores the extracted attribute value", "color": "operator"},
                      {"code": "tag", "role": "Element", "tip": "A BeautifulSoup Tag object representing one HTML element (e.g., an <a> link found with find() or in a loop)", "color": "variable"},
                      {"code": ".", "role": "Dot accessor", "tip": "Accesses the get() method on the Tag object", "color": "syntax"},
                      {"code": "get", "role": "Method", "tip": "Safely retrieves an attribute value from an HTML element. Returns None if the attribute doesn't exist (instead of crashing)", "color": "method"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the attribute name argument", "color": "syntax"},
                      {"code": "'href'", "role": "Attribute name", "tip": "The HTML attribute to extract. 'href' contains the URL in <a> links. Other common attributes: 'src' (images), 'class', 'id', 'data-*'", "color": "string"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the get() call -- returns the attribute value as a string, or None if not found", "color": "syntax"}
                    ],
                    "r": [
                      {"code": "tag_links", "role": "Variable", "tip": "Will hold a character vector of extracted attribute values", "color": "variable"},
                      {"code": " <- ", "role": "Assignment", "tip": "Stores the extracted values", "color": "operator"},
                      {"code": "page", "role": "Parsed HTML", "tip": "The HTML document or a subset of elements", "color": "variable"},
                      {"code": " %>% ", "role": "Pipe", "tip": "Passes to the search function", "color": "operator"},
                      {"code": "html_elements", "role": "Function", "tip": "Finds all matching elements (plural form for multiple results)", "color": "function"},
                      {"code": "(\"a.tag\")", "role": "CSS selector", "tip": "Selects all anchor (link) elements with class 'tag'", "color": "string"},
                      {"code": " %>% ", "role": "Pipe", "tip": "Passes the found elements to the attribute extractor", "color": "operator"},
                      {"code": "html_attr", "role": "Function", "tip": "Extracts the value of a specified attribute from each element -- like Python's .get() but vectorized over all elements", "color": "function"},
                      {"code": "(\"href\")", "role": "Attribute name", "tip": "The attribute to extract from each element -- returns a vector of URL strings", "color": "string"}
                    ]
                  }
                },
                {
                  "instruction": "Loop through scraped elements, extract nested data, and append each item to a list",
                  "pattern": "for item in elements: list.append({...})",
                  "structureNote": "This is the data collection loop -- the heart of most scrapers. For each container element (like a quote div), you search <strong>within</strong> that element for nested sub-elements (text, author). Each iteration extracts one complete record and adds it to your growing collection. This nested find pattern is how you go from raw HTML to structured data.",
                  "languages": {
                    "python": [
                      {"code": "for", "role": "Loop keyword", "tip": "Begins a for loop that will execute the indented block once for each element in the collection", "color": "keyword"},
                      {"code": " ", "role": "Space", "tip": "Separates the keyword from the loop variable", "color": "syntax"},
                      {"code": "quote", "role": "Loop variable", "tip": "On each iteration, holds one BeautifulSoup element from all_quotes. You can call find() on it to search WITHIN this element only", "color": "variable"},
                      {"code": " in ", "role": "In keyword", "tip": "Connects the loop variable to the collection -- 'for each quote in the list of all quotes'", "color": "keyword"},
                      {"code": "all_quotes", "role": "Collection", "tip": "The list of elements returned by find_all() -- the loop processes each one in order", "color": "variable"},
                      {"code": ":", "role": "Colon", "tip": "Marks the end of the for statement -- the indented lines below form the loop body", "color": "syntax"},
                      {"code": "\n    all_data.append({", "role": "Append dict", "tip": "all_data is a list we created earlier. .append() adds one new item to the end. The { opens a dictionary that will become one row of data", "color": "method"},
                      {"code": "\n        'text': quote.find('span', class_='text').text", "role": "Nested extract", "tip": "Searches WITHIN this specific quote element (not the whole page) for a span with class 'text', then extracts its text content", "color": "variable"},
                      {"code": ",", "role": "Separator", "tip": "Separates dictionary entries", "color": "syntax"},
                      {"code": "\n        'author': quote.find('small', class_='author').text", "role": "Nested extract", "tip": "Finds the author name within this quote element -- the 'small' tag with class 'author' contains the name text", "color": "variable"},
                      {"code": "\n    })", "role": "Close", "tip": "Closes the dictionary and the append() call -- one complete record is now added to all_data", "color": "syntax"}
                    ]
                  }
                },
                {
                  "instruction": "Add a polite delay between web requests to avoid overwhelming the server",
                  "pattern": "time.sleep(seconds)",
                  "structureNote": "Rate limiting is not optional -- it's a <strong>core responsibility</strong> of any scraper. Without delays, your code can send hundreds of requests per second, overwhelming the server and getting your IP blocked. The <code>time.sleep()</code> call pauses your script, mimicking human browsing speed. A 2-3 second delay is standard for research scraping.",
                  "languages": {
                    "python": [
                      {"code": "time", "role": "Module", "tip": "Python's built-in time module -- provides functions for delays, timestamps, and time measurement", "color": "library"},
                      {"code": ".", "role": "Dot accessor", "tip": "Accesses the sleep() function from the time module", "color": "syntax"},
                      {"code": "sleep", "role": "Function", "tip": "Pauses your script for the specified number of seconds. The program does nothing during this time -- it just waits", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument", "color": "syntax"},
                      {"code": "2", "role": "Seconds", "tip": "Wait 2 seconds before continuing. For research scraping, 2-5 seconds is polite. Some sites specify a Crawl-delay in robots.txt -- always respect it", "color": "special"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the sleep() call -- execution resumes after the delay", "color": "syntax"}
                    ],
                    "r": [
                      {"code": "Sys.sleep", "role": "Function", "tip": "R's built-in function to pause execution -- equivalent to Python's time.sleep(). 'Sys' refers to system-level operations", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument", "color": "syntax"},
                      {"code": "2", "role": "Seconds", "tip": "Pause for 2 seconds. Adjust based on the site's robots.txt Crawl-delay directive if one exists", "color": "special"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the call -- R waits 2 seconds then continues", "color": "syntax"}
                    ]
                  }
                },
                {
                  "instruction": "Fetch a web page with a custom User-Agent header and a timeout for safety",
                  "pattern": "response = requests.get(url, headers=dict, timeout=N)",
                  "structureNote": "The <code>headers</code> parameter identifies your scraper to the website. A clear User-Agent with contact information is professional and transparent -- it tells website owners who is visiting and how to reach you if there is a problem. The <code>timeout</code> prevents your script from hanging forever if a server does not respond.",
                  "languages": {
                    "python": [
                      {"code": "response", "role": "Variable", "tip": "Stores the server's response including the HTML content, status code, and headers", "color": "variable"},
                      {"code": " = ", "role": "Assignment", "tip": "Stores the response for inspection and parsing", "color": "operator"},
                      {"code": "requests.get", "role": "Function", "tip": "Sends an HTTP GET request to fetch the page at the specified URL", "color": "function"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the arguments", "color": "syntax"},
                      {"code": "url", "role": "URL", "tip": "The web address to fetch -- a string variable containing the target page URL", "color": "variable"},
                      {"code": ", ", "role": "Separator", "tip": "Separates arguments", "color": "syntax"},
                      {"code": "headers", "role": "Keyword arg", "tip": "Named parameter that passes a dictionary of HTTP headers -- metadata about your request sent to the server", "color": "argument"},
                      {"code": "=", "role": "Binding", "tip": "Connects the parameter name to its value", "color": "operator"},
                      {"code": "headers", "role": "Dict variable", "tip": "A dictionary like {'User-Agent': 'Research scraper; contact@university.edu'} that identifies your scraper", "color": "variable"},
                      {"code": ", ", "role": "Separator", "tip": "Separates the headers argument from the timeout", "color": "syntax"},
                      {"code": "timeout", "role": "Keyword arg", "tip": "Named parameter that sets the maximum seconds to wait for a response", "color": "argument"},
                      {"code": "=", "role": "Binding", "tip": "Connects the parameter name to the timeout value", "color": "operator"},
                      {"code": "30", "role": "Seconds", "tip": "Wait at most 30 seconds for the server to respond. Without this, a dead server could freeze your script indefinitely", "color": "special"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the request -- the page is now fetched with proper identification and a safety timeout", "color": "syntax"}
                    ]
                  }
                }
              ]
            }
            </script>
          </div>
        </section>

        <div class="nav-footer">
          <a href="02b-apis.html" class="nav-link prev">Working with APIs (02b)</a>
          <a href="03-data-exploration.html" class="nav-link next">Module 3: Data Exploration</a>
        </div>
      </div>
    </main>
  </div>

  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Questions about web scraping? I can help with HTML parsing, legal considerations, or troubleshooting your scraper.</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/grammar-primer.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <script>
  document.addEventListener('DOMContentLoaded', function() {
    document.querySelectorAll('.run-btn').forEach(btn => {
      btn.addEventListener('click', function() {
        const lang = this.dataset.lang;
        const codeBlock = this.closest('.code-tabs');
        const outputId = codeBlock.dataset.runnable;
        document.querySelectorAll(`.output-simulation[data-output="${outputId}"]`).forEach(out => {
          out.classList.remove('visible');
        });
        const output = document.querySelector(`.output-simulation[data-output="${outputId}"][data-lang="${lang}"]`);
        if (output) {
          output.classList.add('visible');
          output.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
        }
      });
    });

    document.querySelectorAll('.close-output').forEach(btn => {
      btn.addEventListener('click', function() {
        this.closest('.output-simulation').classList.remove('visible');
      });
    });

    document.querySelectorAll('.code-tabs .tab-button').forEach(btn => {
      btn.addEventListener('click', function() {
        const codeBlock = this.closest('.code-tabs');
        const outputId = codeBlock.dataset.runnable;
        if (outputId) {
          document.querySelectorAll(`.output-simulation[data-output="${outputId}"]`).forEach(out => {
            out.classList.remove('visible');
          });
        }
      });
    });

    // Accordion functionality for collapsible sections
    document.querySelectorAll('.accordion-header').forEach(header => {
      header.addEventListener('click', function() {
        const section = this.closest('.accordion-section');
        section.classList.toggle('open');
      });
    });
  });
  </script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    // Create a single tooltip element that will be reused
    let tooltipEl = null;
    let currentTarget = null;
    let hideTimeout = null;

    function createTooltip() {
      if (tooltipEl) return tooltipEl;
      tooltipEl = document.createElement('div');
      tooltipEl.className = 'tooltip-popup';
      document.body.appendChild(tooltipEl);
      return tooltipEl;
    }

    function positionTooltip(target) {
      const tooltip = createTooltip();
      const tipText = target.getAttribute('data-tip');
      if (!tipText) return;

      tooltip.textContent = tipText;
      tooltip.className = 'tooltip-popup'; // Reset classes

      // Get target position
      const targetRect = target.getBoundingClientRect();

      // Find the code block container (pre or .tab-content or .code-tabs)
      let container = target.closest('pre') || target.closest('.tab-content') || target.closest('.code-tabs');
      let containerRect = container ? container.getBoundingClientRect() : {
        left: 0, right: window.innerWidth, top: 0, bottom: window.innerHeight
      };

      // Use viewport as fallback boundary
      const viewportWidth = window.innerWidth;
      const viewportHeight = window.innerHeight;
      const padding = 10; // Minimum distance from edges

      // Temporarily show to measure
      tooltip.style.visibility = 'hidden';
      tooltip.style.display = 'block';
      tooltip.classList.add('visible');

      const tooltipRect = tooltip.getBoundingClientRect();
      const tooltipWidth = tooltipRect.width;
      const tooltipHeight = tooltipRect.height;

      // Calculate ideal position (above the element, centered)
      let left = targetRect.left + (targetRect.width / 2) - (tooltipWidth / 2);
      let top = targetRect.top - tooltipHeight - 8; // 8px gap
      let arrowClass = 'arrow-bottom';

      // Check if tooltip would go above viewport - if so, show below
      if (top < padding) {
        top = targetRect.bottom + 8;
        arrowClass = 'arrow-top';
      }

      // Check if tooltip would go below viewport when shown below
      if (top + tooltipHeight > viewportHeight - padding) {
        top = targetRect.top - tooltipHeight - 8;
        arrowClass = 'arrow-bottom';
      }

      // Horizontal boundary checks - keep within viewport
      if (left < padding) {
        left = padding;
      }
      if (left + tooltipWidth > viewportWidth - padding) {
        left = viewportWidth - tooltipWidth - padding;
      }

      // Additional check: keep within code container horizontally if possible
      if (container) {
        const minLeft = Math.max(padding, containerRect.left);
        const maxRight = Math.min(viewportWidth - padding, containerRect.right);

        if (left < minLeft) {
          left = minLeft;
        }
        if (left + tooltipWidth > maxRight) {
          left = maxRight - tooltipWidth;
        }
      }

      // Apply position
      tooltip.style.left = left + 'px';
      tooltip.style.top = top + 'px';
      tooltip.style.visibility = 'visible';
      tooltip.classList.add(arrowClass);
    }

    function showTooltip(target) {
      if (hideTimeout) {
        clearTimeout(hideTimeout);
        hideTimeout = null;
      }
      currentTarget = target;
      positionTooltip(target);
    }

    function hideTooltip() {
      hideTimeout = setTimeout(function() {
        if (tooltipEl) {
          tooltipEl.classList.remove('visible');
        }
        currentTarget = null;
      }, 100);
    }

    // Event delegation for all tooltips
    document.addEventListener('mouseenter', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) {
        showTooltip(e.target);
      }
    }, true);

    document.addEventListener('mouseleave', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) {
        hideTooltip();
      }
    }, true);

    // Handle scroll - reposition if visible
    document.addEventListener('scroll', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) {
        positionTooltip(currentTarget);
      }
    }, true);

    // Handle window resize
    window.addEventListener('resize', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) {
        positionTooltip(currentTarget);
      }
    });
  })();
  </script>
</body>
</html>
