<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 10: History of NLP | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content {
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
    }
    /* Allow text selection on code blocks for copy-paste */
    .protected-content pre,
    .protected-content code,
    .protected-content .code-block,
    .protected-content .code-tabs {
      -webkit-user-select: text;
      -moz-user-select: text;
      -ms-user-select: text;
      user-select: text;
    }

    /* Code tooltips - hover explanations */
    .code-tooltip {
      position: relative;
      cursor: help;
      border-bottom: 1px dotted #a0aec0;
      text-decoration: none;
    }

    .tooltip-popup {
      position: fixed;
      background: #1f2937;
      color: white;
      padding: 0.5rem 0.75rem;
      border-radius: 6px;
      font-size: 0.75rem;
      white-space: normal;
      max-width: 300px;
      opacity: 0;
      pointer-events: none;
      transition: opacity 0.15s ease-in-out;
      z-index: 10000;
      line-height: 1.4;
      text-align: left;
      font-family: var(--font-body);
      font-style: normal;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    .tooltip-popup.visible { opacity: 1; }
    .tooltip-popup::after {
      content: '';
      position: absolute;
      border: 6px solid transparent;
    }
    .tooltip-popup.arrow-bottom::after {
      top: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-top-color: #1f2937;
    }
    .tooltip-popup.arrow-top::after {
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-bottom-color: #1f2937;
    }

    /* Timeline styling */
    .timeline {
      position: relative;
      padding-left: 2rem;
      margin: 2rem 0;
    }
    .timeline::before {
      content: '';
      position: absolute;
      left: 0;
      top: 0;
      bottom: 0;
      width: 3px;
      background: linear-gradient(to bottom, #667eea, #764ba2, #f093fb);
      border-radius: 2px;
    }
    .timeline-item {
      position: relative;
      margin-bottom: 1.5rem;
      padding-left: 1.5rem;
    }
    .timeline-item::before {
      content: '';
      position: absolute;
      left: -2rem;
      top: 0.5rem;
      width: 12px;
      height: 12px;
      background: white;
      border: 3px solid #667eea;
      border-radius: 50%;
    }
    .timeline-item.highlight::before {
      background: #667eea;
      box-shadow: 0 0 10px rgba(102, 126, 234, 0.5);
    }
    .timeline-year {
      font-weight: 700;
      color: #667eea;
      font-size: 0.9rem;
      margin-bottom: 0.25rem;
    }
    .timeline-title {
      font-weight: 600;
      color: var(--color-primary);
      margin-bottom: 0.25rem;
    }
    .timeline-desc {
      color: #4a5568;
      font-size: 0.95rem;
      line-height: 1.5;
    }

    /* Expert insight boxes */
    .expert-insight {
      background: linear-gradient(135deg, #f6f9fc 0%, #eef2f7 100%);
      border-left: 4px solid #667eea;
      padding: 1.25rem 1.5rem;
      margin: 1.5rem 0;
      border-radius: 0 8px 8px 0;
    }
    .expert-insight-header {
      display: flex;
      align-items: center;
      gap: 0.5rem;
      margin-bottom: 0.75rem;
      font-weight: 600;
      color: #667eea;
    }
    .expert-insight p {
      margin: 0;
      color: #2d3748;
      line-height: 1.7;
    }

    /* Comparison cards */
    .comparison-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
      gap: 1.5rem;
      margin: 1.5rem 0;
    }
    .comparison-card {
      background: white;
      border: 1px solid #e2e8f0;
      border-radius: 12px;
      padding: 1.5rem;
      box-shadow: 0 2px 8px rgba(0,0,0,0.05);
    }
    .comparison-card h4 {
      margin: 0 0 0.75rem 0;
      color: var(--color-primary);
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }
    .comparison-card ul {
      margin: 0;
      padding-left: 1.25rem;
    }
    .comparison-card li {
      margin-bottom: 0.5rem;
      color: #4a5568;
    }

    /* Math formulas */
    .math-block {
      background: #f8fafc;
      border: 1px solid #e2e8f0;
      border-radius: 8px;
      padding: 1rem 1.5rem;
      margin: 1rem 0;
      font-family: var(--font-code);
      font-size: 0.95rem;
      overflow-x: auto;
      color: #1a202c;
    }
    .math-inline {
      font-family: var(--font-code);
      background: #f1f5f9;
      padding: 0.15rem 0.4rem;
      border-radius: 4px;
      font-size: 0.9em;
      color: #1a202c;
    }

    /* Interactive demo boxes */
    .demo-box {
      background: #1a1a2e;
      border-radius: 12px;
      padding: 1.5rem;
      margin: 1.5rem 0;
      color: #e2e8f0;
    }
    .demo-box-title {
      font-weight: 600;
      color: #a5b4fc;
      margin-bottom: 1rem;
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }
    .demo-box pre {
      background: #0f0f1a;
      border-radius: 8px;
      padding: 1rem;
      margin: 0.75rem 0;
      color: #e2e8f0;
      font-size: 0.9rem;
      line-height: 1.6;
    }
    .demo-box code {
      color: #e2e8f0;
    }
    .demo-box .code-comment { color: #6b7280; font-style: italic; }
    .demo-box .code-keyword { color: #c4b5fd; }
    .demo-box .code-string { color: #86efac; }
    .demo-box .code-function { color: #93c5fd; }
    .demo-box .code-number { color: #fdba74; }
    .demo-box .code-variable { color: #f9a8d4; }
    .demo-box .code-operator { color: #67e8f9; }

    /* Key concept highlights */
    .key-concept {
      background: #fffbeb;
      border: 1px solid #fcd34d;
      border-radius: 8px;
      padding: 1rem 1.25rem;
      margin: 1rem 0;
    }
    .key-concept-title {
      font-weight: 600;
      color: #92400e;
      margin-bottom: 0.5rem;
    }
    .key-concept p {
      margin: 0;
      color: #78350f;
    }

    /* Readable code in info boxes */
    .info-box pre {
      background: #1e293b;
      color: #e2e8f0;
      padding: 1rem;
      border-radius: 6px;
      margin-top: 0.75rem;
      font-family: var(--font-code);
      font-size: 0.85rem;
      line-height: 1.6;
      overflow-x: auto;
    }
    .info-box code {
      color: #e2e8f0;
    }

    /* References section */
    .references-section {
      background: #f8fafc;
      border-radius: 12px;
      padding: 2rem;
      margin-top: 3rem;
    }
    .references-section h2 {
      margin-top: 0;
      color: var(--color-primary);
    }
    .reference-category {
      margin-bottom: 2rem;
    }
    .reference-category:last-child {
      margin-bottom: 0;
    }
    .reference-category h3 {
      color: #667eea;
      font-size: 1.1rem;
      margin-bottom: 1rem;
      padding-bottom: 0.5rem;
      border-bottom: 2px solid #e2e8f0;
    }
    .reference-item {
      margin-bottom: 0.75rem;
      padding-left: 1rem;
      border-left: 2px solid #e2e8f0;
    }
    .reference-item:hover {
      border-left-color: #667eea;
    }
    .reference-item a {
      color: var(--color-primary);
      text-decoration: none;
    }
    .reference-item a:hover {
      text-decoration: underline;
    }
    .reference-authors {
      color: #4a5568;
      font-size: 0.9rem;
    }
    .reference-venue {
      color: #718096;
      font-size: 0.85rem;
      font-style: italic;
    }

    /* Quiz styling */
    .nlp-quiz {
      background: #f0fdf4;
      border: 1px solid #86efac;
      border-radius: 12px;
      padding: 1.5rem;
      margin: 2rem 0;
    }
    .nlp-quiz h4 {
      margin: 0 0 1rem 0;
      color: #166534;
    }
    .quiz-option {
      display: block;
      background: white;
      border: 1px solid #d1d5db;
      border-radius: 6px;
      padding: 0.75rem 1rem;
      margin-bottom: 0.5rem;
      cursor: pointer;
      transition: all 0.2s;
    }
    .quiz-option:hover {
      border-color: #22c55e;
      background: #f0fdf4;
    }
    .quiz-option.selected {
      border-color: #22c55e;
      background: #dcfce7;
    }
    .quiz-feedback {
      margin-top: 1rem;
      padding: 1rem;
      border-radius: 6px;
      display: none;
    }
    .quiz-feedback.correct {
      display: block;
      background: #dcfce7;
      color: #166534;
    }
    .quiz-feedback.incorrect {
      display: block;
      background: #fee2e2;
      color: #991b1b;
    }
  </style>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>

      <div class="course-description">
        <h3>Course Modules</h3>
        <ul class="module-list">
          <li><strong>Module 0:</strong> Languages & Platforms</li>
          <li><strong>Module 1:</strong> Getting Started</li>
          <li><strong>Module 2:</strong> Data Harnessing</li>
          <li><strong>Module 3:</strong> Data Exploration</li>
          <li><strong>Module 4:</strong> Data Cleaning</li>
          <li><strong>Module 5:</strong> Data Analysis</li>
          <li><strong>Module 6:</strong> Causal Inference</li>
          <li><strong>Module 7:</strong> Estimation Methods</li>
          <li><strong>Module 8:</strong> Replicability</li>
          <li><strong>Module 9:</strong> Git & GitHub</li>
          <li><strong>Module 10:</strong> History of NLP</li>
          <li><strong>Module 11:</strong> Machine Learning</li>
          <li><strong>Module 12:</strong> Large Language Models</li>
        </ul>
      </div>

      <div class="access-note">
        This course is currently open to <strong>students at Sciences Po</strong>. If you are not a Sciences Po student but would like access, please <a href="mailto:giulia.caprini@sciencespo.fr">email me</a> to request an invite token.
      </div>

      <div class="password-form">
        <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
        <button id="password-submit">Access Course</button>
        <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
      </div>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li class="active"><a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a></li>
          <li><a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a></li>
          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <div class="module-header">
          <h1>10 &nbsp;History of NLP</h1>
          <div class="module-meta">
            <span>From ELIZA to GPT</span>
            <span>70 Years of Language AI</span>
            <span>Conceptual + Technical</span>
          </div>
        </div>

        <p class="lead">
          The story of Natural Language Processing is a fascinating journey from ambitious dreams to unexpected breakthroughs. Understanding this history isn't just academic curiosity&mdash;it reveals <em>why</em> modern language models work the way they do, and helps you use them more effectively.
        </p>

        <div class="learning-objectives">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Trace the evolution of NLP from hand-crafted rules to self-supervised learning</li>
            <li>Understand the key algorithmic breakthroughs and why they mattered</li>
            <li>Grasp the technical intuition behind word embeddings, attention, and transformers</li>
            <li>Connect historical developments to the capabilities and limitations of modern LLMs</li>
            <li>Appreciate the role of scale, data, and compute in the AI revolution</li>
          </ul>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#overview">10.0 The Big Picture: A 70-Year Journey</a></li>
            <li><a href="#early-days">10.1 The Symbolic Era (1950s-1980s)</a></li>
            <li><a href="#statistical">10.2 The Statistical Revolution (1990s-2000s)</a></li>
            <li><a href="#embeddings">10.3 Word Embeddings: Meaning as Geometry (2013)</a></li>
            <li><a href="#rnns">10.4 Sequence Models: Learning to Remember</a></li>
            <li><a href="#attention">10.5 Attention: The Breakthrough (2014-2017)</a></li>
            <li><a href="#transformers">10.6 Transformers: The Architecture That Changed Everything</a></li>
            <li><a href="#modern-era">10.7 The Modern Era: Scale, RLHF, and Beyond</a></li>
            <li><a href="#economics">10.8 Why This Matters for Economists</a></li>
            <li><a href="#references">References & Further Reading</a></li>
          </ul>
        </div>

        <!-- ============================================
             SECTION 10.0: THE BIG PICTURE
             ============================================ -->
        <h2 id="overview">10.0 The Big Picture: A 70-Year Journey</h2>

        <p>
          Before diving into details, let's see the forest before the trees. NLP has gone through four major paradigms, each representing a fundamentally different philosophy about how machines should process language:
        </p>

        <div class="comparison-grid">
          <div class="comparison-card">
            <h4>üèõÔ∏è Era 1: Symbolic (1950s-1980s)</h4>
            <ul>
              <li><strong>Philosophy:</strong> Language follows rules; encode them</li>
              <li><strong>Method:</strong> Hand-written grammars, pattern matching</li>
              <li><strong>Bottleneck:</strong> Rules can't capture all of language</li>
              <li><strong>Legacy:</strong> ELIZA, expert systems</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üìä Era 2: Statistical (1990s-2000s)</h4>
            <ul>
              <li><strong>Philosophy:</strong> Language is probabilistic; learn from data</li>
              <li><strong>Method:</strong> N-grams, HMMs, Naive Bayes, SVMs</li>
              <li><strong>Bottleneck:</strong> Feature engineering is manual</li>
              <li><strong>Legacy:</strong> Spam filters, early MT</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üß† Era 3: Neural (2013-2017)</h4>
            <ul>
              <li><strong>Philosophy:</strong> Learn representations automatically</li>
              <li><strong>Method:</strong> Word2Vec, RNNs, LSTMs, Seq2Seq</li>
              <li><strong>Bottleneck:</strong> Sequential processing is slow</li>
              <li><strong>Legacy:</strong> Word embeddings, neural MT</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üöÄ Era 4: Transformer (2017-present)</h4>
            <ul>
              <li><strong>Philosophy:</strong> Attention + scale = emergence</li>
              <li><strong>Method:</strong> Self-attention, pre-training, RLHF</li>
              <li><strong>Bottleneck:</strong> Compute, alignment, hallucination</li>
              <li><strong>Legacy:</strong> BERT, GPT, Claude, and the AI revolution</li>
            </ul>
          </div>
        </div>

        <div class="expert-insight">
          <div class="expert-insight-header">
            üí° Key Insight
          </div>
          <p>
            Each paradigm shift wasn't just incremental improvement&mdash;it was a fundamental rethinking of the problem. The move from rules to statistics said "let data speak." The move from statistics to neural networks said "let the model find its own features." The move to transformers said "let every word attend to every other word, in parallel." Understanding these shifts helps you understand what modern models can and cannot do.
          </p>
        </div>

        <!-- ============================================
             SECTION 10.1: THE SYMBOLIC ERA
             ============================================ -->
        <h2 id="early-days">10.1 The Symbolic Era (1950s-1980s)</h2>

        <h3>The Dream of Machine Intelligence</h3>
        <p>
          The story begins with Alan Turing's 1950 paper <em>"Computing Machinery and Intelligence"</em>, which posed the question that still drives AI research: <strong>Can machines think?</strong>
        </p>
        <p>
          Turing proposed what we now call the <strong>Turing Test</strong>: a machine passes if a human interrogator, communicating only through text, cannot reliably distinguish it from a human. This framing was remarkably prescient&mdash;it anticipated that language would be central to machine intelligence.
        </p>

        <blockquote>
          "I propose to consider the question, 'Can machines think?' This should begin with definitions of the meaning of the terms 'machine' and 'think.'"
          <br><span style="color: #666;">&mdash; Alan Turing, <em>Computing Machinery and Intelligence</em> (1950)</span>
        </blockquote>

        <h3>ELIZA: The First Chatbot (1966)</h3>
        <p>
          Joseph Weizenbaum at MIT created <strong>ELIZA</strong>, one of the first programs to engage in human-like conversation. ELIZA simulated a Rogerian psychotherapist using simple pattern matching and substitution rules.
        </p>

        <div class="demo-box">
          <div class="demo-box-title">ü§ñ ELIZA in Action</div>
          <pre><code><span class="code-comment"># A typical ELIZA conversation</span>

<span class="code-variable">User:</span>    <span class="code-string">I am feeling sad today.</span>
<span class="code-variable">ELIZA:</span>   <span class="code-string">I am sorry to hear you are feeling sad today.</span>

<span class="code-variable">User:</span>    <span class="code-string">My mother doesn't understand me.</span>
<span class="code-variable">ELIZA:</span>   <span class="code-string">Tell me more about your family.</span>

<span class="code-variable">User:</span>    <span class="code-string">I think you don't really understand me either.</span>
<span class="code-variable">ELIZA:</span>   <span class="code-string">What makes you think I don't really understand you?</span></code></pre>
          <p style="margin-top: 1rem; color: #a0aec0; font-size: 0.9rem;">
            ELIZA worked by matching keywords ("mother" ‚Üí family topic) and transforming sentences with templates. It had <strong>no understanding whatsoever</strong>&mdash;just clever rules. Yet users found it surprisingly engaging, a phenomenon Weizenbaum called the <em>"ELIZA effect."</em>
          </p>
        </div>

        <div class="key-concept">
          <div class="key-concept-title">The ELIZA Effect</div>
          <p>
            People have a strong tendency to attribute understanding and emotion to systems that use language, even very simple ones. This psychological phenomenon is crucial for understanding public reactions to modern chatbots&mdash;and for being appropriately skeptical about what "understanding" really means.
          </p>
        </div>

        <h3>How ELIZA Actually Worked</h3>
        <p>
          Let's peek under the hood. ELIZA's "intelligence" was a set of pattern-matching rules:
        </p>

        <div class="demo-box">
          <div class="demo-box-title">‚öôÔ∏è ELIZA's Pattern Matching Rules (Simplified)</div>
          <pre><code><span class="code-comment"># Simplified ELIZA-style rules in Python</span>

<span class="code-variable">rules</span> = [
    <span class="code-comment"># (pattern, response_template)</span>
    (<span class="code-string">r"I am (.*)"</span>,          <span class="code-string">"Why do you say you are {0}?"</span>),
    (<span class="code-string">r"I feel (.*)"</span>,        <span class="code-string">"Tell me more about feeling {0}."</span>),
    (<span class="code-string">r"(.*) mother (.*)"</span>,   <span class="code-string">"Tell me more about your family."</span>),
    (<span class="code-string">r"(.*) father (.*)"</span>,   <span class="code-string">"How do you feel about your father?"</span>),
    (<span class="code-string">r"I think (.*)"</span>,       <span class="code-string">"What makes you think {0}?"</span>),
    (<span class="code-string">r"(.*)"</span>,               <span class="code-string">"Please go on."</span>),  <span class="code-comment"># fallback</span>
]

<span class="code-keyword">def</span> <span class="code-function">eliza_respond</span>(<span class="code-variable">user_input</span>):
    <span class="code-keyword">for</span> pattern, response <span class="code-keyword">in</span> rules:
        match = re.match(pattern, user_input, re.IGNORECASE)
        <span class="code-keyword">if</span> match:
            <span class="code-keyword">return</span> response.format(*match.groups())
    <span class="code-keyword">return</span> <span class="code-string">"I see. Please continue."</span></code></pre>
        </div>

        <h3>The Chomsky Paradigm</h3>
        <p>
          Meanwhile, Noam Chomsky's work dominated academic linguistics. His theory of <strong>transformational grammar</strong> proposed that language was governed by innate, universal rules&mdash;and that statistical approaches were fundamentally misguided.
        </p>

        <blockquote>
          "It must be recognized that the notion 'probability of a sentence' is an entirely useless one."
          <br><span style="color: #666;">&mdash; Noam Chomsky, <em>Syntactic Structures</em> (1957)</span>
        </blockquote>

        <p>
          Chomsky illustrated this with the famous example: <em>"Colorless green ideas sleep furiously"</em> is grammatically correct but meaningless and has zero probability in any corpus&mdash;yet we can understand it. He argued this proved that language wasn't about statistics.
        </p>

        <div class="expert-insight">
          <div class="expert-insight-header">
            üî¨ Historical Perspective
          </div>
          <p>
            Chomsky was half-right. Human language <em>does</em> have deep structure that pure statistics might miss. But he underestimated what statistical methods could achieve with enough data and the right architectures. The irony is that modern neural networks have learned grammatical structure purely from statistics&mdash;they can distinguish grammatical from ungrammatical sentences, even weird ones, without being told any rules.
          </p>
        </div>

        <h3>The ALPAC Report and the First AI Winter (1966)</h3>
        <p>
          The <strong>Automatic Language Processing Advisory Committee</strong> (ALPAC) issued a devastating report concluding that machine translation was too expensive and too poor quality to be practical. Government funding for NLP research collapsed, leading to what's called the first "AI Winter."
        </p>
        <p>
          The report was <em>correct</em> about the limitations of rule-based approaches. It was <em>wrong</em> about the ultimate possibility of machine translation&mdash;it just required a completely different approach.
        </p>

        <!-- ============================================
             SECTION 10.2: THE STATISTICAL ERA
             ============================================ -->
        <h2 id="statistical">10.2 The Statistical Revolution (1990s-2000s)</h2>

        <p>
          The statistical revolution came when researchers stopped trying to encode linguistic rules and started treating language as <strong>data to be modeled probabilistically</strong>. This paradigm shift was driven by researchers at IBM working on speech recognition.
        </p>

        <blockquote>
          "Every time I fire a linguist, the performance of the speech recognizer goes up."
          <br><span style="color: #666;">&mdash; Fred Jelinek, IBM (apocryphal, but captures the spirit)</span>
        </blockquote>

        <h3>The Core Idea: Language Modeling</h3>
        <p>
          A <strong>language model</strong> assigns probabilities to sequences of words. The fundamental question: given some words, what word is likely to come next?
        </p>

        <div class="math-block">
          P("the cat sat on the mat") = P(the) √ó P(cat|the) √ó P(sat|the,cat) √ó P(on|the,cat,sat) √ó ...
        </div>

        <p>
          This might seem like a detour from "understanding" language, but it turns out that predicting the next word requires capturing a tremendous amount of linguistic knowledge.
        </p>

        <h3>N-gram Models: The Markov Assumption</h3>
        <p>
          Computing the full conditional probability <span class="math-inline">P(word|all previous words)</span> is intractable. The <strong>Markov assumption</strong> simplifies this: only look at the last <em>n-1</em> words.
        </p>

        <div class="demo-box">
          <div class="demo-box-title">üìä N-gram Language Models</div>
          <pre><code><span class="code-comment"># N-gram probability estimation</span>

<span class="code-comment"># Unigram (n=1): Each word is independent</span>
<span class="code-variable">P</span>(<span class="code-string">"cat"</span>) = count(<span class="code-string">"cat"</span>) / total_words

<span class="code-comment"># Bigram (n=2): Depends on previous word only</span>
<span class="code-variable">P</span>(<span class="code-string">"cat"</span> | <span class="code-string">"the"</span>) = count(<span class="code-string">"the cat"</span>) / count(<span class="code-string">"the"</span>)

<span class="code-comment"># Trigram (n=3): Depends on two previous words</span>
<span class="code-variable">P</span>(<span class="code-string">"mat"</span> | <span class="code-string">"on the"</span>) = count(<span class="code-string">"on the mat"</span>) / count(<span class="code-string">"on the"</span>)

<span class="code-comment"># Example with a toy corpus: "the cat sat on the mat"</span>
<span class="code-comment"># Bigram counts: "the cat":1, "cat sat":1, "sat on":1, "on the":1, "the mat":1</span>

<span class="code-variable">P</span>(<span class="code-string">"cat"</span> | <span class="code-string">"the"</span>) = <span class="code-number">1</span>/<span class="code-number">2</span> = <span class="code-number">0.5</span>   <span class="code-comment"># "the" appears twice, followed by "cat" once</span>
<span class="code-variable">P</span>(<span class="code-string">"mat"</span> | <span class="code-string">"the"</span>) = <span class="code-number">1</span>/<span class="code-number">2</span> = <span class="code-number">0.5</span>   <span class="code-comment"># "the" appears twice, followed by "mat" once</span>
<span class="code-variable">P</span>(<span class="code-string">"dog"</span> | <span class="code-string">"the"</span>) = <span class="code-number">0</span>/<span class="code-number">2</span> = <span class="code-number">0.0</span>   <span class="code-comment"># Never seen "the dog" - assigns zero!</span></code></pre>
        </div>

        <h3>The Sparsity Problem</h3>
        <p>
          N-gram models suffer from <strong>sparsity</strong>: most possible n-grams never appear in training data. If you've never seen "the dog" in your corpus, is it impossible? Of course not!
        </p>

        <div class="key-concept">
          <div class="key-concept-title">Smoothing Techniques</div>
          <p>
            Various "smoothing" methods were developed to handle unseen n-grams: add-one smoothing (Laplace), Good-Turing estimation, Kneser-Ney smoothing. These assign small but non-zero probabilities to unseen events. Kneser-Ney, in particular, remained state-of-the-art for language modeling well into the neural era.
          </p>
        </div>

        <h3>Bag of Words and TF-IDF</h3>
        <p>
          For document classification tasks (spam detection, sentiment analysis, topic categorization), simpler representations often worked well:
        </p>

        <div class="comparison-grid">
          <div class="comparison-card">
            <h4>üìù Bag of Words</h4>
            <ul>
              <li>Represent document as word counts</li>
              <li>Ignores word order entirely</li>
              <li>"The cat sat on the mat" = {the:2, cat:1, sat:1, on:1, mat:1}</li>
              <li>Simple but surprisingly effective</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üìà TF-IDF</h4>
            <ul>
              <li><strong>Term Frequency:</strong> How often word appears in document</li>
              <li><strong>Inverse Document Frequency:</strong> Penalize common words</li>
              <li>TF-IDF = TF √ó log(N/df)</li>
              <li>Still used in search engines today</li>
            </ul>
          </div>
        </div>

        <h3>Hidden Markov Models (HMMs)</h3>
        <p>
          For sequence labeling tasks like <strong>part-of-speech tagging</strong> (assigning noun, verb, adjective to each word), Hidden Markov Models became the dominant approach. HMMs model sequences where the underlying "state" (the part of speech) is hidden, and we only observe the words.
        </p>

        <div class="demo-box">
          <div class="demo-box-title">üè∑Ô∏è Part-of-Speech Tagging with HMMs</div>
          <pre><code><span class="code-comment"># Given a sentence, find the most likely sequence of tags</span>

<span class="code-variable">sentence</span> = [<span class="code-string">"The"</span>, <span class="code-string">"cat"</span>, <span class="code-string">"sat"</span>, <span class="code-string">"quickly"</span>]

<span class="code-comment"># HMM estimates two types of probabilities:</span>

<span class="code-comment"># 1. Transition probabilities: P(tag_i | tag_{i-1})</span>
<span class="code-variable">P</span>(NOUN | DET) = <span class="code-number">0.6</span>      <span class="code-comment"># After "the", nouns are common</span>
<span class="code-variable">P</span>(VERB | NOUN) = <span class="code-number">0.4</span>     <span class="code-comment"># After a noun, verbs are common</span>

<span class="code-comment"># 2. Emission probabilities: P(word | tag)</span>
<span class="code-variable">P</span>(<span class="code-string">"cat"</span> | NOUN) = <span class="code-number">0.01</span>   <span class="code-comment"># "cat" is a noun</span>
<span class="code-variable">P</span>(<span class="code-string">"sat"</span> | VERB) = <span class="code-number">0.005</span>  <span class="code-comment"># "sat" is a verb</span>

<span class="code-comment"># Viterbi algorithm finds: DET ‚Üí NOUN ‚Üí VERB ‚Üí ADV</span>
<span class="code-variable">tags</span> = [<span class="code-string">"DET"</span>, <span class="code-string">"NOUN"</span>, <span class="code-string">"VERB"</span>, <span class="code-string">"ADV"</span>]</code></pre>
        </div>

        <h3>The IBM Translation Models (1990s)</h3>
        <p>
          IBM researchers pioneered <strong>statistical machine translation</strong> by learning to align words between languages from parallel corpora (texts in two languages that are translations of each other).
        </p>
        <p>
          The key insight: translation could be decomposed into a <strong>language model</strong> (how likely is this English sentence?) and a <strong>translation model</strong> (how likely is this English given the French?). Using Bayes' theorem:
        </p>

        <div class="math-block">
          P(English | French) ‚àù P(French | English) √ó P(English)
        </div>

        <p>
          This approach dominated machine translation for nearly two decades, culminating in systems like Google Translate (pre-neural version).
        </p>

        <!-- ============================================
             SECTION 10.3: WORD EMBEDDINGS
             ============================================ -->
        <h2 id="embeddings">10.3 Word Embeddings: Meaning as Geometry (2013)</h2>

        <p>
          The breakthrough that bridged statistical and neural NLP came from a deceptively simple idea: represent words as <strong>dense vectors</strong> in a continuous space, where similar words are close together.
        </p>

        <h3>The Distributional Hypothesis</h3>
        <p>
          The theoretical foundation comes from linguistics:
        </p>

        <blockquote>
          "You shall know a word by the company it keeps."
          <br><span style="color: #666;">&mdash; J.R. Firth (1957)</span>
        </blockquote>

        <p>
          Words that appear in similar contexts have similar meanings. "Dog" and "cat" appear near words like "pet," "fur," "veterinarian." "King" and "queen" appear near "royal," "throne," "crown."
        </p>

        <h3>Word2Vec: The Revolution (2013)</h3>
        <p>
          <strong>Tomas Mikolov</strong> and colleagues at Google introduced Word2Vec, which trained neural networks to predict context words. The hidden layer weights became word embeddings.
        </p>

        <div class="comparison-grid">
          <div class="comparison-card">
            <h4>Skip-gram</h4>
            <ul>
              <li>Given center word, predict context</li>
              <li>Input: "cat"</li>
              <li>Predict: "the", "sat", "on", "mat"</li>
              <li>Better for rare words</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>CBOW (Continuous Bag of Words)</h4>
            <ul>
              <li>Given context, predict center word</li>
              <li>Input: "the", "sat", "on", "mat"</li>
              <li>Predict: "cat"</li>
              <li>Faster to train</li>
            </ul>
          </div>
        </div>

        <h3>The Magic of Vector Arithmetic</h3>
        <p>
          Word2Vec's most famous result was that semantic relationships became <strong>vector operations</strong>:
        </p>

        <div class="demo-box">
          <div class="demo-box-title">‚ú® The Magic of Word Vectors</div>
          <pre><code><span class="code-comment"># These relationships emerged automatically from text!</span>

<span class="code-comment"># Gender relationship</span>
<span class="code-function">vector</span>(<span class="code-string">"king"</span>) - <span class="code-function">vector</span>(<span class="code-string">"man"</span>) + <span class="code-function">vector</span>(<span class="code-string">"woman"</span>) ‚âà <span class="code-function">vector</span>(<span class="code-string">"queen"</span>)

<span class="code-comment"># Capital cities</span>
<span class="code-function">vector</span>(<span class="code-string">"paris"</span>) - <span class="code-function">vector</span>(<span class="code-string">"france"</span>) + <span class="code-function">vector</span>(<span class="code-string">"italy"</span>) ‚âà <span class="code-function">vector</span>(<span class="code-string">"rome"</span>)

<span class="code-comment"># Verb tense</span>
<span class="code-function">vector</span>(<span class="code-string">"walking"</span>) - <span class="code-function">vector</span>(<span class="code-string">"walk"</span>) + <span class="code-function">vector</span>(<span class="code-string">"swim"</span>) ‚âà <span class="code-function">vector</span>(<span class="code-string">"swimming"</span>)

<span class="code-comment"># Superlatives</span>
<span class="code-function">vector</span>(<span class="code-string">"biggest"</span>) - <span class="code-function">vector</span>(<span class="code-string">"big"</span>) + <span class="code-function">vector</span>(<span class="code-string">"small"</span>) ‚âà <span class="code-function">vector</span>(<span class="code-string">"smallest"</span>)</code></pre>
          <p style="margin-top: 1rem; color: #a0aec0; font-size: 0.9rem;">
            The model learned these relationships purely from word co-occurrence patterns&mdash;no explicit knowledge was provided!
          </p>
        </div>

        <div class="expert-insight">
          <div class="expert-insight-header">
            üßÆ Technical Intuition
          </div>
          <p>
            Why does this work? The direction from "man" to "woman" captures the concept of "gender change." The direction from "France" to "Paris" captures "capital of." When you add and subtract these directions, you're combining concepts algebraically. It's not perfect, but it's remarkable that it works at all&mdash;and that it emerged automatically from prediction.
          </p>
        </div>

        <h3>GloVe: Combining Count-Based and Neural Methods (2014)</h3>
        <p>
          Stanford's <strong>GloVe</strong> (Global Vectors) combined the insights of count-based methods (like TF-IDF) with neural embedding learning. Instead of predicting context words directly, GloVe trained on word co-occurrence statistics from the entire corpus.
        </p>

        <h3>The Limitation: One Vector Per Word</h3>
        <p>
          Word2Vec and GloVe assign exactly <em>one</em> vector to each word. But words have multiple meanings:
        </p>
        <ul>
          <li><strong>"bank"</strong>: financial institution vs. river bank</li>
          <li><strong>"cell"</strong>: biological cell vs. prison cell vs. phone</li>
          <li><strong>"right"</strong>: correct vs. opposite of left vs. political leaning</li>
        </ul>
        <p>
          These static embeddings average all meanings together. Solving this required models that could understand <em>context</em>&mdash;which led to the next breakthrough.
        </p>

        <!-- ============================================
             SECTION 10.4: SEQUENCE MODELS
             ============================================ -->
        <h2 id="rnns">10.4 Sequence Models: Learning to Remember</h2>

        <p>
          To capture context, we need models that process sequences of words and maintain a "memory" of what they've seen. Enter <strong>Recurrent Neural Networks (RNNs)</strong>.
        </p>

        <h3>The RNN Architecture</h3>
        <p>
          An RNN processes one word at a time, maintaining a <strong>hidden state</strong> that gets updated at each step:
        </p>

        <div class="demo-box">
          <div class="demo-box-title">üîÑ RNN Processing Step-by-Step</div>
          <pre><code><span class="code-comment"># Processing "The cat sat on the mat"</span>

<span class="code-variable">h_0</span> = [<span class="code-number">0</span>, <span class="code-number">0</span>, <span class="code-number">0</span>, ...]        <span class="code-comment"># Initial hidden state (zeros)</span>

<span class="code-comment"># Step 1: Process "The"</span>
<span class="code-variable">h_1</span> = <span class="code-function">tanh</span>(W_h @ h_0 + W_x @ <span class="code-function">embed</span>(<span class="code-string">"The"</span>) + b)

<span class="code-comment"># Step 2: Process "cat" - h_1 carries info about "The"</span>
<span class="code-variable">h_2</span> = <span class="code-function">tanh</span>(W_h @ h_1 + W_x @ <span class="code-function">embed</span>(<span class="code-string">"cat"</span>) + b)

<span class="code-comment"># Step 3: Process "sat" - h_2 carries info about "The cat"</span>
<span class="code-variable">h_3</span> = <span class="code-function">tanh</span>(W_h @ h_2 + W_x @ <span class="code-function">embed</span>(<span class="code-string">"sat"</span>) + b)

<span class="code-comment"># ... and so on</span>

<span class="code-comment"># The hidden state h_t encodes context from all previous words</span>
<span class="code-comment"># But information from early words gets "diluted" over time</span></code></pre>
        </div>

        <h3>The Vanishing Gradient Problem</h3>
        <p>
          Standard RNNs have a critical flaw: during training, gradients either <strong>vanish</strong> (shrink to nearly zero) or <strong>explode</strong> (grow uncontrollably) when backpropagating through many time steps. This means:
        </p>
        <ul>
          <li>The network struggles to learn long-range dependencies</li>
          <li>Information from early words gets lost</li>
          <li>In "The cat that the dog chased <strong>ran</strong> away," connecting "cat" to "ran" is hard</li>
        </ul>

        <h3>LSTMs: Learning What to Remember (1997)</h3>
        <p>
          <strong>Long Short-Term Memory</strong> networks, invented by Sepp Hochreiter and J√ºrgen Schmidhuber, solved this with a clever architecture using <strong>gates</strong> that control information flow:
        </p>

        <div class="comparison-grid">
          <div class="comparison-card">
            <h4>üö™ Forget Gate</h4>
            <p>Decides what information from the previous cell state to discard. "Should I still remember that the subject was 'cat'?"</p>
          </div>
          <div class="comparison-card">
            <h4>üì• Input Gate</h4>
            <p>Decides what new information to store. "Is this word important enough to remember?"</p>
          </div>
          <div class="comparison-card">
            <h4>üì§ Output Gate</h4>
            <p>Decides what to output based on the cell state. "What's relevant for the current prediction?"</p>
          </div>
        </div>

        <div class="expert-insight">
          <div class="expert-insight-header">
            üî¨ Why Gates Matter
          </div>
          <p>
            The gates allow the network to maintain information over long distances. The key is the <strong>cell state</strong>&mdash;a "highway" that information can flow along unchanged. The forget gate can be set to nearly 1.0, meaning "keep everything," allowing gradients to flow unimpeded. This architectural innovation was crucial&mdash;and the idea of gating information flow appears again in transformers.
          </p>
        </div>

        <h3>Sequence-to-Sequence Models (2014)</h3>
        <p>
          For tasks like translation, where input and output lengths differ, <strong>Sutskever et al.</strong> introduced the encoder-decoder architecture:
        </p>

        <div class="demo-box">
          <div class="demo-box-title">üîÑ Seq2Seq for Translation</div>
          <pre><code><span class="code-comment"># Translating "The cat sat" ‚Üí "Le chat s'assit"</span>

<span class="code-comment"># ENCODER: Process input and compress to a single vector</span>
h_1 = LSTM(<span class="code-string">"The"</span>, h_0)
h_2 = LSTM(<span class="code-string">"cat"</span>, h_1)
h_3 = LSTM(<span class="code-string">"sat"</span>, h_2)
<span class="code-variable">context</span> = h_3  <span class="code-comment"># This single vector must encode the entire input!</span>

<span class="code-comment"># DECODER: Generate output one word at a time</span>
s_1 = LSTM(<span class="code-string">"&lt;START&gt;"</span>, context) ‚Üí <span class="code-string">"Le"</span>
s_2 = LSTM(<span class="code-string">"Le"</span>, s_1)          ‚Üí <span class="code-string">"chat"</span>
s_3 = LSTM(<span class="code-string">"chat"</span>, s_2)        ‚Üí <span class="code-string">"s'assit"</span>
s_4 = LSTM(<span class="code-string">"s'assit"</span>, s_3)    ‚Üí <span class="code-string">"&lt;END&gt;"</span></code></pre>
        </div>

        <p>
          This was the state of the art for machine translation in 2014-2016. But there was a fundamental problem: the entire input had to be compressed into a single fixed-size vector. For long sentences, this <strong>bottleneck</strong> caused severe information loss.
        </p>

        <!-- ============================================
             SECTION 10.5: ATTENTION
             ============================================ -->
        <h2 id="attention">10.5 Attention: The Breakthrough (2014-2017)</h2>

        <p>
          The attention mechanism, introduced by <strong>Bahdanau et al. (2015)</strong>, solved the bottleneck problem with an elegant idea: instead of forcing everything through one vector, let the decoder <strong>look back at all encoder states</strong> and decide which ones are relevant at each step.
        </p>

        <h3>How Attention Works</h3>
        <p>
          At each decoding step, attention computes a <strong>weighted sum</strong> of all encoder hidden states. The weights reflect "how relevant is each input word for generating this output word?"
        </p>

        <div class="demo-box">
          <div class="demo-box-title">üëÅÔ∏è Attention Mechanism</div>
          <pre><code><span class="code-comment"># Generating the French word "chat" (cat)</span>

<span class="code-comment"># Encoder states from "The cat sat"</span>
<span class="code-variable">encoder_states</span> = [h_the, h_cat, h_sat]

<span class="code-comment"># Current decoder state</span>
<span class="code-variable">decoder_state</span> = s_1

<span class="code-comment"># Compute attention scores (how relevant is each encoder state?)</span>
<span class="code-variable">scores</span> = [<span class="code-function">dot</span>(s_1, h_the),   <span class="code-comment"># 0.1 - "The" not very relevant</span>
          <span class="code-function">dot</span>(s_1, h_cat),   <span class="code-comment"># 0.8 - "cat" highly relevant!</span>
          <span class="code-function">dot</span>(s_1, h_sat)]   <span class="code-comment"># 0.1 - "sat" not very relevant</span>

<span class="code-comment"># Convert to probabilities with softmax</span>
<span class="code-variable">attention_weights</span> = <span class="code-function">softmax</span>(scores)  <span class="code-comment"># [0.1, 0.8, 0.1]</span>

<span class="code-comment"># Weighted sum of encoder states</span>
<span class="code-variable">context</span> = <span class="code-number">0.1</span>*h_the + <span class="code-number">0.8</span>*h_cat + <span class="code-number">0.1</span>*h_sat

<span class="code-comment"># Use context to help predict "chat"</span>
<span class="code-variable">output</span> = <span class="code-function">predict</span>(decoder_state, context)  <span class="code-comment"># ‚Üí "chat"</span></code></pre>
        </div>

        <div class="key-concept">
          <div class="key-concept-title">Attention as Soft Alignment</div>
          <p>
            Attention learns which input words "align" with which output words. When translating "chat," the model focuses on "cat." When generating a verb, it attends to the input verb. This alignment emerges automatically from training&mdash;no one told the model which words correspond to which.
          </p>
        </div>

        <h3>The Transformer Insight (2017)</h3>
        <p>
          The landmark paper <em><strong>"Attention Is All You Need"</strong></em> by Vaswani et al. asked a radical question: what if we got rid of recurrence entirely and used <strong>only attention</strong>?
        </p>

        <p>
          The key innovation was <strong>self-attention</strong>: instead of the decoder attending to encoder states, every word attends to every other word in the same sequence.
        </p>

        <table>
          <thead>
            <tr>
              <th>RNN/LSTM</th>
              <th>Transformer</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Processes words <strong>sequentially</strong></td>
              <td>Processes all words <strong>in parallel</strong></td>
            </tr>
            <tr>
              <td>Slow to train (can't parallelize across time)</td>
              <td>Fast to train (massively parallelizable)</td>
            </tr>
            <tr>
              <td>Information travels through hidden states</td>
              <td>Every word can directly attend to every other</td>
            </tr>
            <tr>
              <td>Struggles with very long sequences</td>
              <td>Handles long sequences (up to context limit)</td>
            </tr>
            <tr>
              <td>O(n) sequential operations</td>
              <td>O(1) sequential operations, O(n¬≤) attention</td>
            </tr>
          </tbody>
        </table>

        <!-- ============================================
             SECTION 10.6: TRANSFORMERS
             ============================================ -->
        <h2 id="transformers">10.6 Transformers: The Architecture That Changed Everything</h2>

        <p>
          The Transformer architecture has become the foundation of virtually all modern language AI. Let's understand its key components.
        </p>

        <h3>Self-Attention: The Core Mechanism</h3>
        <p>
          In self-attention, each word computes a <strong>query</strong>, <strong>key</strong>, and <strong>value</strong>:
        </p>

        <div class="demo-box">
          <div class="demo-box-title">üîë Query, Key, Value</div>
          <pre><code><span class="code-comment"># For each word, compute Q, K, V by linear projection</span>
<span class="code-variable">Q</span> = X @ W_Q  <span class="code-comment"># Query: "What am I looking for?"</span>
<span class="code-variable">K</span> = X @ W_K  <span class="code-comment"># Key: "What do I contain?"</span>
<span class="code-variable">V</span> = X @ W_V  <span class="code-comment"># Value: "What do I contribute?"</span>

<span class="code-comment"># Attention scores: how much should each word attend to each other word?</span>
<span class="code-variable">scores</span> = Q @ K.T / <span class="code-function">sqrt</span>(d_k)  <span class="code-comment"># Scale by dimension</span>

<span class="code-comment"># Softmax to get probabilities</span>
<span class="code-variable">attention</span> = <span class="code-function">softmax</span>(scores)

<span class="code-comment"># Output: weighted sum of values</span>
<span class="code-variable">output</span> = attention @ V</code></pre>
          <p style="margin-top: 1rem; color: #a0aec0; font-size: 0.9rem;">
            Think of it like a library: Query = "I'm looking for books about cats", Key = "This shelf has books about animals", Value = "Here are the actual books." High query-key similarity means the value gets more weight.
          </p>
        </div>

        <h3>Multi-Head Attention</h3>
        <p>
          Instead of single attention, transformers use <strong>multiple attention heads</strong> that each learn different patterns:
        </p>
        <ul>
          <li><strong>Head 1</strong> might learn syntactic dependencies (subject-verb agreement)</li>
          <li><strong>Head 2</strong> might learn semantic relationships (pronoun resolution)</li>
          <li><strong>Head 3</strong> might learn positional patterns (what's nearby)</li>
        </ul>

        <h3>Positional Encoding</h3>
        <p>
          Since attention is permutation-invariant (order doesn't matter by default), transformers add <strong>positional encodings</strong> to indicate where each word is in the sequence:
        </p>

        <div class="math-block">
          PE(pos, 2i) = sin(pos / 10000^(2i/d))
          <br>
          PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
        </div>

        <p>
          These sinusoidal functions allow the model to learn relative positions and generalize to sequence lengths not seen during training.
        </p>

        <h3>The Full Architecture</h3>
        <p>
          A transformer layer combines:
        </p>
        <ol>
          <li><strong>Multi-head self-attention</strong></li>
          <li><strong>Layer normalization</strong> (stabilizes training)</li>
          <li><strong>Feed-forward network</strong> (processes each position independently)</li>
          <li><strong>Residual connections</strong> (allows gradients to flow)</li>
        </ol>
        <p>
          Stack many such layers (12 for BERT-base, 96 for GPT-4), and you have a modern language model.
        </p>

        <div class="expert-insight">
          <div class="expert-insight-header">
            üèóÔ∏è Why This Architecture Works
          </div>
          <p>
            The transformer's power comes from several factors: (1) Every word can directly attend to every other, enabling global reasoning; (2) All positions are processed in parallel, enabling massive scaling; (3) Deep stacking with residual connections allows complex computations; (4) The architecture is remarkably amenable to scaling&mdash;bigger models consistently perform better. This last property enabled the scaling revolution that produced GPT-3 and beyond.
          </p>
        </div>

        <!-- ============================================
             SECTION 10.7: THE MODERN ERA
             ============================================ -->
        <h2 id="modern-era">10.7 The Modern Era: Scale, RLHF, and Beyond</h2>

        <h3>The Scaling Hypothesis</h3>
        <p>
          A remarkable empirical finding: language model performance improves predictably with scale. The <strong>scaling laws</strong> discovered by OpenAI and others show:
        </p>

        <div class="math-block">
          Performance ‚àù (Parameters)^Œ± √ó (Data)^Œ≤ √ó (Compute)^Œ≥
        </div>

        <p>
          This meant that if you wanted better models, you "just" needed to make them bigger and train them on more data with more compute. This insight drove the race from millions to billions to trillions of parameters.
        </p>

        <h3>Timeline of Pre-trained Models</h3>

        <div class="timeline">
          <div class="timeline-item">
            <div class="timeline-year">2018</div>
            <div class="timeline-title">BERT (Google)</div>
            <div class="timeline-desc">
              <strong>Bidirectional</strong> pre-training with masked language modeling. Revolutionized NLP benchmarks. 110M-340M parameters.
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-year">2018</div>
            <div class="timeline-title">GPT (OpenAI)</div>
            <div class="timeline-desc">
              <strong>Unidirectional</strong> (left-to-right) pre-training. Demonstrated the power of generative pre-training. 117M parameters.
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-year">2019</div>
            <div class="timeline-title">GPT-2</div>
            <div class="timeline-desc">
              Scaled to 1.5B parameters. Generated such coherent text that OpenAI initially withheld the full model, citing concerns about misuse.
            </div>
          </div>
          <div class="timeline-item highlight">
            <div class="timeline-year">2020</div>
            <div class="timeline-title">GPT-3</div>
            <div class="timeline-desc">
              <strong>175 billion parameters</strong>. Demonstrated remarkable "few-shot learning"&mdash;performing tasks from just a few examples in the prompt, with no fine-tuning.
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-year">2020</div>
            <div class="timeline-title">T5 (Google)</div>
            <div class="timeline-desc">
              "Text-to-Text Transfer Transformer." Unified framework that casts all NLP tasks as text generation.
            </div>
          </div>
          <div class="timeline-item highlight">
            <div class="timeline-year">2022</div>
            <div class="timeline-title">ChatGPT (OpenAI)</div>
            <div class="timeline-desc">
              Applied <strong>RLHF</strong> (Reinforcement Learning from Human Feedback) to make models more helpful, harmless, and honest. Sparked the current AI boom.
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-year">2023</div>
            <div class="timeline-title">GPT-4</div>
            <div class="timeline-desc">
              <strong>Multimodal</strong> (text + images). Advanced reasoning. Architecture details not disclosed.
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-year">2023+</div>
            <div class="timeline-title">Claude (Anthropic)</div>
            <div class="timeline-desc">
              <strong>Constitutional AI</strong> approach to safety. Focus on being helpful, harmless, and honest through AI feedback and red-teaming.
            </div>
          </div>
          <div class="timeline-item">
            <div class="timeline-year">2023+</div>
            <div class="timeline-title">Open-Source Models</div>
            <div class="timeline-desc">
              LLaMA (Meta), Mistral, Mixtral, and others democratize access to powerful models. Smaller models achieve competitive performance.
            </div>
          </div>
        </div>

        <h3>RLHF: Teaching Models to Be Helpful</h3>
        <p>
          Pre-training creates models that predict text well, but not necessarily models that are <em>useful</em> or <em>safe</em>. <strong>Reinforcement Learning from Human Feedback</strong> bridges this gap:
        </p>

        <ol>
          <li><strong>Supervised Fine-Tuning:</strong> Train on examples of good conversations</li>
          <li><strong>Reward Model:</strong> Train a model to predict which responses humans prefer</li>
          <li><strong>RL Optimization:</strong> Optimize the language model to generate responses the reward model rates highly</li>
        </ol>

        <div class="key-concept">
          <div class="key-concept-title">The Alignment Problem</div>
          <p>
            RLHF is part of the broader "alignment" challenge: ensuring AI systems do what we actually want, not just what we literally said. A model optimized purely for engagement might learn to be manipulative. One optimized for helpfulness might overpromise. Getting this right is one of the central challenges in AI safety.
          </p>
        </div>

        <h3>Emergent Capabilities</h3>
        <p>
          Large language models exhibit capabilities that weren't explicitly trained for and weren't present in smaller models:
        </p>
        <ul>
          <li><strong>In-context learning:</strong> Learning new tasks from examples in the prompt</li>
          <li><strong>Chain-of-thought reasoning:</strong> Solving problems step-by-step when prompted to "think aloud"</li>
          <li><strong>Code generation:</strong> Writing functional programs from descriptions</li>
          <li><strong>Multilingual transfer:</strong> Capabilities learned in one language appearing in others</li>
        </ul>

        <p>
          Whether these are truly "emergent" or artifacts of measurement is debated, but the practical impact is undeniable.
        </p>

        <!-- ============================================
             SECTION 10.8: WHY THIS MATTERS FOR ECONOMISTS
             ============================================ -->
        <h2 id="economics">10.8 Why This Matters for Economists</h2>

        <p>
          Understanding NLP history isn't just intellectual curiosity&mdash;it has direct implications for empirical research.
        </p>

        <h3>Text as Data in Economics</h3>
        <p>
          Modern NLP enables economists to analyze text at scale:
        </p>

        <div class="comparison-grid">
          <div class="comparison-card">
            <h4>üì∞ Policy Analysis</h4>
            <ul>
              <li>Fed communications and market reactions</li>
              <li>Regulatory text and compliance costs</li>
              <li>Political speech and polarization</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üìà Financial Applications</h4>
            <ul>
              <li>Earnings call sentiment analysis</li>
              <li>News-based volatility prediction</li>
              <li>10-K filing analysis</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üè≠ Firm Behavior</h4>
            <ul>
              <li>Patent text and innovation measurement</li>
              <li>Job postings and skill demand</li>
              <li>Product descriptions and market positioning</li>
            </ul>
          </div>
          <div class="comparison-card">
            <h4>üë• Social Science</h4>
            <ul>
              <li>Survey open-ended responses</li>
              <li>Social media analysis</li>
              <li>Historical document analysis</li>
            </ul>
          </div>
        </div>

        <h3>Practical Implications for Research</h3>

        <div class="info-box tip">
          <div class="info-box-title">When to Use What</div>
          <ul>
            <li><strong>Simple classification (spam, sentiment):</strong> Traditional ML (logistic regression, random forests) often suffices and is more interpretable</li>
            <li><strong>Similarity/clustering:</strong> Word embeddings (Word2Vec, GloVe) or sentence embeddings (Sentence-BERT)</li>
            <li><strong>Named entity recognition:</strong> Fine-tuned BERT models</li>
            <li><strong>Text generation, summarization, Q&A:</strong> Large language models (GPT, Claude)</li>
            <li><strong>Domain-specific tasks:</strong> Consider fine-tuning or domain-specific models (e.g., FinBERT for finance)</li>
          </ul>
        </div>

        <div class="info-box warning">
          <div class="info-box-title">Caveats for Research</div>
          <ul>
            <li><strong>Reproducibility:</strong> LLM outputs can vary; document versions, prompts, and settings carefully</li>
            <li><strong>Bias:</strong> Models reflect training data biases; validate on your specific domain</li>
            <li><strong>Cost:</strong> API costs can add up; design efficient pipelines</li>
            <li><strong>Interpretability:</strong> Neural models are harder to interpret than traditional methods</li>
            <li><strong>Data contamination:</strong> Test data may have been in training data for large models</li>
          </ul>
        </div>

        <h3>Key Papers Using NLP in Economics</h3>
        <ul>
          <li>Gentzkow & Shapiro (2010): Media bias measurement using text</li>
          <li>Hansen & McMahon (2016): FOMC communication and forward guidance</li>
          <li>Bloom et al. (2018): Economic policy uncertainty index</li>
          <li>Ash et al. (2023): Text as Data in Economics (survey)</li>
        </ul>

        <!-- ============================================
             SECTION 10.9: REFERENCES
             ============================================ -->
        <div class="references-section" id="references">
          <h2>References & Further Reading</h2>

          <div class="reference-category">
            <h3>Foundational Papers</h3>

            <div class="reference-item">
              <a href="https://www.csee.umbc.edu/courses/471/papers/turing.pdf" target="_blank">Computing Machinery and Intelligence</a>
              <div class="reference-authors">Turing, A. M. (1950)</div>
              <div class="reference-venue">Mind, 59(236), 433-460. The paper that started it all.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations in Vector Space</a>
              <div class="reference-authors">Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013)</div>
              <div class="reference-venue">The Word2Vec paper. Introduced skip-gram and CBOW.</div>
            </div>

            <div class="reference-item">
              <a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank">GloVe: Global Vectors for Word Representation</a>
              <div class="reference-authors">Pennington, J., Socher, R., & Manning, C. (2014)</div>
              <div class="reference-venue">EMNLP 2014. Combined count-based and neural methods.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/1409.3215" target="_blank">Sequence to Sequence Learning with Neural Networks</a>
              <div class="reference-authors">Sutskever, I., Vinyals, O., & Le, Q. V. (2014)</div>
              <div class="reference-venue">NeurIPS 2014. Introduced seq2seq for machine translation.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/1409.0473" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>
              <div class="reference-authors">Bahdanau, D., Cho, K., & Bengio, Y. (2015)</div>
              <div class="reference-venue">ICLR 2015. Introduced the attention mechanism.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>
              <div class="reference-authors">Vaswani, A., et al. (2017)</div>
              <div class="reference-venue">NeurIPS 2017. The Transformer paper that changed everything.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers</a>
              <div class="reference-authors">Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019)</div>
              <div class="reference-venue">NAACL 2019. Bidirectional pre-training revolutionized NLP benchmarks.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/2005.14165" target="_blank">Language Models are Few-Shot Learners</a>
              <div class="reference-authors">Brown, T., et al. (2020)</div>
              <div class="reference-venue">NeurIPS 2020. The GPT-3 paper demonstrating emergent in-context learning.</div>
            </div>
          </div>

          <div class="reference-category">
            <h3>Scaling and Training</h3>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a>
              <div class="reference-authors">Kaplan, J., et al. (2020)</div>
              <div class="reference-venue">OpenAI. Empirical laws relating model size, data, and compute to performance.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>
              <div class="reference-authors">Ouyang, L., et al. (2022)</div>
              <div class="reference-venue">NeurIPS 2022. The InstructGPT paper introducing RLHF for alignment.</div>
            </div>

            <div class="reference-item">
              <a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional AI: Harmlessness from AI Feedback</a>
              <div class="reference-authors">Bai, Y., et al. (2022)</div>
              <div class="reference-venue">Anthropic. Alternative approach to alignment using AI feedback.</div>
            </div>
          </div>

          <div class="reference-category">
            <h3>Text as Data in Economics</h3>

            <div class="reference-item">
              <a href="https://www.nber.org/papers/w15916" target="_blank">What Drives Media Slant? Evidence from U.S. Daily Newspapers</a>
              <div class="reference-authors">Gentzkow, M., & Shapiro, J. (2010)</div>
              <div class="reference-venue">Econometrica. Pioneering use of text analysis in economics.</div>
            </div>

            <div class="reference-item">
              <a href="https://academic.oup.com/qje/article/131/4/1593/2468873" target="_blank">Shocking Language: Understanding the Macroeconomic Effects of Central Bank Communication</a>
              <div class="reference-authors">Hansen, S., & McMahon, M. (2016)</div>
              <div class="reference-venue">QJE. Text analysis of Federal Reserve communications.</div>
            </div>

            <div class="reference-item">
              <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-economics-082222-074352" target="_blank">Text as Data in Economics</a>
              <div class="reference-authors">Ash, E., & Hansen, S. (2023)</div>
              <div class="reference-venue">Annual Review of Economics. Comprehensive survey of NLP in economics research.</div>
            </div>
          </div>

          <div class="reference-category">
            <h3>Books & Tutorials</h3>

            <div class="reference-item">
              <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank">Speech and Language Processing</a>
              <div class="reference-authors">Jurafsky, D., & Martin, J. H. (3rd ed., in progress)</div>
              <div class="reference-venue">The standard NLP textbook. Free online.</div>
            </div>

            <div class="reference-item">
              <a href="https://d2l.ai/" target="_blank">Dive into Deep Learning</a>
              <div class="reference-authors">Zhang, A., Lipton, Z., Li, M., & Smola, A.</div>
              <div class="reference-venue">Interactive deep learning textbook with code. Excellent transformer chapters.</div>
            </div>

            <div class="reference-item">
              <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ" target="_blank">CS224N: NLP with Deep Learning</a>
              <div class="reference-authors">Stanford University (Manning, et al.)</div>
              <div class="reference-venue">Excellent video lectures covering modern NLP. Free on YouTube.</div>
            </div>

            <div class="reference-item">
              <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank">The Illustrated Transformer</a>
              <div class="reference-authors">Alammar, J.</div>
              <div class="reference-venue">Visual explanation of transformer architecture. Highly recommended.</div>
            </div>
          </div>

          <div class="reference-category">
            <h3>Historical Interest</h3>

            <div class="reference-item">
              <a href="https://dl.acm.org/doi/10.1145/365153.365168" target="_blank">ELIZA‚ÄîA Computer Program For the Study of Natural Language Communication</a>
              <div class="reference-authors">Weizenbaum, J. (1966)</div>
              <div class="reference-venue">CACM. The original ELIZA paper.</div>
            </div>

            <div class="reference-item">
              <a href="https://www.aclweb.org/anthology/J97-1003/" target="_blank">Long Short-Term Memory</a>
              <div class="reference-authors">Hochreiter, S., & Schmidhuber, J. (1997)</div>
              <div class="reference-venue">Neural Computation. The LSTM paper that solved vanishing gradients.</div>
            </div>

            <div class="reference-item">
              <a href="https://www.nap.edu/catalog/9547/language-and-machines-computers-in-translation-and-linguistics" target="_blank">ALPAC Report: Language and Machines</a>
              <div class="reference-authors">ALPAC (1966)</div>
              <div class="reference-venue">National Academy of Sciences. The report that triggered the first AI winter for NLP.</div>
            </div>
          </div>
        </div>

        <div class="nav-footer">
          <a href="09-github.html" class="nav-link prev">Module 9: GitHub</a>
          <a href="11-machine-learning.html" class="nav-link next">Module 11: Machine Learning</a>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about NLP history, transformers, attention mechanisms, or any of the course material. How can I assist you today?</p>
          <p style="font-size: 0.85em; color: #718096; margin-top: 0.5rem;"><em>Note: My answers are generated by an LLM and may contain errors. Always verify important information.</em></p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    let tooltipEl = null;
    let currentTarget = null;
    let hideTimeout = null;

    function createTooltip() {
      if (tooltipEl) return tooltipEl;
      tooltipEl = document.createElement('div');
      tooltipEl.className = 'tooltip-popup';
      document.body.appendChild(tooltipEl);
      return tooltipEl;
    }

    function positionTooltip(target) {
      const tooltip = createTooltip();
      const tipText = target.getAttribute('data-tip');
      if (!tipText) return;

      tooltip.textContent = tipText;
      tooltip.className = 'tooltip-popup';

      const targetRect = target.getBoundingClientRect();
      const viewportWidth = window.innerWidth;
      const viewportHeight = window.innerHeight;
      const padding = 10;

      tooltip.style.visibility = 'hidden';
      tooltip.style.display = 'block';
      tooltip.classList.add('visible');

      const tooltipRect = tooltip.getBoundingClientRect();
      const tooltipWidth = tooltipRect.width;
      const tooltipHeight = tooltipRect.height;

      let left = targetRect.left + (targetRect.width / 2) - (tooltipWidth / 2);
      let top = targetRect.top - tooltipHeight - 8;
      let arrowClass = 'arrow-bottom';

      if (top < padding) {
        top = targetRect.bottom + 8;
        arrowClass = 'arrow-top';
      }

      if (left < padding) left = padding;
      if (left + tooltipWidth > viewportWidth - padding) left = viewportWidth - tooltipWidth - padding;

      tooltip.style.left = left + 'px';
      tooltip.style.top = top + 'px';
      tooltip.style.visibility = 'visible';
      tooltip.classList.add(arrowClass);
    }

    function showTooltip(target) {
      if (hideTimeout) { clearTimeout(hideTimeout); hideTimeout = null; }
      currentTarget = target;
      positionTooltip(target);
    }

    function hideTooltip() {
      hideTimeout = setTimeout(function() {
        if (tooltipEl) tooltipEl.classList.remove('visible');
        currentTarget = null;
      }, 100);
    }

    document.addEventListener('mouseenter', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) showTooltip(e.target);
    }, true);

    document.addEventListener('mouseleave', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) hideTooltip();
    }, true);

    document.addEventListener('scroll', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget);
    }, true);

    window.addEventListener('resize', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget);
    });
  })();
  </script>
</body>
</html>
