<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 9: History of NLP | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content {
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
    }
  </style>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <p style="font-size: 0.9rem; color: #666; margin-bottom: 1.5rem;">Please enter the course password to access the materials.</p>
      <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
      <button id="password-submit">Access Course</button>
      <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html">Welcome</a></li>
          <li><a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a></li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li><a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a></li>
          <li><a href="03-data-cleaning.html"><span class="module-number">3</span> Data Cleaning</a></li>
          <li><a href="04-data-analysis.html"><span class="module-number">4</span> Data Analysis</a></li>
          <li><a href="05-causal-inference.html"><span class="module-number">5</span> Causal Inference</a></li>
          <li><a href="06-estimation.html"><span class="module-number">6</span> Estimation Methods</a></li>
          <li><a href="07-replicability.html"><span class="module-number">7</span> Replicability</a></li>
          <li><a href="08-github.html"><span class="module-number">8</span> Git & GitHub</a></li>
          <li><a href="09-nlp-history.html" class="active"><span class="module-number">9</span> History of NLP</a></li>
          <li><a href="10-machine-learning.html"><span class="module-number">10</span> Machine Learning</a></li>
          <li><a href="11-llms.html"><span class="module-number">11</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact &amp; Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <div class="module-header">
          <h1>9 &nbsp;History of NLP</h1>
          <div class="module-meta">
            <span>~6 hours</span>
            <span>From Rules to Transformers</span>
            <span>Conceptual</span>
          </div>
        </div>

        <div class="learning-objectives">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Trace the evolution of NLP from rule-based systems to deep learning</li>
            <li>Understand the key algorithmic breakthroughs at each stage</li>
            <li>Appreciate why the Transformer architecture was revolutionary</li>
            <li>Connect historical developments to modern LLMs</li>
          </ul>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#early-days">9.1 The Early Days (1950s-1980s)</a></li>
            <li><a href="#statistical">9.2 Statistical NLP (1990s-2000s)</a></li>
            <li><a href="#embeddings">9.3 Word Embeddings (2013)</a></li>
            <li><a href="#rnns">9.4 Recurrent Neural Networks</a></li>
            <li><a href="#attention">9.5 The Attention Revolution (2017)</a></li>
            <li><a href="#transformers">9.6 Transformers & Beyond</a></li>
          </ul>
        </div>

        <h2 id="early-days">9.1 The Early Days (1950s-1980s)</h2>

        <h3>The Turing Test (1950)</h3>
        <p>
          Alan Turing's paper "Computing Machinery and Intelligence" posed the question: Can machines think? He proposed the <strong>imitation game</strong>--later called the Turing Test--where a machine passes if a human can't distinguish it from another human through text conversation.
        </p>

        <h3>Rule-Based Systems</h3>
        <p>
          Early NLP relied entirely on hand-crafted rules. Linguists wrote grammar rules; programmers encoded them. This approach was called <strong>symbolic AI</strong> or <strong>Good Old-Fashioned AI (GOFAI)</strong>.
        </p>

        <div class="info-box note">
          <div class="info-box-title">ELIZA (1966)</div>
          <p>
            Joseph Weizenbaum at MIT created ELIZA, one of the first chatbots. It used pattern matching to simulate a Rogerian psychotherapist. Despite its simplicity, people found it surprisingly engaging--the "ELIZA effect."
          </p>
<pre style="background: #f7fafc; padding: 1rem; margin-top: 0.5rem; font-family: var(--font-code); font-size: 0.85rem;">
User: I am sad.
ELIZA: I am sorry to hear you are sad.
User: My mother makes me angry.
ELIZA: Tell me more about your family.
</pre>
          <p>ELIZA worked by matching keywords ("mother" -> family topic) and transforming sentences with templates. No understanding--just clever rules.</p>
        </div>

        <h3>The ALPAC Report (1966)</h3>
        <p>
          The Automatic Language Processing Advisory Committee concluded that machine translation was not feasible with current approaches. Funding dried up, leading to an "AI winter" for NLP. The report was right about rule-based methods--but wrong about the ultimate possibility.
        </p>

        <h3>Chomsky vs. Statistical Methods</h3>
        <p>
          Noam Chomsky's transformational grammar dominated linguistics. He argued that language was governed by innate, universal rules--and that statistical approaches were fundamentally misguided. This view held back statistical NLP for decades.
        </p>

        <blockquote>
          "It must be recognized that the notion 'probability of a sentence' is an entirely useless one."<br>
          -- Noam Chomsky, Syntactic Structures (1957)
        </blockquote>

        <h2 id="statistical">9.2 Statistical NLP (1990s-2000s)</h2>

        <p>
          The statistical revolution came when researchers started treating language as data to be modeled probabilistically, rather than rules to be encoded.
        </p>

        <h3>N-gram Language Models</h3>
        <p>
          An <strong>n-gram model</strong> predicts the next word based on the previous n-1 words. Despite their simplicity, n-grams powered early speech recognition and machine translation.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">N-gram Example</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Bigram (n=2) probability</span>
<span class="code-comment"># P(word | previous_word) = count(prev, word) / count(prev)</span>

<span class="code-comment"># Example corpus: "the cat sat on the mat"</span>

<span class="code-comment"># Bigram counts:</span>
<span class="code-comment"># "the cat": 1, "cat sat": 1, "sat on": 1, "on the": 1, "the mat": 1</span>

<span class="code-comment"># P("cat" | "the") = count("the cat") / count("the") = 1/2</span>
<span class="code-comment"># P("mat" | "the") = count("the mat") / count("the") = 1/2</span>

<span class="code-comment"># Problem: What about "the dog"?</span>
<span class="code-comment"># P("dog" | "the") = 0/2 = 0  (Never seen -> impossible!)</span>

<span class="code-comment"># Solution: Smoothing (add small counts to unseen n-grams)</span></code></pre>
          </div>
        </div>

        <h3>Hidden Markov Models (HMMs)</h3>
        <p>
          HMMs model sequences where the underlying state is hidden. For NLP, they were used for part-of-speech tagging: given a sequence of words, what's the most likely sequence of tags (noun, verb, adjective...)?
        </p>

        <h3>Bag of Words & TF-IDF</h3>
        <p>
          For document classification and information retrieval, the <strong>bag of words</strong> model ignores word order entirely--a document is just a count of words. <strong>TF-IDF</strong> (Term Frequency-Inverse Document Frequency) weights words by how distinctive they are.
        </p>

        <h3>The IBM Models (1990s)</h3>
        <p>
          IBM's statistical machine translation models learned to translate by aligning words in parallel corpora (texts in two languages). The famous quote:
        </p>
        <blockquote>
          "Every time I fire a linguist, the performance of the speech recognizer goes up."<br>
          -- Fred Jelinek, IBM (apocryphal)
        </blockquote>

        <h2 id="embeddings">9.3 Word Embeddings (2013)</h2>

        <p>
          The breakthrough that changed everything: representing words as <strong>dense vectors</strong> where similar words are close together in vector space.
        </p>

        <h3>Word2Vec</h3>
        <p>
          Tomas Mikolov at Google introduced Word2Vec in 2013. The key insight: train a neural network to predict context words, and the hidden layer weights become word embeddings.
        </p>

        <div class="info-box note">
          <div class="info-box-title">The Magic of Word Vectors</div>
          <p>
            Word2Vec famously captured semantic relationships as vector arithmetic:
          </p>
<pre style="background: #f7fafc; padding: 1rem; margin-top: 0.5rem; font-family: var(--font-code); font-size: 0.85rem;">
vector("king") - vector("man") + vector("woman") = vector("queen")
vector("paris") - vector("france") + vector("italy") = vector("rome")
</pre>
          <p>The model learned these relationships purely from text co-occurrence patterns--no explicit knowledge was provided!</p>
        </div>

        <h3>GloVe (2014)</h3>
        <p>
          Stanford's GloVe (Global Vectors) combined the benefits of count-based methods (like TF-IDF) with neural embedding learning. It trained on word co-occurrence statistics from massive corpora.
        </p>

        <h3>Limitations of Static Embeddings</h3>
        <p>
          Word2Vec and GloVe assign <em>one</em> vector per word. But "bank" means different things in "river bank" vs. "bank account." This limitation motivated the next breakthrough.
        </p>

        <h2 id="rnns">9.4 Recurrent Neural Networks</h2>

        <p>
          To capture context, we need models that process sequences. <strong>Recurrent Neural Networks (RNNs)</strong> maintain a hidden state that updates with each word.
        </p>

        <h3>The Vanishing Gradient Problem</h3>
        <p>
          Standard RNNs struggle with long sequences. Gradients either vanish (-> 0) or explode (-> infinity) when backpropagating through many time steps. The network "forgets" early words.
        </p>

        <h3>LSTMs and GRUs</h3>
        <p>
          <strong>Long Short-Term Memory</strong> (LSTM) networks, invented by Hochreiter & Schmidhuber in 1997, solve this with "gates" that control information flow. <strong>Gated Recurrent Units</strong> (GRUs) are a simpler variant.
        </p>

        <table>
          <thead>
            <tr>
              <th>Architecture</th>
              <th>Year</th>
              <th>Key Innovation</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>RNN</td>
              <td>1986</td>
              <td>Hidden state for sequences</td>
            </tr>
            <tr>
              <td>LSTM</td>
              <td>1997</td>
              <td>Gated memory cells</td>
            </tr>
            <tr>
              <td>GRU</td>
              <td>2014</td>
              <td>Simplified gating</td>
            </tr>
            <tr>
              <td>Seq2Seq</td>
              <td>2014</td>
              <td>Encoder-decoder for translation</td>
            </tr>
          </tbody>
        </table>

        <h3>Sequence-to-Sequence (Seq2Seq)</h3>
        <p>
          For tasks like translation, Sutskever et al. (2014) introduced the encoder-decoder architecture: one LSTM encodes the input into a fixed vector; another decodes it into the output. This was state-of-the-art until attention arrived.
        </p>

        <h2 id="attention">9.5 The Attention Revolution (2017)</h2>

        <h3>Attention Mechanism</h3>
        <p>
          Bahdanau et al. (2015) introduced <strong>attention</strong> for machine translation. Instead of compressing the entire input into one vector, the decoder can "attend" to different parts of the input at each step.
        </p>

        <div class="info-box note">
          <div class="info-box-title">How Attention Works</div>
          <p>
            At each decoding step, attention computes a weighted sum of encoder states. The weights reflect "how relevant is each input word to generating this output word?"
          </p>
          <p>
            This allows the model to handle long sentences and learn alignments (which input words map to which output words) automatically.
          </p>
        </div>

        <h3>"Attention Is All You Need" (2017)</h3>
        <p>
          The landmark paper by Vaswani et al. at Google introduced the <strong>Transformer</strong>--an architecture using <em>only</em> attention, no recurrence. This seemingly simple change was revolutionary.
        </p>

        <table>
          <thead>
            <tr>
              <th>RNNs</th>
              <th>Transformers</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Process words sequentially</td>
              <td>Process all words in parallel</td>
            </tr>
            <tr>
              <td>Slow to train (sequential)</td>
              <td>Fast to train (parallelizable)</td>
            </tr>
            <tr>
              <td>Struggle with long sequences</td>
              <td>Handle long sequences well</td>
            </tr>
            <tr>
              <td>Fixed hidden state size</td>
              <td>Attention over entire sequence</td>
            </tr>
          </tbody>
        </table>

        <h2 id="transformers">9.6 Transformers & Beyond</h2>

        <h3>Key Transformer Components</h3>
        <ul>
          <li><strong>Self-Attention:</strong> Each word attends to every other word in the sequence</li>
          <li><strong>Multi-Head Attention:</strong> Multiple attention patterns learned in parallel</li>
          <li><strong>Positional Encoding:</strong> Since there's no recurrence, position info is added</li>
          <li><strong>Layer Normalization:</strong> Stabilizes training</li>
          <li><strong>Feed-Forward Networks:</strong> Process each position independently</li>
        </ul>

        <h3>The Explosion of Pre-trained Models</h3>

        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Year</th>
              <th>Innovation</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>BERT</strong> (Google)</td>
              <td>2018</td>
              <td>Bidirectional pre-training, masked language modeling</td>
            </tr>
            <tr>
              <td><strong>GPT</strong> (OpenAI)</td>
              <td>2018</td>
              <td>Unidirectional, generative pre-training</td>
            </tr>
            <tr>
              <td><strong>GPT-2</strong></td>
              <td>2019</td>
              <td>Scaled up, "too dangerous to release"</td>
            </tr>
            <tr>
              <td><strong>T5</strong> (Google)</td>
              <td>2020</td>
              <td>Text-to-text framework for all NLP tasks</td>
            </tr>
            <tr>
              <td><strong>GPT-3</strong></td>
              <td>2020</td>
              <td>175B parameters, few-shot learning</td>
            </tr>
            <tr>
              <td><strong>ChatGPT</strong></td>
              <td>2022</td>
              <td>RLHF fine-tuning for dialogue</td>
            </tr>
            <tr>
              <td><strong>GPT-4</strong></td>
              <td>2023</td>
              <td>Multimodal, advanced reasoning</td>
            </tr>
            <tr>
              <td><strong>Claude</strong> (Anthropic)</td>
              <td>2023+</td>
              <td>Constitutional AI, safety focus</td>
            </tr>
          </tbody>
        </table>

        <h3>The Paradigm Shift</h3>
        <p>
          Pre-transformer NLP required task-specific architectures and labeled data for each task. Post-transformer: pre-train once on massive unlabeled text, then fine-tune (or prompt) for any task. This is the foundation of modern LLMs.
        </p>

        <div class="citation">
          <div class="citation-title">Essential Papers</div>
          <ul>
            <li>Vaswani, A., et al. (2017). <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>. NeurIPS.</li>
            <li>Devlin, J., et al. (2018). <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers</a>. NAACL.</li>
            <li>Mikolov, T., et al. (2013). <a href="https://arxiv.org/abs/1301.3781" target="_blank">Efficient Estimation of Word Representations</a>. (Word2Vec)</li>
          </ul>
        </div>

        <div class="nav-footer">
          <a href="08-github.html" class="nav-link prev">Module 8: Git & GitHub</a>
          <a href="10-machine-learning.html" class="nav-link next">Module 10: Machine Learning</a>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about Python, Stata, R, causal inference methods, or any of the course material. How can I assist you today?</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>
</body>
</html>
