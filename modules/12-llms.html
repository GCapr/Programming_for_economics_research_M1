<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 12: Large Language Models | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content {
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
    }
    /* Code tooltips - hover explanations (JavaScript-powered for boundary detection) */
    .code-tooltip {
      position: relative;
      cursor: help;
      border-bottom: 1px dotted #888;
      text-decoration: none;
    }

    /* Tooltip element created by JavaScript */
    .tooltip-popup {
      position: fixed;
      background: #1f2937;
      color: white;
      padding: 0.5rem 0.75rem;
      border-radius: 6px;
      font-size: 0.75rem;
      white-space: normal;
      max-width: 300px;
      opacity: 0;
      pointer-events: none;
      transition: opacity 0.15s ease-in-out;
      z-index: 10000;
      line-height: 1.4;
      text-align: left;
      font-family: var(--font-body);
      font-style: normal;
      box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    }
    .tooltip-popup.visible {
      opacity: 1;
    }

    /* Small arrow pointer */
    .tooltip-popup::after {
      content: '';
      position: absolute;
      border: 6px solid transparent;
    }
    .tooltip-popup.arrow-bottom::after {
      top: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-top-color: #1f2937;
    }
    .tooltip-popup.arrow-top::after {
      bottom: 100%;
      left: 50%;
      transform: translateX(-50%);
      border-bottom-color: #1f2937;
    }
    .tooltip-popup.arrow-left::after {
      top: 50%;
      right: 100%;
      transform: translateY(-50%);
      border-right-color: #1f2937;
    }
    .tooltip-popup.arrow-right::after {
      top: 50%;
      left: 100%;
      transform: translateY(-50%);
      border-left-color: #1f2937;
    }
  </style>
</head>
<body>
  <!-- Password Protection Overlay -->
    <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>

      <div class="course-description">
        <h3>Course Modules</h3>
        <ul class="module-list">
          <li><strong>Module 0:</strong> Languages & Platforms ‚Äî Python, Stata, R setup; IDEs (RStudio, VS Code, Jupyter)</li>
          <li><strong>Module 1:</strong> Getting Started ‚Äî Installation, basic syntax, packages</li>
          <li><strong>Module 2:</strong> Data Harnessing ‚Äî File import, APIs, web scraping</li>
          <li><strong>Module 3:</strong> Data Exploration ‚Äî Inspection, summary statistics, visualization</li>
          <li><strong>Module 4:</strong> Data Cleaning ‚Äî Data quality, transformation, validation</li>
          <li><strong>Module 5:</strong> Data Analysis ‚Äî Statistical analysis, simulation, experimental design</li>
          <li><strong>Module 6:</strong> Causal Inference ‚Äî Matching, DiD, RDD, IV, Synthetic Control</li>
          <li><strong>Module 7:</strong> Estimation Methods ‚Äî Standard errors, panel data, MLE/GMM</li>
          <li><strong>Module 8:</strong> Replicability ‚Äî Project organization, documentation, replication packages</li>
          <li><strong>Module 9:</strong> Git & GitHub ‚Äî Version control, collaboration, branching</li>
          <li><strong>Module 10:</strong> History of NLP ‚Äî From ELIZA to Transformers</li>
          <li><strong>Module 11:</strong> Machine Learning ‚Äî Prediction, regularization, neural networks</li>
          <li><strong>Module 12:</strong> Large Language Models ‚Äî How LLMs work, prompting, APIs</li>
        </ul>
      </div>

      <div class="access-note">
        This course is currently open to <strong>students at Sciences Po</strong>. If you are not a Sciences Po student but would like access, please <a href="mailto:giulia.caprini@sciencespo.fr">email me</a> to request an invite token.
      </div>

      <div class="password-form">
        <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
        <button id="password-submit">Access Course</button>
        <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
      </div>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li><a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a></li>
          <li><a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a></li>
          <li class="active"><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <div class="module-header">
          <h1>12 &nbsp;Large Language Models</h1>
          <div class="module-meta">
            <span>~8 hours</span>
            <span>How LLMs Work</span>
            <span>Conceptual + Practical</span>
          </div>
        </div>

        <div class="learning-objectives">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Understand how LLMs are trained and how they generate text</li>
            <li>Grasp the Transformer architecture conceptually</li>
            <li>Learn about RLHF and alignment</li>
            <li>Use LLMs effectively through APIs and prompting</li>
            <li>Understand limitations and risks</li>
          </ul>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#what-are-llms">12.1 What Are LLMs?</a></li>
            <li><a href="#how-they-work">12.2 How LLMs Work</a></li>
            <li><a href="#training">12.3 Training LLMs</a></li>
            <li><a href="#rlhf">12.4 RLHF and Alignment</a></li>
            <li><a href="#prompting">12.5 Prompt Engineering</a></li>
            <li><a href="#using-apis">12.6 Using LLM APIs</a></li>
            <li><a href="#limitations">12.7 Limitations and Risks</a></li>
            <li><a href="#research">12.8 LLMs in Research</a></li>
          </ul>
        </div>

        <h2 id="what-are-llms">12.1 What Are LLMs?</h2>

        <p>
          <strong>Large Language Models</strong> (LLMs) are neural networks trained to predict the next word in a sequence. Despite this simple objective, scaling up has led to emergent abilities: reasoning, coding, translation, and more.
        </p>

        <div class="info-box note">
          <div class="info-box-title">The Key Insight</div>
          <p>
            LLMs are "just" predicting the next word. But to predict well, they must learn grammar, facts, reasoning patterns, and even social conventions--all compressed into billions of parameters.
          </p>
        </div>

        <h3>The LLM Landscape</h3>

        <table>
          <thead>
            <tr>
              <th>Model Family</th>
              <th>Organization</th>
              <th>Access</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT-4, GPT-4o</td>
              <td>OpenAI</td>
              <td>API, ChatGPT</td>
            </tr>
            <tr>
              <td>Claude 3, 3.5</td>
              <td>Anthropic</td>
              <td>API, claude.ai</td>
            </tr>
            <tr>
              <td>Gemini</td>
              <td>Google</td>
              <td>API, Gemini app</td>
            </tr>
            <tr>
              <td>LLaMA 2, 3</td>
              <td>Meta</td>
              <td>Open weights</td>
            </tr>
            <tr>
              <td>Mistral, Mixtral</td>
              <td>Mistral AI</td>
              <td>Open weights, API</td>
            </tr>
          </tbody>
        </table>

        <h2 id="how-they-work">12.2 How LLMs Work</h2>

        <h3>Tokenization</h3>
        <p>
          Text is split into <strong>tokens</strong>--subword units. "Tokenization" might become ["Token", "ization"]. This handles rare words by breaking them into common pieces.
        </p>

        <div class="code-tabs" data-runnable="llm-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Tokenization Example</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Example: How text becomes tokens</span>
<span class="code-tooltip" data-tip="The raw text input that will be processed by the LLM. Before the model can understand it, it must be broken into smaller pieces called tokens.">text = <span class="code-string">"Hello, how are you doing today?"</span></span>

<span class="code-comment"># Tokenized (roughly):</span>
<span class="code-tooltip" data-tip="Tokens are pieces of text - sometimes words, sometimes word parts. Notice spaces are often attached to tokens. Punctuation gets its own tokens.">tokens = [<span class="code-string">"Hello"</span>, <span class="code-string">","</span>, <span class="code-string">" how"</span>, <span class="code-string">" are"</span>, <span class="code-string">" you"</span>, <span class="code-string">" doing"</span>, <span class="code-string">" today"</span>, <span class="code-string">"?"</span>]</span>

<span class="code-comment"># Each token maps to an integer ID</span>
<span class="code-tooltip" data-tip="The model's vocabulary assigns a unique number to each token. The model actually works with these numbers, not the text. Each model has its own vocabulary.">token_ids = [<span class="code-number">15496</span>, <span class="code-number">11</span>, <span class="code-number">703</span>, <span class="code-number">527</span>, <span class="code-number">499</span>, <span class="code-number">3815</span>, <span class="code-number">3432</span>, <span class="code-number">30</span>]</span>

<span class="code-comment"># Models work with token IDs, not text</span>
<span class="code-comment"># <span class="code-tooltip" data-tip="A rough rule of thumb for estimating token counts. 'Hello world' is 2 words but might be 2-3 tokens depending on the tokenizer.">Typical: 1 word ‚âà 1.3 tokens (or ~4 characters per token)</span></span></code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <!-- Output simulation for llm-1 -->
        <div class="code-output-container" data-for="llm-1" style="display:none;">
          <div class="code-output" data-lang="python">
            <div class="output-header">Python Output</div>
            <pre class="output-content">Input text: "Hello, how are you doing today?"

Tokenization result (GPT-4 tokenizer):
  Token 0: "Hello"    -> ID: 15496
  Token 1: ","        -> ID: 11
  Token 2: " how"     -> ID: 703
  Token 3: " are"     -> ID: 527
  Token 4: " you"     -> ID: 499
  Token 5: " doing"   -> ID: 3815
  Token 6: " today"   -> ID: 3432
  Token 7: "?"        -> ID: 30

Total tokens: 8
Words in original: 6
Ratio: 1.33 tokens per word

Note: Spaces are typically attached to the following word.
Punctuation is usually a separate token.</pre>
          </div>
        </div>

        <h3>The Transformer Architecture</h3>
        <p>
          LLMs use the <strong>Transformer</strong> architecture (Vaswani et al., 2017). The key innovation is <strong>self-attention</strong>, which lets each token "look at" all other tokens when computing its representation.
        </p>

        <div class="info-box note">
          <div class="info-box-title">Self-Attention Intuition</div>
          <p>
            Consider: "The cat sat on the mat because <strong>it</strong> was tired."
          </p>
          <p>
            To predict what comes after "it," the model needs to know that "it" refers to "cat." Self-attention computes a weighted average of all previous tokens, with weights learned to capture such relationships.
          </p>
        </div>

        <h3>Next-Token Prediction</h3>
        <p>
          Given tokens [t1, t2, ..., tn], the model outputs a probability distribution over the vocabulary for t(n+1). During generation, it samples from this distribution and repeats.
        </p>

        <div class="code-tabs" data-runnable="llm-2">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Generation Process</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Simplified generation loop</span>
<span class="code-tooltip" data-tip="The user's input text. The model will continue from where this prompt ends, predicting what comes next.">prompt = <span class="code-string">"The capital of France is"</span></span>
<span class="code-tooltip" data-tip="Converts text to token IDs. The model only understands numbers, so we must translate text first.">tokens = tokenize(prompt)</span>  <span class="code-comment"># [464, 3361, 315, 9822, 374]</span>

<span class="code-tooltip" data-tip="Generation is a loop: predict one token, add it, repeat. max_tokens limits how long the response can be."><span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(max_tokens):</span>
    <span class="code-comment"># Model outputs probability for each vocabulary word</span>
    <span class="code-tooltip" data-tip="The model outputs a probability for EVERY word in its vocabulary. High probability = more likely to be the next word.">probs = model(tokens)</span>  <span class="code-comment"># Shape: [vocab_size]</span>

    <span class="code-comment"># Sample next token (or take argmax for greedy)</span>
    <span class="code-tooltip" data-tip="Chooses the next token based on probabilities. temperature controls randomness: 0 = always pick most likely, higher = more random/creative.">next_token = sample(probs, temperature=<span class="code-number">0.7</span>)</span>

    <span class="code-comment"># Append and continue</span>
    <span class="code-tooltip" data-tip="Add the chosen token to the sequence. Now the model will use ALL previous tokens (including this one) to predict the next.">tokens.append(next_token)</span>

    <span class="code-tooltip" data-tip="Stop when the model generates a special 'end of text' token, or when we hit max_tokens."><span class="code-keyword">if</span> next_token == END_TOKEN:
        <span class="code-keyword">break</span></span>

<span class="code-comment"># Detokenize back to text</span>
<span class="code-tooltip" data-tip="Converts token IDs back to readable text. The reverse of tokenization - numbers become words again.">output = detokenize(tokens)</span>
<span class="code-comment"># "The capital of France is Paris."</span></code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <!-- Output simulation for llm-2 -->
        <div class="code-output-container" data-for="llm-2" style="display:none;">
          <div class="code-output" data-lang="python">
            <div class="output-header">Python Output</div>
            <pre class="output-content">Prompt: "The capital of France is"
Tokenized: [464, 3361, 315, 9822, 374]

Generation step 1:
  Top 5 predictions: Paris (0.89), Lyon (0.03), a (0.02), known (0.01), the (0.01)
  Sampled token: "Paris" (ID: 6342)

Generation step 2:
  Top 5 predictions: . (0.72), , (0.15), and (0.05), which (0.03), - (0.02)
  Sampled token: "." (ID: 13)

Generation step 3:
  Top 5 predictions: [END] (0.45), It (0.18), Paris (0.12), The (0.10), This (0.08)
  Sampled token: [END] (ID: 50256)

Generation complete!

Final output: "The capital of France is Paris."
Total tokens generated: 2
Temperature used: 0.7</pre>
          </div>
        </div>

        <h3>Temperature and Sampling</h3>
        <ul>
          <li><strong>Temperature = 0:</strong> Always pick the most likely token (deterministic)</li>
          <li><strong>Temperature = 1:</strong> Sample according to model probabilities</li>
          <li><strong>Temperature > 1:</strong> More random/creative</li>
          <li><strong>Temperature < 1:</strong> More focused/deterministic</li>
        </ul>

        <h2 id="training">12.3 Training LLMs</h2>

        <h3>Pre-training</h3>
        <p>
          LLMs are trained on massive text corpora--often trillions of tokens from the web, books, code, and more. The objective is simple: predict the next token. This is called <strong>self-supervised learning</strong> because no human labels are needed.
        </p>

        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Parameters</th>
              <th>Training Tokens</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT-2</td>
              <td>1.5B</td>
              <td>~40B</td>
            </tr>
            <tr>
              <td>GPT-3</td>
              <td>175B</td>
              <td>~300B</td>
            </tr>
            <tr>
              <td>LLaMA 2</td>
              <td>70B</td>
              <td>2T</td>
            </tr>
            <tr>
              <td>GPT-4</td>
              <td>~1.8T (rumored)</td>
              <td>~13T (rumored)</td>
            </tr>
          </tbody>
        </table>

        <h3>Scaling Laws</h3>
        <p>
          Kaplan et al. (2020) and Hoffmann et al. (2022) showed that performance scales predictably with compute, data, and parameters. More of each leads to better models. This motivated the race to train ever-larger models.
        </p>

        <h2 id="rlhf">12.4 RLHF and Alignment</h2>

        <p>
          Pre-trained models are good at predicting text but not at being helpful or safe. <strong>RLHF</strong> (Reinforcement Learning from Human Feedback) fine-tunes models to behave as intended.
        </p>

        <h3>The RLHF Process</h3>
        <ol>
          <li><strong>Supervised Fine-Tuning (SFT):</strong> Train on high-quality examples of helpful responses</li>
          <li><strong>Reward Model:</strong> Train a model to predict which responses humans prefer</li>
          <li><strong>RL Fine-Tuning:</strong> Optimize the LLM to maximize the reward model's score</li>
        </ol>

        <div class="info-box note">
          <div class="info-box-title">Why RLHF Matters</div>
          <p>
            Without RLHF, models might:
          </p>
          <ul>
            <li>Be unhelpfully verbose or terse</li>
            <li>Follow harmful instructions</li>
            <li>Hallucinate confidently</li>
            <li>Be inconsistent in style</li>
          </ul>
          <p>RLHF teaches models to be helpful, harmless, and honest--the "HHH" criteria.</p>
        </div>

        <h3>Constitutional AI</h3>
        <p>
          Anthropic's approach uses a set of principles (a "constitution") to guide AI behavior. The model critiques and revises its own outputs according to these principles, reducing reliance on human labeling.
        </p>

        <h2 id="prompting">12.5 Prompt Engineering</h2>

        <p>
          <strong>Prompt engineering</strong> is the art of crafting inputs that elicit the best outputs from LLMs. Small changes in wording can dramatically affect results.
        </p>

        <h3>Key Techniques</h3>

        <table>
          <thead>
            <tr>
              <th>Technique</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Few-shot</strong></td>
              <td>Provide examples of input-output pairs</td>
            </tr>
            <tr>
              <td><strong>Chain-of-thought</strong></td>
              <td>Ask model to show reasoning steps</td>
            </tr>
            <tr>
              <td><strong>Role prompting</strong></td>
              <td>"You are an expert economist..."</td>
            </tr>
            <tr>
              <td><strong>Structured output</strong></td>
              <td>Request JSON, markdown, or specific format</td>
            </tr>
          </tbody>
        </table>

        <div class="code-tabs" data-runnable="llm-3">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Prompt Examples</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Zero-shot</span>
<span class="code-tooltip" data-tip="Zero-shot means no examples provided - just the task. Works for simple, well-understood tasks. The model relies entirely on its training.">prompt = <span class="code-string">"Translate to French: Hello, how are you?"</span></span>

<span class="code-comment"># Few-shot</span>
<span class="code-tooltip" data-tip="Few-shot learning: provide examples of input-output pairs before the actual task. The model learns the pattern from your examples and applies it.">prompt = <span class="code-string">"""Translate to French:
English: Hello
French: Bonjour

English: Thank you
French: Merci

English: How are you?
French:"""</span></span>

<span class="code-comment"># Chain-of-thought</span>
<span class="code-tooltip" data-tip="Chain-of-thought prompting asks the model to show its reasoning step by step. This dramatically improves accuracy on math and logic problems.">prompt = <span class="code-string">"""Solve step by step:
If a train travels 120 miles in 2 hours, then continues
for 3 more hours at the same speed, what is the total distance?

Let's think step by step:"""</span></span>

<span class="code-comment"># Role + structured output</span>
<span class="code-tooltip" data-tip="Role prompting ('You are a...') sets context and expertise level. Requesting structured output (JSON) makes responses easier to parse programmatically.">prompt = <span class="code-string">"""You are a data analyst. Given this data,
identify the top 3 trends. Format as JSON:
{"trends": ["trend1", "trend2", "trend3"]}"""</span></span></code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <!-- Output simulation for llm-3 -->
        <div class="code-output-container" data-for="llm-3" style="display:none;">
          <div class="code-output" data-lang="python">
            <div class="output-header">Python Output</div>
            <pre class="output-content">=== Zero-shot Response ===
Prompt: "Translate to French: Hello, how are you?"
Response: "Bonjour, comment allez-vous?"

=== Few-shot Response ===
Prompt: [examples + "How are you?"]
Response: "Comment allez-vous?"

Note: Few-shot learned the formal style from examples!

=== Chain-of-thought Response ===
Prompt: "Solve step by step: train problem..."
Response:
"Let's think step by step:
1. First, find the speed: 120 miles / 2 hours = 60 mph
2. The train continues for 3 more hours at 60 mph
3. Additional distance: 60 mph x 3 hours = 180 miles
4. Total distance: 120 + 180 = 300 miles

The total distance is 300 miles."

=== Role + JSON Response ===
Prompt: "You are a data analyst..."
Response:
{
  "trends": [
    "Revenue increased 23% year-over-year",
    "Customer churn decreased in Q4",
    "Mobile users now exceed desktop users"
  ]
}</pre>
          </div>
        </div>

        <h2 id="using-apis">12.6 Using LLM APIs</h2>

        <div class="code-tabs" data-runnable="llm-4">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python API Usage</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Using OpenAI API</span>
<span class="code-keyword">from</span> <span class="code-tooltip" data-tip="The official OpenAI Python library. Install with: pip install openai">openai</span> <span class="code-keyword">import</span> OpenAI

<span class="code-tooltip" data-tip="Creates an API client. It automatically reads your API key from the OPENAI_API_KEY environment variable - never hardcode keys in your code!">client = OpenAI()</span>  <span class="code-comment"># Uses OPENAI_API_KEY env var</span>

<span class="code-tooltip" data-tip="Sends a request to the OpenAI API. chat.completions is for conversational models like GPT-4. The create() method makes the actual API call.">response = client.chat.completions.create(</span>
    <span class="code-tooltip" data-tip="Which model to use. Different models have different capabilities and costs. gpt-4 is more capable but slower and pricier than gpt-3.5-turbo.">model=<span class="code-string">"gpt-4"</span>,</span>
    <span class="code-tooltip" data-tip="Conversation history as a list. 'system' sets behavior, 'user' is the human, 'assistant' is the AI. The model sees all messages to maintain context.">messages=[
        {<span class="code-string">"role"</span>: <span class="code-string">"system"</span>, <span class="code-string">"content"</span>: <span class="code-string">"You are a helpful assistant."</span>},
        {<span class="code-string">"role"</span>: <span class="code-string">"user"</span>, <span class="code-string">"content"</span>: <span class="code-string">"Explain regression discontinuity."</span>}
    ],</span>
    <span class="code-tooltip" data-tip="Controls randomness: 0 = deterministic, 1 = creative. For factual tasks use lower values; for creative writing use higher.">temperature=<span class="code-number">0.7</span>,</span>
    <span class="code-tooltip" data-tip="Maximum tokens in the response. Limits cost and response length. 500 tokens is roughly 375 words.">max_tokens=<span class="code-number">500</span></span>
)

<span class="code-tooltip" data-tip="The response contains multiple possible completions (choices). We take the first one and extract the actual text content."><span class="code-function">print</span>(response.choices[<span class="code-number">0</span>].message.content)</span>

<span class="code-comment"># Using Anthropic API</span>
<span class="code-keyword">from</span> <span class="code-tooltip" data-tip="The official Anthropic Python library for Claude models. Install with: pip install anthropic">anthropic</span> <span class="code-keyword">import</span> Anthropic

<span class="code-tooltip" data-tip="Creates Anthropic client. Reads from ANTHROPIC_API_KEY environment variable automatically.">client = Anthropic()</span>

<span class="code-tooltip" data-tip="Sends request to Claude. Anthropic uses 'messages' instead of 'chat.completions'. The API structure is similar but with some differences.">message = client.messages.create(</span>
    <span class="code-tooltip" data-tip="Claude model version. The date in the name indicates the model snapshot. Newer versions may have improved capabilities.">model=<span class="code-string">"claude-3-sonnet-20240229"</span>,</span>
    <span class="code-tooltip" data-tip="Maximum response length. Claude's max_tokens is required (not optional like OpenAI's).">max_tokens=<span class="code-number">1024</span>,</span>
    messages=[
        {<span class="code-string">"role"</span>: <span class="code-string">"user"</span>, <span class="code-string">"content"</span>: <span class="code-string">"Explain difference-in-differences."</span>}
    ]
)

<span class="code-tooltip" data-tip="Claude's response structure differs from OpenAI's. The content is a list, so we access the first item's text."><span class="code-function">print</span>(message.content[<span class="code-number">0</span>].text)</span></code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
        </div>

        <!-- Output simulation for llm-4 -->
        <div class="code-output-container" data-for="llm-4" style="display:none;">
          <div class="code-output" data-lang="python">
            <div class="output-header">Python Output</div>
            <pre class="output-content">=== OpenAI API Response ===
Model: gpt-4
Tokens used: 312 (prompt: 28, completion: 284)
Latency: 2.3s

Response:
Regression Discontinuity Design (RDD) is a quasi-experimental
method used when treatment is assigned based on a cutoff. For
example, if students scoring above 70% get a scholarship, we
can compare students just above and just below 70% to estimate
the scholarship's causal effect.

The key assumption is that students near the cutoff are similar
in all ways except treatment status. This creates a "local"
randomized experiment around the threshold.

=== Anthropic API Response ===
Model: claude-3-sonnet-20240229
Input tokens: 15
Output tokens: 256
Latency: 1.8s

Response:
Difference-in-Differences (DiD) is a causal inference method
that compares changes over time between a treatment group and
a control group. The key idea is that the control group shows
what would have happened to the treatment group absent the
intervention.

The parallel trends assumption is crucial: both groups must
have followed similar trajectories before treatment...</pre>
          </div>
        </div>

        <h2 id="limitations">12.7 Limitations and Risks</h2>

        <h3>Known Limitations</h3>
        <ul>
          <li><strong>Hallucinations:</strong> LLMs confidently generate false information</li>
          <li><strong>Knowledge cutoff:</strong> Training data has a date limit</li>
          <li><strong>Context limits:</strong> Can only process limited text at once</li>
          <li><strong>Math/reasoning:</strong> Struggles with complex calculations</li>
          <li><strong>No real-time info:</strong> Can't access the internet (unless given tools)</li>
        </ul>

        <div class="info-box warning">
          <div class="info-box-title">Critical for Research</div>
          <p>
            <strong>Never trust LLM outputs without verification.</strong> They can generate plausible-sounding but false citations, statistics, and claims. Always verify facts from primary sources.
          </p>
        </div>

        <h3>Ethical Considerations</h3>
        <ul>
          <li><strong>Bias:</strong> Models reflect biases in training data</li>
          <li><strong>Privacy:</strong> May memorize and reveal private information</li>
          <li><strong>Misuse:</strong> Can be used for spam, disinformation, cheating</li>
          <li><strong>Environmental:</strong> Training requires enormous compute/energy</li>
        </ul>

        <h2 id="research">12.8 LLMs in Research</h2>

        <p>
          LLMs are increasingly used as research tools--for coding, literature review, data analysis, and writing. Use them wisely.
        </p>

        <h3>Appropriate Uses</h3>
        <ul>
          <li>Debugging code</li>
          <li>Explaining concepts</li>
          <li>Brainstorming research questions</li>
          <li>Drafting and editing prose</li>
          <li>Translating between programming languages</li>
        </ul>

        <h3>Inappropriate Uses</h3>
        <ul>
          <li>Generating citations (they hallucinate!)</li>
          <li>Performing calculations you can't verify</li>
          <li>Replacing critical thinking</li>
          <li>Submitting AI-generated work as your own (check your institution's policy)</li>
        </ul>

        <div class="citation">
          <div class="citation-title">Further Reading</div>
          <ul>
            <li>Vaswani, A., et al. (2017). <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>.</li>
            <li>Ouyang, L., et al. (2022). <a href="https://arxiv.org/abs/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>. (RLHF paper)</li>
            <li>Anthropic. (2023). <a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional AI</a>.</li>
          </ul>
        </div>

        <div class="nav-footer">
          <a href="11-machine-learning.html" class="nav-link prev">Module 11: Machine Learning</a>
          <span class="nav-link next disabled"></span>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about Python, Stata, R, causal inference methods, or any of the course material. How can I assist you today?</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    let tooltipEl = null;
    let currentTarget = null;
    let hideTimeout = null;

    function createTooltip() {
      if (tooltipEl) return tooltipEl;
      tooltipEl = document.createElement('div');
      tooltipEl.className = 'tooltip-popup';
      document.body.appendChild(tooltipEl);
      return tooltipEl;
    }

    function positionTooltip(target) {
      const tooltip = createTooltip();
      const tipText = target.getAttribute('data-tip');
      if (!tipText) return;

      tooltip.textContent = tipText;
      tooltip.className = 'tooltip-popup';

      const targetRect = target.getBoundingClientRect();
      let container = target.closest('pre') || target.closest('.tab-content') || target.closest('.code-tabs');
      let containerRect = container ? container.getBoundingClientRect() : {
        left: 0, right: window.innerWidth, top: 0, bottom: window.innerHeight
      };

      const viewportWidth = window.innerWidth;
      const viewportHeight = window.innerHeight;
      const padding = 10;

      tooltip.style.visibility = 'hidden';
      tooltip.style.display = 'block';
      tooltip.classList.add('visible');

      const tooltipRect = tooltip.getBoundingClientRect();
      const tooltipWidth = tooltipRect.width;
      const tooltipHeight = tooltipRect.height;

      let left = targetRect.left + (targetRect.width / 2) - (tooltipWidth / 2);
      let top = targetRect.top - tooltipHeight - 8;
      let arrowClass = 'arrow-bottom';

      if (top < padding) {
        top = targetRect.bottom + 8;
        arrowClass = 'arrow-top';
      }

      if (top + tooltipHeight > viewportHeight - padding) {
        top = targetRect.top - tooltipHeight - 8;
        arrowClass = 'arrow-bottom';
      }

      if (left < padding) left = padding;
      if (left + tooltipWidth > viewportWidth - padding) left = viewportWidth - tooltipWidth - padding;

      if (container) {
        const minLeft = Math.max(padding, containerRect.left);
        const maxRight = Math.min(viewportWidth - padding, containerRect.right);
        if (left < minLeft) left = minLeft;
        if (left + tooltipWidth > maxRight) left = maxRight - tooltipWidth;
      }

      tooltip.style.left = left + 'px';
      tooltip.style.top = top + 'px';
      tooltip.style.visibility = 'visible';
      tooltip.classList.add(arrowClass);
    }

    function showTooltip(target) {
      if (hideTimeout) { clearTimeout(hideTimeout); hideTimeout = null; }
      currentTarget = target;
      positionTooltip(target);
    }

    function hideTooltip() {
      hideTimeout = setTimeout(function() {
        if (tooltipEl) tooltipEl.classList.remove('visible');
        currentTarget = null;
      }, 100);
    }

    document.addEventListener('mouseenter', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) showTooltip(e.target);
    }, true);

    document.addEventListener('mouseleave', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) hideTooltip();
    }, true);

    document.addEventListener('scroll', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget);
    }, true);

    window.addEventListener('resize', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget);
    });
  })();
  </script>
</body>
</html>
