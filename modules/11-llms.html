<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Module 11: Large Language Models | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content {
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
    }
  </style>
</head>
<body>
  <!-- Password Protection Overlay -->
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <p style="font-size: 0.9rem; color: #666; margin-bottom: 1.5rem;">Please enter the course password to access the materials.</p>
      <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
      <button id="password-submit">Access Course</button>
      <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html">Welcome</a></li>
          <li><a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a></li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li><a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a></li>
          <li><a href="03-data-cleaning.html"><span class="module-number">3</span> Data Cleaning</a></li>
          <li><a href="04-data-analysis.html"><span class="module-number">4</span> Data Analysis</a></li>
          <li><a href="05-causal-inference.html"><span class="module-number">5</span> Causal Inference</a></li>
          <li><a href="06-estimation.html"><span class="module-number">6</span> Estimation Methods</a></li>
          <li><a href="07-replicability.html"><span class="module-number">7</span> Replicability</a></li>
          <li><a href="08-github.html"><span class="module-number">8</span> Git & GitHub</a></li>
          <li><a href="09-nlp-history.html"><span class="module-number">9</span> History of NLP</a></li>
          <li><a href="10-machine-learning.html"><span class="module-number">10</span> Machine Learning</a></li>
          <li><a href="11-llms.html" class="active"><span class="module-number">11</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <div class="module-header">
          <h1>11 &nbsp;Large Language Models</h1>
          <div class="module-meta">
            <span>~8 hours</span>
            <span>How LLMs Work</span>
            <span>Conceptual + Practical</span>
          </div>
        </div>

        <div class="learning-objectives">
          <h3>Learning Objectives</h3>
          <ul>
            <li>Understand how LLMs are trained and how they generate text</li>
            <li>Grasp the Transformer architecture conceptually</li>
            <li>Learn about RLHF and alignment</li>
            <li>Use LLMs effectively through APIs and prompting</li>
            <li>Understand limitations and risks</li>
          </ul>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#what-are-llms">11.1 What Are LLMs?</a></li>
            <li><a href="#how-they-work">11.2 How LLMs Work</a></li>
            <li><a href="#training">11.3 Training LLMs</a></li>
            <li><a href="#rlhf">11.4 RLHF and Alignment</a></li>
            <li><a href="#prompting">11.5 Prompt Engineering</a></li>
            <li><a href="#using-apis">11.6 Using LLM APIs</a></li>
            <li><a href="#limitations">11.7 Limitations and Risks</a></li>
            <li><a href="#research">11.8 LLMs in Research</a></li>
          </ul>
        </div>

        <h2 id="what-are-llms">11.1 What Are LLMs?</h2>

        <p>
          <strong>Large Language Models</strong> (LLMs) are neural networks trained to predict the next word in a sequence. Despite this simple objective, scaling up has led to emergent abilities: reasoning, coding, translation, and more.
        </p>

        <div class="info-box note">
          <div class="info-box-title">The Key Insight</div>
          <p>
            LLMs are "just" predicting the next word. But to predict well, they must learn grammar, facts, reasoning patterns, and even social conventions--all compressed into billions of parameters.
          </p>
        </div>

        <h3>The LLM Landscape</h3>

        <table>
          <thead>
            <tr>
              <th>Model Family</th>
              <th>Organization</th>
              <th>Access</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT-4, GPT-4o</td>
              <td>OpenAI</td>
              <td>API, ChatGPT</td>
            </tr>
            <tr>
              <td>Claude 3, 3.5</td>
              <td>Anthropic</td>
              <td>API, claude.ai</td>
            </tr>
            <tr>
              <td>Gemini</td>
              <td>Google</td>
              <td>API, Gemini app</td>
            </tr>
            <tr>
              <td>LLaMA 2, 3</td>
              <td>Meta</td>
              <td>Open weights</td>
            </tr>
            <tr>
              <td>Mistral, Mixtral</td>
              <td>Mistral AI</td>
              <td>Open weights, API</td>
            </tr>
          </tbody>
        </table>

        <h2 id="how-they-work">11.2 How LLMs Work</h2>

        <h3>Tokenization</h3>
        <p>
          Text is split into <strong>tokens</strong>--subword units. "Tokenization" might become ["Token", "ization"]. This handles rare words by breaking them into common pieces.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Tokenization Example</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Example: How text becomes tokens</span>
text = <span class="code-string">"Hello, how are you doing today?"</span>

<span class="code-comment"># Tokenized (roughly):</span>
tokens = [<span class="code-string">"Hello"</span>, <span class="code-string">","</span>, <span class="code-string">" how"</span>, <span class="code-string">" are"</span>, <span class="code-string">" you"</span>, <span class="code-string">" doing"</span>, <span class="code-string">" today"</span>, <span class="code-string">"?"</span>]

<span class="code-comment"># Each token maps to an integer ID</span>
token_ids = [<span class="code-number">15496</span>, <span class="code-number">11</span>, <span class="code-number">703</span>, <span class="code-number">527</span>, <span class="code-number">499</span>, <span class="code-number">3815</span>, <span class="code-number">3432</span>, <span class="code-number">30</span>]

<span class="code-comment"># Models work with token IDs, not text</span>
<span class="code-comment"># Typical: 1 token is approximately 0.75 words</span></code></pre>
          </div>
        </div>

        <h3>The Transformer Architecture</h3>
        <p>
          LLMs use the <strong>Transformer</strong> architecture (Vaswani et al., 2017). The key innovation is <strong>self-attention</strong>, which lets each token "look at" all other tokens when computing its representation.
        </p>

        <div class="info-box note">
          <div class="info-box-title">Self-Attention Intuition</div>
          <p>
            Consider: "The cat sat on the mat because <strong>it</strong> was tired."
          </p>
          <p>
            To predict what comes after "it," the model needs to know that "it" refers to "cat." Self-attention computes a weighted average of all previous tokens, with weights learned to capture such relationships.
          </p>
        </div>

        <h3>Next-Token Prediction</h3>
        <p>
          Given tokens [t1, t2, ..., tn], the model outputs a probability distribution over the vocabulary for t(n+1). During generation, it samples from this distribution and repeats.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Generation Process</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Simplified generation loop</span>
prompt = <span class="code-string">"The capital of France is"</span>
tokens = tokenize(prompt)  <span class="code-comment"># [464, 3361, 315, 9822, 374]</span>

<span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(max_tokens):
    <span class="code-comment"># Model outputs probability for each vocabulary word</span>
    probs = model(tokens)  <span class="code-comment"># Shape: [vocab_size]</span>

    <span class="code-comment"># Sample next token (or take argmax for greedy)</span>
    next_token = sample(probs, temperature=<span class="code-number">0.7</span>)

    <span class="code-comment"># Append and continue</span>
    tokens.append(next_token)

    <span class="code-keyword">if</span> next_token == END_TOKEN:
        <span class="code-keyword">break</span>

<span class="code-comment"># Detokenize back to text</span>
output = detokenize(tokens)
<span class="code-comment"># "The capital of France is Paris."</span></code></pre>
          </div>
        </div>

        <h3>Temperature and Sampling</h3>
        <ul>
          <li><strong>Temperature = 0:</strong> Always pick the most likely token (deterministic)</li>
          <li><strong>Temperature = 1:</strong> Sample according to model probabilities</li>
          <li><strong>Temperature > 1:</strong> More random/creative</li>
          <li><strong>Temperature < 1:</strong> More focused/deterministic</li>
        </ul>

        <h2 id="training">11.3 Training LLMs</h2>

        <h3>Pre-training</h3>
        <p>
          LLMs are trained on massive text corpora--often trillions of tokens from the web, books, code, and more. The objective is simple: predict the next token. This is called <strong>self-supervised learning</strong> because no human labels are needed.
        </p>

        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Parameters</th>
              <th>Training Tokens</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>GPT-2</td>
              <td>1.5B</td>
              <td>~40B</td>
            </tr>
            <tr>
              <td>GPT-3</td>
              <td>175B</td>
              <td>~300B</td>
            </tr>
            <tr>
              <td>LLaMA 2</td>
              <td>70B</td>
              <td>2T</td>
            </tr>
            <tr>
              <td>GPT-4</td>
              <td>~1.8T (rumored)</td>
              <td>~13T (rumored)</td>
            </tr>
          </tbody>
        </table>

        <h3>Scaling Laws</h3>
        <p>
          Kaplan et al. (2020) and Hoffmann et al. (2022) showed that performance scales predictably with compute, data, and parameters. More of each leads to better models. This motivated the race to train ever-larger models.
        </p>

        <h2 id="rlhf">11.4 RLHF and Alignment</h2>

        <p>
          Pre-trained models are good at predicting text but not at being helpful or safe. <strong>RLHF</strong> (Reinforcement Learning from Human Feedback) fine-tunes models to behave as intended.
        </p>

        <h3>The RLHF Process</h3>
        <ol>
          <li><strong>Supervised Fine-Tuning (SFT):</strong> Train on high-quality examples of helpful responses</li>
          <li><strong>Reward Model:</strong> Train a model to predict which responses humans prefer</li>
          <li><strong>RL Fine-Tuning:</strong> Optimize the LLM to maximize the reward model's score</li>
        </ol>

        <div class="info-box note">
          <div class="info-box-title">Why RLHF Matters</div>
          <p>
            Without RLHF, models might:
          </p>
          <ul>
            <li>Be unhelpfully verbose or terse</li>
            <li>Follow harmful instructions</li>
            <li>Hallucinate confidently</li>
            <li>Be inconsistent in style</li>
          </ul>
          <p>RLHF teaches models to be helpful, harmless, and honest--the "HHH" criteria.</p>
        </div>

        <h3>Constitutional AI</h3>
        <p>
          Anthropic's approach uses a set of principles (a "constitution") to guide AI behavior. The model critiques and revises its own outputs according to these principles, reducing reliance on human labeling.
        </p>

        <h2 id="prompting">11.5 Prompt Engineering</h2>

        <p>
          <strong>Prompt engineering</strong> is the art of crafting inputs that elicit the best outputs from LLMs. Small changes in wording can dramatically affect results.
        </p>

        <h3>Key Techniques</h3>

        <table>
          <thead>
            <tr>
              <th>Technique</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Few-shot</strong></td>
              <td>Provide examples of input-output pairs</td>
            </tr>
            <tr>
              <td><strong>Chain-of-thought</strong></td>
              <td>Ask model to show reasoning steps</td>
            </tr>
            <tr>
              <td><strong>Role prompting</strong></td>
              <td>"You are an expert economist..."</td>
            </tr>
            <tr>
              <td><strong>Structured output</strong></td>
              <td>Request JSON, markdown, or specific format</td>
            </tr>
          </tbody>
        </table>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Prompt Examples</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Zero-shot</span>
prompt = <span class="code-string">"Translate to French: Hello, how are you?"</span>

<span class="code-comment"># Few-shot</span>
prompt = <span class="code-string">"""Translate to French:
English: Hello
French: Bonjour

English: Thank you
French: Merci

English: How are you?
French:"""</span>

<span class="code-comment"># Chain-of-thought</span>
prompt = <span class="code-string">"""Solve step by step:
If a train travels 120 miles in 2 hours, then continues
for 3 more hours at the same speed, what is the total distance?

Let's think step by step:"""</span>

<span class="code-comment"># Role + structured output</span>
prompt = <span class="code-string">"""You are a data analyst. Given this data,
identify the top 3 trends. Format as JSON:
{"trends": ["trend1", "trend2", "trend3"]}"""</span></code></pre>
          </div>
        </div>

        <h2 id="using-apis">11.6 Using LLM APIs</h2>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python API Usage</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Using OpenAI API</span>
<span class="code-keyword">from</span> openai <span class="code-keyword">import</span> OpenAI

client = OpenAI()  <span class="code-comment"># Uses OPENAI_API_KEY env var</span>

response = client.chat.completions.create(
    model=<span class="code-string">"gpt-4"</span>,
    messages=[
        {<span class="code-string">"role"</span>: <span class="code-string">"system"</span>, <span class="code-string">"content"</span>: <span class="code-string">"You are a helpful assistant."</span>},
        {<span class="code-string">"role"</span>: <span class="code-string">"user"</span>, <span class="code-string">"content"</span>: <span class="code-string">"Explain regression discontinuity."</span>}
    ],
    temperature=<span class="code-number">0.7</span>,
    max_tokens=<span class="code-number">500</span>
)

<span class="code-function">print</span>(response.choices[<span class="code-number">0</span>].message.content)

<span class="code-comment"># Using Anthropic API</span>
<span class="code-keyword">from</span> anthropic <span class="code-keyword">import</span> Anthropic

client = Anthropic()

message = client.messages.create(
    model=<span class="code-string">"claude-3-sonnet-20240229"</span>,
    max_tokens=<span class="code-number">1024</span>,
    messages=[
        {<span class="code-string">"role"</span>: <span class="code-string">"user"</span>, <span class="code-string">"content"</span>: <span class="code-string">"Explain difference-in-differences."</span>}
    ]
)

<span class="code-function">print</span>(message.content[<span class="code-number">0</span>].text)</code></pre>
          </div>
        </div>

        <h2 id="limitations">11.7 Limitations and Risks</h2>

        <h3>Known Limitations</h3>
        <ul>
          <li><strong>Hallucinations:</strong> LLMs confidently generate false information</li>
          <li><strong>Knowledge cutoff:</strong> Training data has a date limit</li>
          <li><strong>Context limits:</strong> Can only process limited text at once</li>
          <li><strong>Math/reasoning:</strong> Struggles with complex calculations</li>
          <li><strong>No real-time info:</strong> Can't access the internet (unless given tools)</li>
        </ul>

        <div class="info-box warning">
          <div class="info-box-title">Critical for Research</div>
          <p>
            <strong>Never trust LLM outputs without verification.</strong> They can generate plausible-sounding but false citations, statistics, and claims. Always verify facts from primary sources.
          </p>
        </div>

        <h3>Ethical Considerations</h3>
        <ul>
          <li><strong>Bias:</strong> Models reflect biases in training data</li>
          <li><strong>Privacy:</strong> May memorize and reveal private information</li>
          <li><strong>Misuse:</strong> Can be used for spam, disinformation, cheating</li>
          <li><strong>Environmental:</strong> Training requires enormous compute/energy</li>
        </ul>

        <h2 id="research">11.8 LLMs in Research</h2>

        <p>
          LLMs are increasingly used as research tools--for coding, literature review, data analysis, and writing. Use them wisely.
        </p>

        <h3>Appropriate Uses</h3>
        <ul>
          <li>Debugging code</li>
          <li>Explaining concepts</li>
          <li>Brainstorming research questions</li>
          <li>Drafting and editing prose</li>
          <li>Translating between programming languages</li>
        </ul>

        <h3>Inappropriate Uses</h3>
        <ul>
          <li>Generating citations (they hallucinate!)</li>
          <li>Performing calculations you can't verify</li>
          <li>Replacing critical thinking</li>
          <li>Submitting AI-generated work as your own (check your institution's policy)</li>
        </ul>

        <div class="citation">
          <div class="citation-title">Further Reading</div>
          <ul>
            <li>Vaswani, A., et al. (2017). <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a>.</li>
            <li>Ouyang, L., et al. (2022). <a href="https://arxiv.org/abs/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>. (RLHF paper)</li>
            <li>Anthropic. (2023). <a href="https://arxiv.org/abs/2212.08073" target="_blank">Constitutional AI</a>.</li>
          </ul>
        </div>

        <div class="nav-footer">
          <a href="10-machine-learning.html" class="nav-link prev">Module 10: Machine Learning</a>
          <a href="../resources.html" class="nav-link next">Resources</a>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about Python, Stata, R, causal inference methods, or any of the course material. How can I assist you today?</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>
</body>
</html>
