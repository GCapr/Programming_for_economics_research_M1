<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>11A Regularization | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content { -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; }
    .protected-content pre, .protected-content code, .protected-content .code-block, .protected-content .code-tabs { -webkit-user-select: text; -moz-user-select: text; -ms-user-select: text; user-select: text; }
    .code-tooltip { position: relative; cursor: help; border-bottom: 1px dotted #888; text-decoration: none; }
    .tooltip-popup { position: fixed; background: #1f2937; color: white; padding: 0.5rem 0.75rem; border-radius: 6px; font-size: 0.75rem; white-space: normal; max-width: 300px; opacity: 0; pointer-events: none; transition: opacity 0.15s ease-in-out; z-index: 10000; }
    .tooltip-popup.visible { opacity: 1; }
    .distinction-box { display: grid; grid-template-columns: 1fr 1fr; gap: 1rem; margin: 1.5rem 0; }
    @media (max-width: 768px) { .distinction-box { grid-template-columns: 1fr; } }
    .distinction-card { background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1rem; }
    .distinction-card h4 { margin: 0 0 0.5rem 0; color: #2563eb; }
    .distinction-card ul { margin: 0; padding-left: 1.25rem; }
  </style>
</head>
<body>
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <div class="course-description">
        <h3>Course Modules</h3>
        <ul class="module-list">
          <li><strong>Module 0:</strong> Languages & Platforms ‚Äî Python, Stata, R setup; IDEs (RStudio, VS Code, Jupyter)</li>
          <li><strong>Module 1:</strong> Getting Started ‚Äî Installation, basic syntax, packages</li>
          <li><strong>Module 2:</strong> Data Harnessing ‚Äî File import, APIs, web scraping</li>
          <li><strong>Module 3:</strong> Data Exploration ‚Äî Inspection, summary statistics, visualization</li>
          <li><strong>Module 4:</strong> Data Cleaning ‚Äî Data quality, transformation, validation</li>
          <li><strong>Module 5:</strong> Data Analysis ‚Äî Statistical analysis, simulation, experimental design</li>
          <li><strong>Module 6:</strong> Causal Inference ‚Äî Matching, DiD, RDD, IV, Synthetic Control</li>
          <li><strong>Module 7:</strong> Estimation Methods ‚Äî Standard errors, panel data, MLE/GMM</li>
          <li><strong>Module 8:</strong> Replicability ‚Äî Project organization, documentation, replication packages</li>
          <li><strong>Module 9:</strong> Git & GitHub ‚Äî Version control, collaboration, branching</li>
          <li><strong>Module 10:</strong> History of NLP ‚Äî From ELIZA to Transformers</li>
          <li><strong>Module 11:</strong> Machine Learning ‚Äî Prediction, regularization, neural networks</li>
          <li><strong>Module 12:</strong> Large Language Models ‚Äî How LLMs work, prompting, APIs</li>
        </ul>
      </div>
      <div class="access-note">
        This course is currently open to <strong>students at Sciences Po</strong>. If you are not a Sciences Po student but would like access, please <a href="mailto:giulia.caprini@sciencespo.fr">email me</a> to request an invite token.
      </div>
      <div class="password-form">
        <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
        <button id="password-submit">Access Course</button>
        <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
      </div>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li class="has-subnav">
            <a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a>
            <ul class="sub-nav">
              <li><a href="05a-data-simulation.html">Data Simulation</a></li>
            </ul>
          </li>
          <li class="has-subnav">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
              <li><a href="05b-experiments.html">Experiments</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li class="has-subnav">
            <a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a>
            <ul class="sub-nav">
              <li><a href="10a-text-analysis-today.html">Text Analysis Today</a></li>
            </ul>
          </li>
          <li class="has-subnav active">
            <a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a>
            <ul class="sub-nav">
              <li class="active"><a href="11a-regularization.html">Regularization</a></li>
              <li><a href="11b-trees.html">Tree-Based Methods</a></li>
              <li><a href="11c-neural-networks.html">Neural Networks</a></li>
              <li><a href="11d-causal-ml.html">Causal ML</a></li>
              <li><a href="11e-model-evaluation.html">Model Evaluation</a></li>
            </ul>
          </li>
          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content-wrapper">

        <!-- Breadcrumb -->
        <p class="breadcrumb" style="font-size: 0.9rem; color: #64748b; margin-bottom: 0.5rem;">
          <a href="11-machine-learning.html" style="color: #2563eb; text-decoration: none;">11 Machine Learning</a> &raquo; Regularization
        </p>

        <div class="module-header">
          <h1>11A &nbsp;Regularization</h1>
          <div class="module-meta">
            <span>~2.5 hours</span>
            <span>Ridge, LASSO, Elastic Net, Post-LASSO</span>
          </div>
        </div>

        <!-- Learning Objectives -->
        <div class="info-box" style="background: #f0f9ff; border-left: 4px solid #2563eb; padding: 1.25rem 1.5rem; border-radius: 0 8px 8px 0; margin: 1.5rem 0;">
          <h3 style="margin: 0 0 0.75rem 0; color: #1e40af;">Learning Objectives</h3>
          <ul style="margin: 0; padding-left: 1.25rem; line-height: 1.7;">
            <li>Understand why penalizing model complexity improves predictions</li>
            <li>Implement Ridge, LASSO, and Elastic Net regression</li>
            <li>Use cross-validation to select the optimal penalty parameter</li>
            <li>Apply post-LASSO methods for valid causal inference after variable selection</li>
          </ul>
        </div>

        <!-- ‚ïê‚ïê‚ïê Code Grammar Primer ‚ïê‚ïê‚ïê -->
        <section class="grammar-primer-section">
          <h2 id="code-grammar">Code Grammar</h2>
          <p>Practice the syntax patterns used in this module. In <strong>Build</strong> mode, read the descriptions and figure out the code. In <strong>Read</strong> mode, look at the code and identify what each piece does. Click cards to check your answers.</p>
          <div class="grammar-primer">
            <script type="application/json">
            {
              "exercises": [
                {
                  "instruction": "Import a cross-validated Ridge regression class from scikit-learn",
                  "pattern": "from module import ClassName",
                  "structureNote": "Python's <code>from ... import</code> syntax lets you grab specific classes from a module without importing the entire library. <code>sklearn.linear_model</code> is the submodule containing all linear regression estimators: OLS, Ridge, LASSO, Elastic Net.",
                  "languages": {
                    "python": [
                      {"code": "from", "role": "Keyword", "tip": "Begins a selective import -- lets you grab specific classes from a module without importing the entire library", "color": "mauve"},
                      {"code": " ", "role": "Space", "tip": "Required separator between the keyword and the module path", "color": "text"},
                      {"code": "sklearn.linear_model", "role": "Module path", "tip": "The submodule inside scikit-learn that contains all linear regression estimators: OLS, Ridge, LASSO, Elastic Net", "color": "yellow"},
                      {"code": " ", "role": "Space", "tip": "Required separator", "color": "text"},
                      {"code": "import", "role": "Keyword", "tip": "Specifies which names to bring into the current namespace from the module", "color": "mauve"},
                      {"code": " ", "role": "Space", "tip": "Required separator", "color": "text"},
                      {"code": "RidgeCV", "role": "Class name", "tip": "Ridge regression with built-in cross-validation -- automatically selects the best alpha (penalty strength) from a grid of values", "color": "blue"}
                    ],
                    "r": [
                      {"code": "library", "role": "Function", "tip": "Loads an R package into the current session, making all its functions available", "color": "mauve"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument to library()", "color": "overlay"},
                      {"code": "glmnet", "role": "Package name", "tip": "The standard R package for regularized regression -- provides Ridge, LASSO, and Elastic Net via the alpha parameter", "color": "yellow"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the library() call -- the glmnet package is now loaded", "color": "overlay"}
                    ]
                  }
                },
                {
                  "instruction": "Create a Ridge model with cross-validated penalty selection and fit it to training data",
                  "pattern": "model = ClassName(params).fit(X, y)",
                  "structureNote": "The scikit-learn workflow has two steps: (1) <strong>instantiate</strong> the model with hyperparameters, and (2) <strong>fit</strong> it to data. <code>RidgeCV</code> combines model fitting and penalty selection in one step -- it tries every alpha in the grid using K-fold CV and picks the one with the lowest error.",
                  "languages": {
                    "python": [
                      {"code": "ridge", "role": "Variable", "tip": "Stores the fitted RidgeCV model object -- you can later call .predict() or inspect .alpha_ and .coef_", "color": "pink"},
                      {"code": " = ", "role": "Assignment", "tip": "Binds the fitted model to the variable name", "color": "peach"},
                      {"code": "RidgeCV", "role": "Constructor", "tip": "Creates a Ridge regression estimator with built-in cross-validation for penalty selection", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the hyperparameter arguments", "color": "overlay"},
                      {"code": "alphas", "role": "Keyword arg", "tip": "The name of the parameter that specifies which penalty strengths to try", "color": "sky"},
                      {"code": "=", "role": "Binding", "tip": "Connects the parameter name to its value", "color": "peach"},
                      {"code": "alphas", "role": "Variable", "tip": "An array of alpha values to try, e.g., np.logspace(-4, 4, 50) -- 50 values from 0.0001 to 10000", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates keyword arguments", "color": "overlay"},
                      {"code": "cv", "role": "Keyword arg", "tip": "Specifies the number of cross-validation folds", "color": "sky"},
                      {"code": "=", "role": "Binding", "tip": "Connects the parameter name to its value", "color": "peach"},
                      {"code": "5", "role": "Integer", "tip": "5-fold cross-validation: train on 80%, validate on 20%, repeated 5 times", "color": "green"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the constructor call -- the model object is now configured but not yet fitted", "color": "overlay"}
                    ],
                    "stata": [
                      {"code": "cvlasso", "role": "Command", "tip": "Cross-validated LASSO/Ridge from the lassopack package -- selects the optimal lambda via K-fold CV", "color": "mauve"},
                      {"code": " ", "role": "Space", "tip": "Separator between command and variable list", "color": "text"},
                      {"code": "y", "role": "Dependent var", "tip": "The outcome variable to be predicted", "color": "pink"},
                      {"code": " ", "role": "Space", "tip": "Separator", "color": "text"},
                      {"code": "x1-x50", "role": "Independent vars", "tip": "Stata's variable range notation -- includes all variables from x1 through x50 as predictors", "color": "pink"},
                      {"code": ", ", "role": "Comma", "tip": "In Stata, the comma separates the main command from its options", "color": "overlay"},
                      {"code": "alpha(0)", "role": "Option", "tip": "Sets the elastic net mixing parameter: alpha(0) = pure Ridge (L2 penalty only), alpha(1) = LASSO (L1 only)", "color": "sky"},
                      {"code": " ", "role": "Space", "tip": "Separator between options", "color": "text"},
                      {"code": "lopt", "role": "Option", "tip": "Tells cvlasso to select the lambda that minimizes the cross-validation error (the 'optimal' lambda)", "color": "sky"}
                    ],
                    "r": [
                      {"code": "cv_ridge", "role": "Variable", "tip": "Stores the fitted cv.glmnet object -- contains the optimal lambda, CV error curve, and fitted coefficients", "color": "pink"},
                      {"code": " <- ", "role": "Assignment", "tip": "R's assignment operator -- stores the result on the right into the variable on the left", "color": "peach"},
                      {"code": "cv.glmnet", "role": "Function", "tip": "Fits a regularized regression with K-fold cross-validation and returns the optimal penalty (lambda)", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument list", "color": "overlay"},
                      {"code": "X_train", "role": "Feature matrix", "tip": "The predictor matrix (must be a matrix, not a data frame)", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates positional arguments", "color": "overlay"},
                      {"code": "y_train", "role": "Response vector", "tip": "The outcome variable as a numeric vector", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates arguments", "color": "overlay"},
                      {"code": "alpha", "role": "Parameter", "tip": "The elastic net mixing parameter", "color": "sky"},
                      {"code": " = ", "role": "Binding", "tip": "Connects the parameter name to its value", "color": "peach"},
                      {"code": "0", "role": "Value", "tip": "alpha = 0 means pure Ridge (L2 penalty only); alpha = 1 means LASSO (L1 only)", "color": "green"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the cv.glmnet() call -- the model is fitted and CV is performed in one step", "color": "overlay"}
                    ]
                  }
                },
                {
                  "instruction": "Fit a cross-validated LASSO model that automatically selects the penalty strength",
                  "pattern": "model = LassoCV(cv=K).fit(X, y)",
                  "structureNote": "LASSO (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty that can shrink coefficients to <strong>exactly zero</strong>, performing automatic variable selection. <code>LassoCV</code> tests many penalty values and picks the best via cross-validation. This is the key difference from Ridge, which shrinks but never eliminates variables.",
                  "languages": {
                    "python": [
                      {"code": "lasso", "role": "Variable", "tip": "Stores the fitted LassoCV model -- will contain .alpha_ (best penalty) and .coef_ (with zeros for dropped features)", "color": "pink"},
                      {"code": " = ", "role": "Assignment", "tip": "Binds the fitted model to the variable name", "color": "peach"},
                      {"code": "LassoCV", "role": "Constructor", "tip": "Creates a LASSO estimator with built-in CV for penalty selection -- automatically tests many alpha values", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the hyperparameter arguments", "color": "overlay"},
                      {"code": "cv", "role": "Keyword arg", "tip": "Number of cross-validation folds for selecting the best alpha", "color": "sky"},
                      {"code": "=", "role": "Binding", "tip": "Connects parameter name to value", "color": "peach"},
                      {"code": "5", "role": "Integer", "tip": "5-fold CV -- a common default that balances bias and variance in the CV estimate", "color": "green"},
                      {"code": ", ", "role": "Separator", "tip": "Separates keyword arguments", "color": "overlay"},
                      {"code": "random_state", "role": "Keyword arg", "tip": "Sets the random seed for reproducibility of the CV fold assignments", "color": "sky"},
                      {"code": "=", "role": "Binding", "tip": "Connects parameter name to value", "color": "peach"},
                      {"code": "42", "role": "Integer", "tip": "Any fixed integer ensures the same folds every time you run the code", "color": "green"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the constructor -- model is configured but not yet fitted", "color": "overlay"}
                    ],
                    "stata": [
                      {"code": "cvlasso", "role": "Command", "tip": "Cross-validated LASSO from lassopack -- without alpha() option, defaults to alpha(1) which is LASSO", "color": "mauve"},
                      {"code": " ", "role": "Space", "tip": "Separator between command and variable list", "color": "text"},
                      {"code": "y", "role": "Dependent var", "tip": "The outcome variable", "color": "pink"},
                      {"code": " ", "role": "Space", "tip": "Separator", "color": "text"},
                      {"code": "x1-x50", "role": "Independent vars", "tip": "All 50 predictor variables -- Stata's range notation x1-x50 expands to x1 x2 ... x50", "color": "pink"},
                      {"code": ", ", "role": "Comma", "tip": "Separates the main command from options", "color": "overlay"},
                      {"code": "lopt", "role": "Option", "tip": "Selects the lambda that minimizes cross-validation error -- the 'optimal' penalty strength", "color": "sky"}
                    ],
                    "r": [
                      {"code": "cv_lasso", "role": "Variable", "tip": "Stores the cv.glmnet object -- access $lambda.min for the optimal penalty", "color": "pink"},
                      {"code": " <- ", "role": "Assignment", "tip": "R's assignment arrow", "color": "peach"},
                      {"code": "cv.glmnet", "role": "Function", "tip": "Fits regularized regression with cross-validation", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins argument list", "color": "overlay"},
                      {"code": "X_train", "role": "Feature matrix", "tip": "Predictor matrix -- must be a matrix object in glmnet", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates positional arguments", "color": "overlay"},
                      {"code": "y_train", "role": "Response", "tip": "The outcome vector", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates arguments", "color": "overlay"},
                      {"code": "alpha", "role": "Parameter", "tip": "Elastic net mixing parameter", "color": "sky"},
                      {"code": " = ", "role": "Binding", "tip": "Connects parameter to value", "color": "peach"},
                      {"code": "1", "role": "Value", "tip": "alpha = 1 means pure LASSO (L1 penalty only) -- coefficients can be shrunk to exactly zero", "color": "green"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the call -- cross-validated LASSO is fitted", "color": "overlay"}
                    ]
                  }
                },
                {
                  "instruction": "Create an Elastic Net model that searches over multiple L1/L2 mixing ratios",
                  "pattern": "model = ElasticNetCV(l1_ratio=[...], cv=K)",
                  "structureNote": "Elastic Net combines L1 (LASSO) and L2 (Ridge) penalties. The <code>l1_ratio</code> parameter controls the mix: 0 = pure Ridge, 1 = pure LASSO. By providing a list of ratios, <code>ElasticNetCV</code> searches over both the mixing ratio and the penalty strength simultaneously via cross-validation.",
                  "languages": {
                    "python": [
                      {"code": "enet", "role": "Variable", "tip": "Stores the fitted ElasticNetCV model -- check .l1_ratio_ and .alpha_ to see which mix and penalty won", "color": "pink"},
                      {"code": " = ", "role": "Assignment", "tip": "Binds the model to the variable name", "color": "peach"},
                      {"code": "ElasticNetCV", "role": "Constructor", "tip": "Elastic Net with built-in CV -- searches over both the L1/L2 mixing ratio and the penalty strength", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the hyperparameter arguments", "color": "overlay"},
                      {"code": "l1_ratio", "role": "Keyword arg", "tip": "The mixing parameter between L1 and L2: 0 = Ridge, 1 = LASSO, values in between = Elastic Net", "color": "sky"},
                      {"code": "=", "role": "Binding", "tip": "Connects parameter name to value", "color": "peach"},
                      {"code": "[.1, .5, .7, .9, .95, 1]", "role": "List", "tip": "A list of mixing ratios to try -- CV will find the best combination of l1_ratio and alpha", "color": "green"},
                      {"code": ", ", "role": "Separator", "tip": "Separates keyword arguments", "color": "overlay"},
                      {"code": "cv", "role": "Keyword arg", "tip": "Number of cross-validation folds", "color": "sky"},
                      {"code": "=", "role": "Binding", "tip": "Connects parameter name to value", "color": "peach"},
                      {"code": "5", "role": "Integer", "tip": "5-fold cross-validation", "color": "green"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the constructor -- call .fit(X, y) next to train the model", "color": "overlay"}
                    ],
                    "stata": [
                      {"code": "cvlasso", "role": "Command", "tip": "Cross-validated penalized regression from lassopack", "color": "mauve"},
                      {"code": " ", "role": "Space", "tip": "Separator", "color": "text"},
                      {"code": "y", "role": "Dependent var", "tip": "The outcome variable", "color": "pink"},
                      {"code": " ", "role": "Space", "tip": "Separator", "color": "text"},
                      {"code": "x1-x50", "role": "Independent vars", "tip": "All predictor variables", "color": "pink"},
                      {"code": ", ", "role": "Comma", "tip": "Separates the variable list from options", "color": "overlay"},
                      {"code": "alpha(0.5)", "role": "Option", "tip": "alpha(0.5) = equal mix of L1 and L2 penalties -- Stata uses alpha where Python uses l1_ratio", "color": "sky"},
                      {"code": " ", "role": "Space", "tip": "Separator between options", "color": "text"},
                      {"code": "lopt", "role": "Option", "tip": "Select the lambda that minimizes CV error", "color": "sky"}
                    ],
                    "r": [
                      {"code": "cv_enet", "role": "Variable", "tip": "Stores the cv.glmnet object for Elastic Net", "color": "pink"},
                      {"code": " <- ", "role": "Assignment", "tip": "R's assignment arrow", "color": "peach"},
                      {"code": "cv.glmnet", "role": "Function", "tip": "Same function as Ridge/LASSO -- the alpha parameter controls the L1/L2 mix", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins argument list", "color": "overlay"},
                      {"code": "X_train", "role": "Feature matrix", "tip": "Predictor matrix", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates arguments", "color": "overlay"},
                      {"code": "y_train", "role": "Response", "tip": "Outcome vector", "color": "pink"},
                      {"code": ", ", "role": "Separator", "tip": "Separates arguments", "color": "overlay"},
                      {"code": "alpha", "role": "Parameter", "tip": "The L1/L2 mixing parameter in glmnet", "color": "sky"},
                      {"code": " = ", "role": "Binding", "tip": "Connects parameter to value", "color": "peach"},
                      {"code": "0.5", "role": "Value", "tip": "alpha = 0.5 gives equal weight to L1 and L2 penalties -- the standard Elastic Net", "color": "green"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the call -- to search over multiple alpha values in R, loop over them manually", "color": "overlay"}
                    ]
                  }
                },
                {
                  "instruction": "Standardize features by fitting the scaler on training data, then transform both train and test",
                  "pattern": "scaler = StandardScaler(); X_train = scaler.fit_transform(X_train); X_test = scaler.transform(X_test)",
                  "structureNote": "Feature scaling is <strong>essential</strong> before regularization because the penalty treats all coefficients equally. Without scaling, a feature measured in thousands (income) would be penalized differently from one measured in fractions (percentage). The critical rule: <code>fit_transform</code> on training data only, then <code>transform</code> (not fit_transform!) on test data to prevent data leakage.",
                  "languages": {
                    "python": [
                      {"code": "X_train_sc", "role": "Variable", "tip": "Will hold the standardized training features (mean=0, std=1)", "color": "pink"},
                      {"code": " = ", "role": "Assignment", "tip": "Stores the transformed data", "color": "peach"},
                      {"code": "scaler", "role": "Object", "tip": "A StandardScaler instance created earlier with StandardScaler() -- it remembers the training mean and std", "color": "pink"},
                      {"code": ".", "role": "Dot accessor", "tip": "Accesses the fit_transform method on the scaler object", "color": "overlay"},
                      {"code": "fit_transform", "role": "Method", "tip": "Two steps in one: (1) computes mean and std from this data (fit), (2) applies the transformation (transform). Only use on training data!", "color": "blue"},
                      {"code": "(", "role": "Open paren", "tip": "Begins the argument", "color": "overlay"},
                      {"code": "X_train", "role": "Argument", "tip": "The raw training feature matrix -- the scaler learns its mean and std from this data", "color": "pink"},
                      {"code": ")", "role": "Close paren", "tip": "Ends the call -- returns the standardized matrix", "color": "overlay"}
                    ]
                  }
                }
              ]
            }
            </script>
          </div>
        </section>

        <!-- Table of Contents -->
        <div class="toc" style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1.25rem 1.5rem; margin: 1.5rem 0;">
          <h3 style="margin: 0 0 0.75rem 0; color: #334155;">In This Section</h3>
          <ol style="margin: 0; padding-left: 1.25rem; line-height: 1.8;">
            <li><a href="#intuition">The Idea Behind Regularization</a></li>
            <li><a href="#ridge">Ridge Regression (L2)</a></li>
            <li><a href="#lasso">LASSO (L1)</a></li>
            <li><a href="#elastic-net">Elastic Net</a></li>
            <li><a href="#lambda-selection">Cross-Validated Penalty Selection</a></li>
            <li><a href="#post-lasso">Post-LASSO and Post-Selection Inference</a></li>
            <li><a href="#comparison">Choosing the Right Method</a></li>
            <li><a href="#limitations">Limitations</a></li>
          </ol>
        </div>


        <!-- ============================================================ -->
        <!-- THE IDEA BEHIND REGULARIZATION -->
        <!-- ============================================================ -->
        <h2 id="intuition">The Idea Behind Regularization</h2>

        <p>In standard OLS regression, we estimate coefficients by minimizing the sum of squared residuals: find the &beta; that makes the predicted values as close to the observed values as possible. This works beautifully when we have a modest number of predictors relative to observations. But when the number of predictors is large &mdash; especially when p approaches or exceeds n &mdash; OLS overfits badly. It fits the noise in the training data, not the signal, and its predictions on new data can be wildly inaccurate. You saw this problem in the polynomial example in the main module page.</p>

        <p><strong>Regularization</strong> addresses this by adding a penalty term to the objective function that discourages large coefficients. Instead of simply minimizing residuals, we minimize residuals <em>plus</em> a penalty for model complexity. The penalty forces the model to &ldquo;keep things simple&rdquo; by shrinking coefficients toward zero. Larger coefficients are more expensive, so the model must have a good reason (strong signal in the data) to give any predictor a large coefficient.</p>

        <p>Think of it like a budget constraint in economics. Without regularization, the model can allocate unlimited &ldquo;weight&rdquo; to each predictor &mdash; it spends freely. With regularization, the model has a fixed budget of coefficient magnitude to distribute across all predictors, so it must allocate wisely. Predictors with strong signal receive substantial weight; predictors with weak signal or noise get shrunk toward zero. This is exactly the bias-variance tradeoff at work: regularization introduces some bias (coefficients are systematically shrunk) but dramatically reduces variance (predictions are more stable across samples), and the net result is often a substantial improvement in prediction accuracy.</p>

        <p>The general form of the regularized objective function is:</p>

        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1.25rem; margin: 1.5rem 0; text-align: center; font-size: 1.05rem;">
          <strong>min<sub>&beta;</sub> { ||Y &minus; X&beta;||&sup2; + &lambda; &middot; Penalty(&beta;) }</strong>
        </div>

        <p>The parameter &lambda; (lambda) controls the strength of the penalty. When &lambda; = 0, there is no penalty and we get standard OLS. As &lambda; increases, the penalty becomes stronger and coefficients are shrunk more aggressively toward zero. The three main regularization methods differ in their choice of penalty function, which leads to fundamentally different behavior.</p>

        <!-- Geometric Intuition Diagram -->
        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 12px; padding: 2rem; margin: 1.5rem 0;">
          <h4 style="text-align: center; margin: 0 0 1.5rem 0; color: #1e293b;">Geometric Intuition: Constraint Regions</h4>
          <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; max-width: 600px; margin: 0 auto;">
            <div style="text-align: center;">
              <div style="position: relative; width: 150px; height: 150px; margin: 0 auto;">
                <div style="position: absolute; top: 50%; left: 50%; width: 100px; height: 100px; border: 3px solid #3b82f6; border-radius: 50%; transform: translate(-50%, -50%);"></div>
                <div style="position: absolute; top: 50%; left: 50%; width: 4px; height: 4px; background: #ef4444; border-radius: 50%; transform: translate(-50%, -50%);"></div>
                <div style="position: absolute; top: 5px; right: 5px; width: 8px; height: 8px; background: #059669; border-radius: 50%;"></div>
              </div>
              <p style="margin: 0.75rem 0 0; font-weight: 600; color: #3b82f6;">Ridge (L2)</p>
              <p style="margin: 0.25rem 0 0; font-size: 0.85rem; color: #64748b;">Circle: shrinks all coefficients<br>but none reach exactly zero</p>
            </div>
            <div style="text-align: center;">
              <div style="position: relative; width: 150px; height: 150px; margin: 0 auto;">
                <div style="position: absolute; top: 50%; left: 50%; width: 100px; height: 100px; transform: translate(-50%, -50%) rotate(45deg); border: 3px solid #ef4444;"></div>
                <div style="position: absolute; top: 50%; left: 50%; width: 4px; height: 4px; background: #ef4444; border-radius: 50%; transform: translate(-50%, -50%);"></div>
                <div style="position: absolute; top: 25px; left: 50%; width: 8px; height: 8px; background: #059669; border-radius: 50%; transform: translateX(-50%);"></div>
              </div>
              <p style="margin: 0.75rem 0 0; font-weight: 600; color: #ef4444;">LASSO (L1)</p>
              <p style="margin: 0.25rem 0 0; font-size: 0.85rem; color: #64748b;">Diamond: OLS solution hits corners<br>&rarr; some coefficients = exactly 0</p>
            </div>
          </div>
          <p style="text-align: center; margin: 1.25rem 0 0; font-size: 0.85rem; color: #475569;">
            <span style="color: #059669;">&bull;</span> OLS solution &nbsp;&nbsp;
            <span style="color: #ef4444;">&bull;</span> Origin &nbsp;&nbsp;
            The constrained solution is where the OLS contours first touch the constraint region.
          </p>
        </div>


        <!-- ============================================================ -->
        <!-- RIDGE REGRESSION -->
        <!-- ============================================================ -->
        <h2 id="ridge">Ridge Regression (L2)</h2>

        <p>Ridge regression uses the <strong>L2 penalty</strong> &mdash; the sum of squared coefficients. The objective function is:</p>

        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1.25rem; margin: 1.5rem 0; text-align: center; font-size: 1.05rem;">
          <strong>min<sub>&beta;</sub> { ||Y &minus; X&beta;||&sup2; + &lambda; &sum; &beta;<sub>j</sub>&sup2; }</strong>
        </div>

        <p>The parameter &lambda; controls the penalty strength. When &lambda; = 0, we get OLS. As &lambda; grows, every coefficient is shrunk toward zero &mdash; but crucially, no coefficient is set to <em>exactly</em> zero. Ridge regression keeps all predictors in the model; it just makes their coefficients smaller. Geometrically, the constraint region is a circle (or sphere in higher dimensions). The elliptical OLS contours touch the circle smoothly, so the constrained solution lands on the circle&rsquo;s surface &mdash; where all coordinates are nonzero.</p>

        <p>Ridge regression is especially useful when you have many <strong>correlated predictors</strong>. In this setting, OLS coefficients are unstable: small changes in the data can cause large swings in individual coefficients, even though predictions might be similar. Ridge stabilizes the estimates by spreading the weight more evenly across correlated predictors. Think of it as a form of shrinkage that calms down the wild fluctuations that OLS produces when predictors are highly collinear.</p>

        <p>One important practical detail: because the penalty depends on the magnitude of coefficients, you must <strong>standardize your features</strong> before fitting Ridge (and all other regularized models). If one feature is measured in dollars and another in millions of dollars, the latter will have tiny coefficients simply because of its scale, not because it is unimportant. Standardizing ensures that all features are penalized equally.</p>

        <div class="code-tabs" data-runnable="ml-reg-1">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="Ridge regression with built-in cross-validation. Automatically selects the best alpha (lambda) from a grid of values.">RidgeCV</span>
<span class="code-keyword">from</span> sklearn.model_selection <span class="code-keyword">import</span> train_test_split
<span class="code-keyword">from</span> sklearn.preprocessing <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="Standardizes features to have mean=0 and std=1. Essential before applying regularization so all features are on the same scale.">StandardScaler</span>
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error

<span class="code-comment"># Generate data with 50 features (many irrelevant)</span>
<span class="code-function">np.random.seed</span>(<span class="code-number">42</span>)
n = <span class="code-number">200</span>
X = <span class="code-function">np.random.randn</span>(n, <span class="code-number">50</span>)
y = <span class="code-number">3</span>*X[:, <span class="code-number">0</span>] + <span class="code-number">2</span>*X[:, <span class="code-number">1</span>] - <span class="code-number">1.5</span>*X[:, <span class="code-number">2</span>] + <span class="code-number">0.8</span>*X[:, <span class="code-number">3</span>] + <span class="code-function">np.random.randn</span>(n)*<span class="code-number">0.5</span>

<span class="code-comment"># Split and standardize</span>
X_train, X_test, y_train, y_test = <span class="code-function">train_test_split</span>(X, y, test_size=<span class="code-number">0.2</span>, random_state=<span class="code-number">42</span>)
scaler = <span class="code-function">StandardScaler</span>()
X_train_sc = scaler.<span class="code-tooltip" data-tip="Computes mean and std from training data and transforms it. Always fit_transform on train, then transform (not fit_transform) on test.">fit_transform</span>(X_train)
X_test_sc  = scaler.<span class="code-function">transform</span>(X_test)

<span class="code-comment"># Fit Ridge with cross-validated alpha selection</span>
alphas = <span class="code-tooltip" data-tip="Creates 50 values logarithmically spaced from 10^-4 to 10^4. This wide range ensures we search over very weak to very strong penalties.">np.logspace</span>(-<span class="code-number">4</span>, <span class="code-number">4</span>, <span class="code-number">50</span>)
ridge = <span class="code-function">RidgeCV</span>(alphas=alphas, cv=<span class="code-number">5</span>)
ridge.<span class="code-function">fit</span>(X_train_sc, y_train)

<span class="code-comment"># Evaluate</span>
y_pred = ridge.<span class="code-function">predict</span>(X_test_sc)
rmse = np.<span class="code-function">sqrt</span>(<span class="code-function">mean_squared_error</span>(y_test, y_pred))

<span class="code-function">print</span>(<span class="code-string">f"Best alpha (lambda): {ridge.alpha_:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Test RMSE:           {rmse:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Non-zero coefs:      {np.sum(ridge.coef_ != 0)} out of {len(ridge.coef_)}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"(Ridge never sets coefficients to exactly zero)"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Generate data with 50 features</span>
<span class="code-keyword">clear</span>
<span class="code-keyword">set seed</span> 42
<span class="code-keyword">set obs</span> 200

<span class="code-keyword">forvalues</span> i = 1/50 {
    <span class="code-keyword">gen</span> x`i' = <span class="code-function">rnormal</span>()
}
<span class="code-keyword">gen</span> y = 3*x1 + 2*x2 - 1.5*x3 + 0.8*x4 + <span class="code-function">rnormal</span>()*0.5

<span class="code-comment">* Fit Ridge regression using lasso2 with alpha(0)</span>
<span class="code-comment">* alpha(0) = Ridge, alpha(1) = LASSO</span>
<span class="code-tooltip" data-tip="The lasso2 command from the lassopack package. alpha(0) specifies Ridge regression (pure L2 penalty). cvlasso can be used for cross-validated selection.">lasso2</span> y x1-x50, <span class="code-keyword">alpha</span>(0) <span class="code-keyword">lambda</span>(0.01 0.1 1 10 100)

<span class="code-comment">* Cross-validated Ridge</span>
<span class="code-tooltip" data-tip="Cross-validated LASSO/Ridge. lopt selects the lambda that minimizes CV error. alpha(0) specifies Ridge.">cvlasso</span> y x1-x50, <span class="code-keyword">alpha</span>(0) <span class="code-keyword">lopt</span>

<span class="code-comment">* Predict and evaluate</span>
<span class="code-keyword">predict</span> yhat
<span class="code-keyword">gen</span> resid_sq = (y - yhat)^2
<span class="code-keyword">summarize</span> resid_sq
<span class="code-keyword">display</span> <span class="code-string">"RMSE: "</span> <span class="code-function">sqrt</span>(r(mean))</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="The standard R package for regularized regression. Provides Ridge, LASSO, and Elastic Net via the alpha parameter.">glmnet</span>)

<span class="code-comment"># Generate data with 50 features</span>
<span class="code-function">set.seed</span>(<span class="code-number">42</span>)
n &lt;- <span class="code-number">200</span>
X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * <span class="code-number">50</span>), ncol = <span class="code-number">50</span>)
y &lt;- <span class="code-number">3</span>*X[,<span class="code-number">1</span>] + <span class="code-number">2</span>*X[,<span class="code-number">2</span>] - <span class="code-number">1.5</span>*X[,<span class="code-number">3</span>] + <span class="code-number">0.8</span>*X[,<span class="code-number">4</span>] + <span class="code-function">rnorm</span>(n)*<span class="code-number">0.5</span>

<span class="code-comment"># Train/test split</span>
train_idx &lt;- <span class="code-function">sample</span>(<span class="code-number">1</span>:n, <span class="code-number">160</span>)
X_train &lt;- X[train_idx, ]; y_train &lt;- y[train_idx]
X_test  &lt;- X[-train_idx, ]; y_test  &lt;- y[-train_idx]

<span class="code-comment"># Fit Ridge with cross-validation (alpha = 0 for Ridge)</span>
cv_ridge &lt;- <span class="code-tooltip" data-tip="Fits a regularized regression with K-fold CV. alpha=0 means Ridge (pure L2). Returns optimal lambda and CV error curve.">cv.glmnet</span>(X_train, y_train, alpha = <span class="code-number">0</span>)

<span class="code-comment"># Best lambda and predictions</span>
best_lambda &lt;- cv_ridge$lambda.min
y_pred &lt;- <span class="code-function">predict</span>(cv_ridge, X_test, s = best_lambda)
rmse &lt;- <span class="code-function">sqrt</span>(<span class="code-function">mean</span>((y_test - y_pred)^2))

<span class="code-function">cat</span>(<span class="code-string">"Best lambda:    "</span>, <span class="code-function">round</span>(best_lambda, <span class="code-number">4</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Test RMSE:      "</span>, <span class="code-function">round</span>(rmse, <span class="code-number">4</span>), <span class="code-string">"\n"</span>)
coefs &lt;- <span class="code-function">as.numeric</span>(<span class="code-function">coef</span>(cv_ridge, s = best_lambda))[-<span class="code-number">1</span>]
<span class="code-function">cat</span>(<span class="code-string">"Non-zero coefs:"</span>, <span class="code-function">sum</span>(coefs != <span class="code-number">0</span>), <span class="code-string">"out of"</span>, <span class="code-function">length</span>(coefs), <span class="code-string">"\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="ml-reg-1" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Best alpha (lambda): 0.2848
Test RMSE:           0.5314
Non-zero coefs:      50 out of 50
(Ridge never sets coefficients to exactly zero)</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-1" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Cross-validation results (alpha = 0, Ridge):
  Optimal lambda: 0.3124
  CV MSPE:        0.2856

RMSE: .5347</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-1" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Best lambda:     0.2713
Test RMSE:       0.5289
Non-zero coefs: 50 out of 50</pre></div>
        </div>


        <!-- ============================================================ -->
        <!-- LASSO -->
        <!-- ============================================================ -->
        <h2 id="lasso">LASSO (L1)</h2>

        <p>The LASSO (Least Absolute Shrinkage and Selection Operator), introduced by Tibshirani (1996), uses the <strong>L1 penalty</strong> &mdash; the sum of absolute values of coefficients:</p>

        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1.25rem; margin: 1.5rem 0; text-align: center; font-size: 1.05rem;">
          <strong>min<sub>&beta;</sub> { ||Y &minus; X&beta;||&sup2; + &lambda; &sum; |&beta;<sub>j</sub>| }</strong>
        </div>

        <p>The critical difference from Ridge is that LASSO can set coefficients to <strong>exactly zero</strong>, thereby performing automatic variable selection. This means the LASSO does not just shrink coefficients &mdash; it identifies which predictors are important and discards the rest entirely. This makes the LASSO particularly attractive when you believe that only a subset of your many predictors actually matter (a condition called <em>sparsity</em>).</p>

        <p>Why does the L1 penalty produce exact zeros while L2 does not? The answer lies in the geometry. The L1 constraint region is shaped like a diamond (in 2D) or a cross-polytope (in higher dimensions). This shape has corners that lie on the coordinate axes &mdash; that is, at points where one or more coordinates are exactly zero. The elliptical contours of the OLS objective function tend to first touch the diamond at one of these corners, producing a solution with some coefficients at zero. By contrast, the L2 constraint region (a circle) has no corners, so the contours touch it at a point where all coordinates are generically nonzero.</p>

        <p>The LASSO is your go-to method when you have many potential predictors but suspect that only a few are truly relevant. It is widely used in economics for variable selection in high-dimensional settings &mdash; for example, selecting which of hundreds of control variables to include in a regression.</p>

        <div class="code-tabs" data-runnable="ml-reg-2">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="LASSO with built-in cross-validation. Automatically tests many alpha values and selects the one with lowest CV error.">LassoCV</span>
<span class="code-keyword">from</span> sklearn.preprocessing <span class="code-keyword">import</span> StandardScaler
<span class="code-keyword">from</span> sklearn.model_selection <span class="code-keyword">import</span> train_test_split
<span class="code-keyword">from</span> sklearn.metrics <span class="code-keyword">import</span> mean_squared_error

<span class="code-comment"># Same data as before</span>
<span class="code-function">np.random.seed</span>(<span class="code-number">42</span>)
n = <span class="code-number">200</span>
X = <span class="code-function">np.random.randn</span>(n, <span class="code-number">50</span>)
y = <span class="code-number">3</span>*X[:, <span class="code-number">0</span>] + <span class="code-number">2</span>*X[:, <span class="code-number">1</span>] - <span class="code-number">1.5</span>*X[:, <span class="code-number">2</span>] + <span class="code-number">0.8</span>*X[:, <span class="code-number">3</span>] + <span class="code-function">np.random.randn</span>(n)*<span class="code-number">0.5</span>

X_train, X_test, y_train, y_test = <span class="code-function">train_test_split</span>(X, y, test_size=<span class="code-number">0.2</span>, random_state=<span class="code-number">42</span>)
scaler = <span class="code-function">StandardScaler</span>()
X_train_sc = scaler.<span class="code-function">fit_transform</span>(X_train)
X_test_sc  = scaler.<span class="code-function">transform</span>(X_test)

<span class="code-comment"># Fit LASSO with cross-validation</span>
lasso = <span class="code-function">LassoCV</span>(cv=<span class="code-number">5</span>, random_state=<span class="code-number">42</span>)
lasso.<span class="code-function">fit</span>(X_train_sc, y_train)

<span class="code-comment"># Evaluate</span>
y_pred = lasso.<span class="code-function">predict</span>(X_test_sc)
rmse = np.<span class="code-function">sqrt</span>(<span class="code-function">mean_squared_error</span>(y_test, y_pred))

<span class="code-function">print</span>(<span class="code-string">f"Best alpha (lambda): {lasso.alpha_:.6f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Test RMSE:           {rmse:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Non-zero coefs:      {np.sum(lasso.coef_ != 0)} out of {len(lasso.coef_)}"</span>)

<span class="code-comment"># Show which features were selected</span>
selected = np.<span class="code-function">where</span>(lasso.coef_ != <span class="code-number">0</span>)[<span class="code-number">0</span>]
<span class="code-function">print</span>(<span class="code-string">f"Selected features:   {selected}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"(True features are 0, 1, 2, 3)"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* LASSO with cross-validation</span>
<span class="code-comment">* alpha(1) = LASSO (default)</span>
<span class="code-tooltip" data-tip="Cross-validated LASSO. Without the alpha() option, lasso2 defaults to alpha(1) which is LASSO. lopt selects the lambda minimizing CV error.">cvlasso</span> y x1-x50, <span class="code-keyword">lopt</span>

<span class="code-comment">* Show selected variables</span>
<span class="code-tooltip" data-tip="lasso2 with the optimal lambda found by cvlasso. The output shows which variables have non-zero coefficients.">lasso2</span> y x1-x50, <span class="code-keyword">lambda</span>(<span class="code-function">`e(lopt)'</span>)

<span class="code-comment">* View selected coefficients</span>
<span class="code-keyword">display</span> <span class="code-string">"Number of selected variables: "</span> e(s)

<span class="code-comment">* Predict and evaluate</span>
<span class="code-keyword">predict</span> yhat_lasso
<span class="code-keyword">gen</span> resid_sq = (y - yhat_lasso)^2
<span class="code-keyword">summarize</span> resid_sq
<span class="code-keyword">display</span> <span class="code-string">"RMSE: "</span> <span class="code-function">sqrt</span>(r(mean))</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(glmnet)

<span class="code-comment"># Same data</span>
<span class="code-function">set.seed</span>(<span class="code-number">42</span>)
n &lt;- <span class="code-number">200</span>
X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * <span class="code-number">50</span>), ncol = <span class="code-number">50</span>)
y &lt;- <span class="code-number">3</span>*X[,<span class="code-number">1</span>] + <span class="code-number">2</span>*X[,<span class="code-number">2</span>] - <span class="code-number">1.5</span>*X[,<span class="code-number">3</span>] + <span class="code-number">0.8</span>*X[,<span class="code-number">4</span>] + <span class="code-function">rnorm</span>(n)*<span class="code-number">0.5</span>

train_idx &lt;- <span class="code-function">sample</span>(<span class="code-number">1</span>:n, <span class="code-number">160</span>)
X_train &lt;- X[train_idx, ]; y_train &lt;- y[train_idx]
X_test  &lt;- X[-train_idx, ]; y_test  &lt;- y[-train_idx]

<span class="code-comment"># Fit LASSO with cross-validation (alpha = 1 for LASSO)</span>
cv_lasso &lt;- <span class="code-tooltip" data-tip="cv.glmnet with alpha=1 fits LASSO. It tests a sequence of lambda values and selects the best via cross-validation.">cv.glmnet</span>(X_train, y_train, alpha = <span class="code-number">1</span>)

<span class="code-comment"># Extract results</span>
coef_lasso &lt;- <span class="code-function">coef</span>(cv_lasso, s = <span class="code-string">"lambda.min"</span>)
nonzero &lt;- <span class="code-function">sum</span>(coef_lasso[-<span class="code-number">1</span>] != <span class="code-number">0</span>)  <span class="code-comment"># exclude intercept</span>

y_pred &lt;- <span class="code-function">predict</span>(cv_lasso, X_test, s = <span class="code-string">"lambda.min"</span>)
rmse &lt;- <span class="code-function">sqrt</span>(<span class="code-function">mean</span>((y_test - y_pred)^2))

<span class="code-function">cat</span>(<span class="code-string">"Best lambda:        "</span>, <span class="code-function">round</span>(cv_lasso$lambda.min, <span class="code-number">6</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Test RMSE:          "</span>, <span class="code-function">round</span>(rmse, <span class="code-number">4</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Non-zero coefs:    "</span>, nonzero, <span class="code-string">"out of 50\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"(True features: 1, 2, 3, 4)\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="ml-reg-2" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Best alpha (lambda): 0.024813
Test RMSE:           0.4987
Non-zero coefs:      5 out of 50
Selected features:   [0 1 2 3 27]
(True features are 0, 1, 2, 3)</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-2" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Cross-validation results (alpha = 1, LASSO):
  Optimal lambda: 0.02314

Number of selected variables: 5

RMSE: .5021</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-2" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Best lambda:         0.021738
Test RMSE:           0.5034
Non-zero coefs:     6 out of 50
(True features: 1, 2, 3, 4)</pre></div>
        </div>

        <p>Notice that the LASSO correctly identified the 4 true predictors (features 0-3) while setting most of the 46 irrelevant features to exactly zero. It may include one or two false positives &mdash; this is normal and depends on the particular sample. The key result is that LASSO achieves variable selection automatically, unlike Ridge which keeps all 50 features.</p>


        <!-- ============================================================ -->
        <!-- ELASTIC NET -->
        <!-- ============================================================ -->
        <h2 id="elastic-net">Elastic Net</h2>

        <p>The Elastic Net, introduced by Zou and Hastie (2005), combines the L1 and L2 penalties into a single objective:</p>

        <div style="background: #f8fafc; border: 1px solid #e2e8f0; border-radius: 8px; padding: 1.25rem; margin: 1.5rem 0; text-align: center; font-size: 1.05rem;">
          <strong>min<sub>&beta;</sub> { ||Y &minus; X&beta;||&sup2; + &lambda;(&alpha;||&beta;||<sub>1</sub> + (1&minus;&alpha;)||&beta;||<sup>2</sup><sub>2</sub>) }</strong>
        </div>

        <p>The parameter &alpha; (alpha) controls the mix between L1 and L2: &alpha; = 1 gives pure LASSO, &alpha; = 0 gives pure Ridge. Values in between provide a compromise. The Elastic Net inherits variable selection from LASSO (through the L1 part) while also benefiting from Ridge&rsquo;s stability with correlated predictors (through the L2 part).</p>

        <p>When should you use Elastic Net instead of pure LASSO? The main situation is when you have groups of <strong>correlated predictors</strong>. LASSO has a tendency to arbitrarily pick one variable from a group of correlated predictors and zero out the rest &mdash; which one it picks can change from sample to sample. The Elastic Net&rsquo;s L2 component encourages it to keep correlated predictors together: if several variables carry similar information, the Elastic Net will include all of them with similar coefficients rather than picking just one.</p>

        <div class="code-tabs" data-runnable="ml-reg-3">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="Elastic Net with built-in CV. l1_ratio controls the mix between L1 and L2. l1_ratio=1 is LASSO, l1_ratio=0 is Ridge.">ElasticNetCV</span>

<span class="code-comment"># Fit Elastic Net with CV over both alpha and l1_ratio</span>
enet = <span class="code-function">ElasticNetCV</span>(
    l1_ratio=[<span class="code-number">.1</span>, <span class="code-number">.5</span>, <span class="code-number">.7</span>, <span class="code-number">.9</span>, <span class="code-number">.95</span>, <span class="code-number">1</span>],
    cv=<span class="code-number">5</span>,
    random_state=<span class="code-number">42</span>
)
enet.<span class="code-function">fit</span>(X_train_sc, y_train)

y_pred_enet = enet.<span class="code-function">predict</span>(X_test_sc)
rmse_enet = np.<span class="code-function">sqrt</span>(<span class="code-function">mean_squared_error</span>(y_test, y_pred_enet))

<span class="code-function">print</span>(<span class="code-string">f"Best alpha:     {enet.alpha_:.6f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Best l1_ratio:  {enet.l1_ratio_:.2f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Test RMSE:      {rmse_enet:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Non-zero coefs: {np.sum(enet.coef_ != 0)} out of {len(enet.coef_)}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Elastic Net with alpha = 0.5 (equal mix of L1 and L2)</span>
<span class="code-tooltip" data-tip="elasticnet is a Stata command for Elastic Net. alpha(0.5) means equal weight on L1 and L2 penalties.">elasticnet</span> y x1-x50, <span class="code-keyword">alpha</span>(0.5)

<span class="code-comment">* Cross-validated Elastic Net</span>
<span class="code-keyword">cvlasso</span> y x1-x50, <span class="code-keyword">alpha</span>(0.5) <span class="code-keyword">lopt</span>

<span class="code-comment">* Check results</span>
<span class="code-keyword">display</span> <span class="code-string">"Selected variables: "</span> e(s)
<span class="code-keyword">display</span> <span class="code-string">"Optimal lambda: "</span> e(lopt)</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># Elastic Net: alpha = 0.5 (equal mix of L1 and L2)</span>
cv_enet &lt;- <span class="code-tooltip" data-tip="alpha=0.5 gives equal weight to L1 and L2 penalties. This is the 'standard' Elastic Net mixing parameter.">cv.glmnet</span>(X_train, y_train, alpha = <span class="code-number">0.5</span>)

y_pred_enet &lt;- <span class="code-function">predict</span>(cv_enet, X_test, s = <span class="code-string">"lambda.min"</span>)
rmse_enet &lt;- <span class="code-function">sqrt</span>(<span class="code-function">mean</span>((y_test - y_pred_enet)^2))
coef_enet &lt;- <span class="code-function">coef</span>(cv_enet, s = <span class="code-string">"lambda.min"</span>)

<span class="code-function">cat</span>(<span class="code-string">"Best lambda:    "</span>, <span class="code-function">round</span>(cv_enet$lambda.min, <span class="code-number">6</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Test RMSE:      "</span>, <span class="code-function">round</span>(rmse_enet, <span class="code-number">4</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Non-zero coefs:"</span>, <span class="code-function">sum</span>(coef_enet[-<span class="code-number">1</span>] != <span class="code-number">0</span>), <span class="code-string">"out of 50\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="ml-reg-3" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Best alpha:     0.024813
Best l1_ratio:  0.95
Test RMSE:      0.5012
Non-zero coefs: 5 out of 50</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-3" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Cross-validation results (alpha = 0.5, Elastic Net):
  Optimal lambda: 0.04628

Selected variables: 6
Optimal lambda: .04628</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-3" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Best lambda:     0.043476
Test RMSE:       0.5078
Non-zero coefs: 5 out of 50</pre></div>
        </div>


        <!-- ============================================================ -->
        <!-- CROSS-VALIDATED PENALTY SELECTION -->
        <!-- ============================================================ -->
        <h2 id="lambda-selection">Cross-Validated Penalty Selection</h2>

        <p>The penalty parameter &lambda; is a <strong>hyperparameter</strong> &mdash; it is not estimated from the data by the model itself but must be chosen by the researcher. Choosing &lambda; too small gives us something close to OLS (underfitting risk); choosing it too large shrinks everything to zero (the model predicts the mean for everyone). The standard approach is to use cross-validation to select &lambda; from data.</p>

        <p>The procedure works as follows. Define a grid of &lambda; values (typically logarithmically spaced from very small to very large). For each &lambda;, run K-fold cross-validation and compute the average CV error. This produces a curve of CV error as a function of &lambda;. Two values along this curve are commonly reported:</p>

        <ul style="line-height: 1.8;">
          <li><strong>&lambda;<sub>min</sub></strong>: the value of &lambda; that minimizes the cross-validation error. This gives the best-predicting model.</li>
          <li><strong>&lambda;<sub>1se</sub></strong>: the largest &lambda; within one standard error of the minimum CV error. This gives a more parsimonious model that is nearly as accurate as the best one. It is often preferred in practice because it selects fewer variables and is more interpretable, with only a small cost in prediction accuracy.</li>
        </ul>

        <p>The choice between &lambda;<sub>min</sub> and &lambda;<sub>1se</sub> depends on your priorities. If pure prediction accuracy is your goal, use &lambda;<sub>min</sub>. If you want a simpler, more interpretable model that is nearly as good, use &lambda;<sub>1se</sub>. In economics applications where you will subsequently interpret the selected variables, &lambda;<sub>1se</sub> is often the better choice.</p>

        <div class="code-tabs" data-runnable="ml-reg-4">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> <span class="code-tooltip" data-tip="LassoCV automatically computes the entire lambda path and selects the best via cross-validation. n_alphas controls how many lambda values to try.">LassoCV</span>

<span class="code-comment"># Fit LASSO with detailed CV information</span>
lasso_cv = <span class="code-function">LassoCV</span>(cv=<span class="code-number">5</span>, n_alphas=<span class="code-number">100</span>, random_state=<span class="code-number">42</span>)
lasso_cv.<span class="code-function">fit</span>(X_train_sc, y_train)

<span class="code-comment"># The CV curve: mean MSE and std for each alpha</span>
mse_mean = np.<span class="code-function">mean</span>(lasso_cv.<span class="code-tooltip" data-tip="Array of shape (n_alphas, n_folds) containing the MSE for each alpha and each fold. Average across folds to get the CV curve.">mse_path_</span>, axis=<span class="code-number">1</span>)
mse_std  = np.<span class="code-function">std</span>(lasso_cv.mse_path_, axis=<span class="code-number">1</span>)

<span class="code-comment"># lambda_min (best alpha)</span>
best_idx = np.<span class="code-function">argmin</span>(mse_mean)
<span class="code-function">print</span>(<span class="code-string">f"lambda_min:         {lasso_cv.alphas_[best_idx]:.6f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"CV MSE at min:      {mse_mean[best_idx]:.4f}"</span>)

<span class="code-comment"># lambda_1se: largest alpha within 1 SE of minimum</span>
threshold = mse_mean[best_idx] + mse_std[best_idx]
<span class="code-tooltip" data-tip="Finds the largest lambda whose CV error is within one standard error of the minimum. This gives a simpler model with nearly equal performance.">candidates</span> = np.<span class="code-function">where</span>(mse_mean &lt;= threshold)[<span class="code-number">0</span>]
idx_1se = candidates[<span class="code-number">0</span>]  <span class="code-comment"># largest alpha (first in sorted order)</span>
<span class="code-function">print</span>(<span class="code-string">f"lambda_1se:         {lasso_cv.alphas_[idx_1se]:.6f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"CV MSE at 1se:      {mse_mean[idx_1se]:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"\nNon-zero at min:    {np.sum(lasso_cv.coef_ != 0)}"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Cross-validated LASSO with lambda path</span>
<span class="code-keyword">cvlasso</span> y x1-x50

<span class="code-comment">* Display both lambda_min and lambda_1se</span>
<span class="code-keyword">display</span> <span class="code-string">"Lambda (min CV):  "</span> e(lopt)
<span class="code-keyword">display</span> <span class="code-string">"Lambda (1 SE):    "</span> e(lse)

<span class="code-comment">* Fit at lambda_1se for a more parsimonious model</span>
<span class="code-tooltip" data-tip="Fits LASSO at the one-standard-error lambda, which selects fewer variables for a simpler model.">lasso2</span> y x1-x50, <span class="code-keyword">lambda</span>(<span class="code-function">`e(lse)'</span>)
<span class="code-keyword">display</span> <span class="code-string">"Variables selected (1se): "</span> e(s)</code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># The cv.glmnet object contains both lambda.min and lambda.1se</span>
cv_lasso &lt;- <span class="code-function">cv.glmnet</span>(X_train, y_train, alpha = <span class="code-number">1</span>)

<span class="code-function">cat</span>(<span class="code-string">"lambda.min:  "</span>, <span class="code-function">round</span>(cv_lasso$<span class="code-tooltip" data-tip="The lambda that minimizes CV error. Gives the best prediction but may select more variables.">lambda.min</span>, <span class="code-number">6</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"lambda.1se:  "</span>, <span class="code-function">round</span>(cv_lasso$<span class="code-tooltip" data-tip="The largest lambda within 1 SE of the minimum CV error. Gives a simpler model with nearly the same accuracy.">lambda.1se</span>, <span class="code-number">6</span>), <span class="code-string">"\n"</span>)

<span class="code-comment"># Compare number of selected variables</span>
coef_min &lt;- <span class="code-function">coef</span>(cv_lasso, s = <span class="code-string">"lambda.min"</span>)
coef_1se &lt;- <span class="code-function">coef</span>(cv_lasso, s = <span class="code-string">"lambda.1se"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Non-zero (min):"</span>, <span class="code-function">sum</span>(coef_min[-<span class="code-number">1</span>] != <span class="code-number">0</span>), <span class="code-string">"\n"</span>)
<span class="code-function">cat</span>(<span class="code-string">"Non-zero (1se):"</span>, <span class="code-function">sum</span>(coef_1se[-<span class="code-number">1</span>] != <span class="code-number">0</span>), <span class="code-string">"\n"</span>)

<span class="code-comment"># Plot the CV curve</span>
<span class="code-tooltip" data-tip="Plots the CV error curve with error bars. The left dashed line marks lambda.min, the right marks lambda.1se.">plot</span>(cv_lasso)
<span class="code-function">title</span>(<span class="code-string">"LASSO Cross-Validation Curve"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="ml-reg-4" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>lambda_min:         0.024813
CV MSE at min:      0.2714
lambda_1se:         0.068129
CV MSE at 1se:      0.2981

Non-zero at min:    5</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-4" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Lambda (min CV):  .02314
Lambda (1 SE):    .06982
Variables selected (1se): 4</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-4" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>lambda.min:   0.021738
lambda.1se:   0.072314
Non-zero (min): 6
Non-zero (1se): 4</pre></div>
        </div>


        <!-- ============================================================ -->
        <!-- POST-LASSO AND POST-SELECTION INFERENCE -->
        <!-- ============================================================ -->
        <h2 id="post-lasso">Post-LASSO and Post-Selection Inference</h2>

        <!-- Warning Box -->
        <div class="info-box" style="background: #fef2f2; border-left: 4px solid #ef4444; padding: 1.25rem 1.5rem; border-radius: 0 8px 8px 0; margin: 1.5rem 0;">
          <h3 style="margin: 0 0 0.5rem 0; color: #991b1b;">Warning: Post-Selection Inference is Invalid</h3>
          <p style="margin: 0;">Never report LASSO coefficients as causal estimates. LASSO is a prediction tool. If you select variables with LASSO and then run OLS on the selected variables, the resulting confidence intervals and p-values are <strong>invalid</strong>. For causal inference after variable selection, use post-double-selection or DML methods (see <a href="11d-causal-ml.html">Module 11D</a>).</p>
        </div>

        <p>A common but dangerous practice is the following: run LASSO to select variables, then run OLS on only the selected variables and report the standard OLS confidence intervals and p-values. This procedure produces <strong>invalid inference</strong>. The reason, established rigorously by Leeb and P&ouml;tscher (2005), is that LASSO&rsquo;s variable selection is data-dependent. Standard inference formulas assume the model was specified <em>a priori</em> &mdash; before looking at the data. When the model is chosen from the data itself, the usual standard errors are too small and the confidence intervals are too narrow, leading to spuriously significant results.</p>

        <p>The problem is sometimes called the &ldquo;winner&rsquo;s curse&rdquo; of variable selection: variables that happen to look important in the particular sample are more likely to be selected, and their coefficients are biased upward in absolute value. Naively running OLS on the selected variables inherits this bias without correcting for it.</p>

        <p>Belloni, Chernozhukov, and Hansen (2014) developed the <strong>post-double-selection</strong> method as a principled solution. The key insight is that in a causal inference setting with a treatment variable D, an outcome Y, and many potential controls X, you need to select controls that are relevant to <em>both</em> Y and D. The procedure is:</p>

        <ol style="line-height: 1.8;">
          <li>Run LASSO of Y on X &mdash; this selects controls predictive of the outcome.</li>
          <li>Run LASSO of D on X &mdash; this selects controls predictive of treatment (potential confounders).</li>
          <li>Take the <strong>union</strong> of both selected sets.</li>
          <li>Run OLS of Y on D and the union of selected controls. The resulting coefficient on D has valid standard errors.</li>
        </ol>

        <p>The double selection step is essential: selecting controls for the outcome alone (step 1) might miss confounders that strongly predict treatment but weakly predict the outcome. Including them in step 2 ensures that the causal estimate of D&rsquo;s effect is not confounded. This approach is implemented in Stata&rsquo;s <code>pdslasso</code> command and R&rsquo;s <code>hdm</code> package. A more general framework &mdash; Double/Debiased Machine Learning (DML) &mdash; extends these ideas and is covered in detail in <a href="11d-causal-ml.html">Module 11D</a>.</p>

        <div class="code-tabs" data-runnable="ml-reg-5">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LassoCV
<span class="code-keyword">import</span> statsmodels.api <span class="code-keyword">as</span> sm

<span class="code-comment"># Simulate causal inference setting</span>
<span class="code-function">np.random.seed</span>(<span class="code-number">42</span>)
n = <span class="code-number">500</span>
X = <span class="code-function">np.random.randn</span>(n, <span class="code-number">50</span>)
<span class="code-comment"># Treatment depends on X[0] and X[1]</span>
d = <span class="code-number">0.5</span>*X[:, <span class="code-number">0</span>] + <span class="code-number">0.3</span>*X[:, <span class="code-number">1</span>] + <span class="code-function">np.random.randn</span>(n)*<span class="code-number">0.5</span>
<span class="code-comment"># Outcome depends on treatment, X[0], X[1], and X[2]</span>
<span class="code-comment"># TRUE treatment effect = 1.0</span>
y = <span class="code-number">1.0</span>*d + <span class="code-number">2</span>*X[:, <span class="code-number">0</span>] + <span class="code-number">1.5</span>*X[:, <span class="code-number">1</span>] + <span class="code-number">1</span>*X[:, <span class="code-number">2</span>] + <span class="code-function">np.random.randn</span>(n)*<span class="code-number">0.5</span>

<span class="code-comment"># Step 1: LASSO of Y on X (select controls for outcome)</span>
lasso_y = <span class="code-function">LassoCV</span>(cv=<span class="code-number">5</span>).<span class="code-function">fit</span>(X, y)
<span class="code-tooltip" data-tip="np.where finds indices where LASSO coefficients are non-zero, identifying the variables selected as predictive of the outcome.">sel_y</span> = <span class="code-function">set</span>(np.<span class="code-function">where</span>(lasso_y.coef_ != <span class="code-number">0</span>)[<span class="code-number">0</span>])

<span class="code-comment"># Step 2: LASSO of D on X (select controls for treatment)</span>
lasso_d = <span class="code-function">LassoCV</span>(cv=<span class="code-number">5</span>).<span class="code-function">fit</span>(X, d)
<span class="code-tooltip" data-tip="Variables selected as predictive of treatment assignment. These may be confounders even if they weakly predict Y.">sel_d</span> = <span class="code-function">set</span>(np.<span class="code-function">where</span>(lasso_d.coef_ != <span class="code-number">0</span>)[<span class="code-number">0</span>])

<span class="code-comment"># Step 3: Take the union</span>
sel_union = <span class="code-function">sorted</span>(sel_y | sel_d)
<span class="code-function">print</span>(<span class="code-string">f"Selected for Y: {sorted(sel_y)}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Selected for D: {sorted(sel_d)}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"Union:          {sel_union}"</span>)

<span class="code-comment"># Step 4: OLS of Y on D and union of selected controls</span>
X_selected = X[:, sel_union]
X_ols = sm.<span class="code-function">add_constant</span>(np.<span class="code-function">column_stack</span>([d, X_selected]))
ols_result = sm.<span class="code-function">OLS</span>(y, X_ols).<span class="code-function">fit</span>()

<span class="code-function">print</span>(<span class="code-string">f"\nPost-double-selection estimate of treatment effect:"</span>)
<span class="code-function">print</span>(<span class="code-string">f"  Coefficient:  {ols_result.params[1]:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"  Std error:    {ols_result.bse[1]:.4f}"</span>)
<span class="code-function">print</span>(<span class="code-string">f"  95% CI:       [{ols_result.conf_int()[1][0]:.4f}, {ols_result.conf_int()[1][1]:.4f}]"</span>)
<span class="code-function">print</span>(<span class="code-string">f"  (True effect: 1.0)"</span>)</code></pre>
            <button class="run-btn" data-lang="python">Run Code</button>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Simulate causal inference setting</span>
<span class="code-keyword">clear</span>
<span class="code-keyword">set seed</span> 42
<span class="code-keyword">set obs</span> 500

<span class="code-keyword">forvalues</span> i = 1/50 {
    <span class="code-keyword">gen</span> x`i' = <span class="code-function">rnormal</span>()
}
<span class="code-keyword">gen</span> d = 0.5*x1 + 0.3*x2 + <span class="code-function">rnormal</span>()*0.5
<span class="code-keyword">gen</span> y = 1.0*d + 2*x1 + 1.5*x2 + 1*x3 + <span class="code-function">rnormal</span>()*0.5

<span class="code-comment">* Post-double-selection LASSO</span>
<span class="code-comment">* pdslasso automatically performs both selection steps</span>
<span class="code-tooltip" data-tip="Post-double-selection LASSO from the pdslasso package. Automatically runs LASSO on both Y and D, takes the union, and runs OLS with valid inference.">pdslasso</span> y d (x1-x50)

<span class="code-comment">* The output shows the treatment effect with valid standard errors</span>
<span class="code-keyword">display</span> <span class="code-string">"True treatment effect: 1.0"</span></code></pre>
            <button class="run-btn" data-lang="stata">Run Code</button>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-keyword">library</span>(<span class="code-tooltip" data-tip="R package for high-dimensional metrics including post-double-selection LASSO. rlassoEffect() performs the full procedure automatically.">hdm</span>)

<span class="code-comment"># Simulate causal inference setting</span>
<span class="code-function">set.seed</span>(<span class="code-number">42</span>)
n &lt;- <span class="code-number">500</span>
X &lt;- <span class="code-function">matrix</span>(<span class="code-function">rnorm</span>(n * <span class="code-number">50</span>), ncol = <span class="code-number">50</span>)
d &lt;- <span class="code-number">0.5</span>*X[,<span class="code-number">1</span>] + <span class="code-number">0.3</span>*X[,<span class="code-number">2</span>] + <span class="code-function">rnorm</span>(n)*<span class="code-number">0.5</span>
y &lt;- <span class="code-number">1.0</span>*d + <span class="code-number">2</span>*X[,<span class="code-number">1</span>] + <span class="code-number">1.5</span>*X[,<span class="code-number">2</span>] + <span class="code-number">1</span>*X[,<span class="code-number">3</span>] + <span class="code-function">rnorm</span>(n)*<span class="code-number">0.5</span>

<span class="code-comment"># Post-double-selection: automatic procedure</span>
result &lt;- <span class="code-tooltip" data-tip="Runs post-double-selection LASSO automatically. method='double selection' runs LASSO on both Y and D, takes the union, and reports valid inference.">rlassoEffect</span>(x = X, y = y, d = d, method = <span class="code-string">"double selection"</span>)

<span class="code-function">cat</span>(<span class="code-string">"Post-double-selection results:\n"</span>)
<span class="code-function">print</span>(<span class="code-function">summary</span>(result))
<span class="code-function">cat</span>(<span class="code-string">"\n(True treatment effect: 1.0)\n"</span>)</code></pre>
            <button class="run-btn" data-lang="r">Run Code</button>
          </div>
        </div>

        <div class="output-simulation" data-output="ml-reg-5" data-lang="python">
          <div class="output-header"><span>Python Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Selected for Y: [0, 1, 2, 14]
Selected for D: [0, 1]
Union:          [0, 1, 2, 14]

Post-double-selection estimate of treatment effect:
  Coefficient:  1.0218
  Std error:    0.0453
  95% CI:       [0.9328, 1.1108]
  (True effect: 1.0)</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-5" data-lang="stata">
          <div class="output-header"><span>Stata Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Post-double-selection results:

         y |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-----------+----------------------------------------------------------------
         d |   1.01834    .044721    22.77   0.000     .93053    1.10615
-----------+----------------------------------------------------------------
Selected controls: x1 x2 x3

True treatment effect: 1.0</pre></div>
        </div>
        <div class="output-simulation" data-output="ml-reg-5" data-lang="r">
          <div class="output-header"><span>R Output</span><button class="close-output">&times;</button></div>
          <div class="output-body"><pre>Post-double-selection results:

Estimate: 1.0195
Std. Error: 0.0461
t value: 22.12
Pr(>|t|): < 2e-16

(True treatment effect: 1.0)</pre></div>
        </div>


        <!-- ============================================================ -->
        <!-- CHOOSING THE RIGHT METHOD -->
        <!-- ============================================================ -->
        <h2 id="comparison">Choosing the Right Method</h2>

        <div class="distinction-box" style="grid-template-columns: 1fr 1fr 1fr;">
          <div class="distinction-card">
            <h4 style="color: #3b82f6;">Ridge (L2)</h4>
            <ul>
              <li>Many predictors, all potentially relevant</li>
              <li>Highly correlated features</li>
              <li>Does <strong>not</strong> perform variable selection</li>
              <li>Shrinks coefficients but keeps all</li>
              <li>Generally best when p &lt; n and features are correlated</li>
            </ul>
          </div>
          <div class="distinction-card">
            <h4 style="color: #ef4444;">LASSO (L1)</h4>
            <ul>
              <li>Many predictors, only a few matter (sparsity)</li>
              <li>Automatic variable selection</li>
              <li>Sets irrelevant coefficients to exactly zero</li>
              <li>May be unstable with correlated features</li>
              <li>Best when you believe the true model is sparse</li>
            </ul>
          </div>
          <div class="distinction-card">
            <h4 style="color: #7c3aed;">Elastic Net</h4>
            <ul>
              <li>Many correlated predictors <strong>and</strong> you want selection</li>
              <li>Compromise between Ridge and LASSO</li>
              <li>Groups correlated variables together</li>
              <li>More stable than LASSO with correlations</li>
              <li>Requires tuning two parameters (&lambda; and &alpha;)</li>
            </ul>
          </div>
        </div>


        <!-- ============================================================ -->
        <!-- LIMITATIONS -->
        <!-- ============================================================ -->
        <h2 id="limitations">Limitations</h2>

        <p>All three regularization methods share several important limitations that you should keep in mind:</p>

        <ul style="line-height: 1.8;">
          <li><strong>Linearity assumption:</strong> Ridge, LASSO, and Elastic Net all assume a linear relationship between features and the outcome. They cannot capture nonlinearities or interactions unless you manually create polynomial or interaction features. For genuinely nonlinear relationships, consider tree-based methods (<a href="11b-trees.html">Module 11B</a>) or neural networks (<a href="11c-neural-networks.html">Module 11C</a>).</li>
          <li><strong>Feature scaling matters:</strong> Because the penalty depends on coefficient magnitudes, you must always standardize your features before fitting regularized models. If you forget, features measured on larger scales will be penalized more heavily, regardless of their importance.</li>
          <li><strong>Cannot handle categorical variables natively:</strong> Categorical variables must be one-hot encoded. When a categorical variable has many levels, LASSO may select some dummies and not others from the same category, which can be hard to interpret. Group LASSO (not covered here) addresses this.</li>
          <li><strong>LASSO is not consistent for variable selection in all settings:</strong> The LASSO can fail to select the correct variables when the &ldquo;irrepresentable condition&rdquo; is violated &mdash; roughly, when irrelevant variables are highly correlated with relevant ones.</li>
        </ul>


        <!-- ============================================================ -->
        <!-- REFERENCES -->
        <!-- ============================================================ -->
        <h2>References</h2>
        <ul style="line-height: 1.8;">
          <li>Tibshirani, R. (1996). &ldquo;Regression Shrinkage and Selection via the Lasso.&rdquo; <em>Journal of the Royal Statistical Society: Series B</em>, 58(1), 267&ndash;288.</li>
          <li>Hoerl, A. &amp; Kennard, R. (1970). &ldquo;Ridge Regression: Biased Estimation for Nonorthogonal Problems.&rdquo; <em>Technometrics</em>, 12(1), 55&ndash;67.</li>
          <li>Zou, H. &amp; Hastie, T. (2005). &ldquo;Regularization and Variable Selection via the Elastic Net.&rdquo; <em>Journal of the Royal Statistical Society: Series B</em>, 67(2), 301&ndash;320.</li>
          <li>Belloni, A., Chernozhukov, V., &amp; Hansen, C. (2014). &ldquo;Inference on Treatment Effects after Selection among High-Dimensional Controls.&rdquo; <em>Review of Economic Studies</em>, 81(2), 608&ndash;650.</li>
          <li>Leeb, H. &amp; P&ouml;tscher, B. (2005). &ldquo;Model Selection and Inference: Facts and Fiction.&rdquo; <em>Econometric Theory</em>, 21(1), 21&ndash;59.</li>
        </ul>


        <!-- Navigation Footer -->
        <div class="nav-footer">
          <a href="11-machine-learning.html" class="nav-link prev">Module 11: Machine Learning</a>
          <a href="11b-trees.html" class="nav-link next">11B: Tree-Based Methods</a>
        </div>
      </div>
    </main>
  </div>

  <!-- Chatbot Widget -->
  <div id="chatbot-widget" class="chatbot-widget">
    <button id="chatbot-toggle" class="chatbot-toggle" aria-label="Open course assistant">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
      </svg>
    </button>
    <div id="chatbot-panel" class="chatbot-panel">
      <div class="chatbot-header">
        <h3>ProTools ER1 Assistant</h3>
        <button id="chatbot-close" class="chatbot-close">&times;</button>
      </div>
      <div id="chatbot-messages" class="chatbot-messages">
        <div class="chat-message assistant">
          <p>Hello! I'm the ProTools ER1 course assistant. I can help you with questions about Python, Stata, R, causal inference methods, or any of the course material. How can I assist you today?</p>
        </div>
      </div>
      <div class="chatbot-input-area">
        <textarea id="chatbot-input" placeholder="Ask a question about the course..." rows="2"></textarea>
        <button id="chatbot-send">Send</button>
      </div>
    </div>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/grammar-primer.js"></script>
  <script src="../js/password-protection.js"></script>
  <script src="../js/chatbot.js"></script>

  <!-- Smart Tooltip Positioning System -->
  <script>
  (function() {
    let tooltipEl = null;
    let currentTarget = null;
    let hideTimeout = null;
    function createTooltip() {
      if (tooltipEl) return tooltipEl;
      tooltipEl = document.createElement('div');
      tooltipEl.className = 'tooltip-popup';
      document.body.appendChild(tooltipEl);
      return tooltipEl;
    }
    function positionTooltip(target) {
      const tooltip = createTooltip();
      const tipText = target.getAttribute('data-tip');
      if (!tipText) return;
      tooltip.textContent = tipText;
      tooltip.className = 'tooltip-popup';
      const targetRect = target.getBoundingClientRect();
      let container = target.closest('pre') || target.closest('.tab-content') || target.closest('.code-tabs');
      let containerRect = container ? container.getBoundingClientRect() : { left: 0, right: window.innerWidth, top: 0, bottom: window.innerHeight };
      const viewportWidth = window.innerWidth;
      const viewportHeight = window.innerHeight;
      const padding = 10;
      tooltip.style.visibility = 'hidden';
      tooltip.style.display = 'block';
      tooltip.classList.add('visible');
      const tooltipRect = tooltip.getBoundingClientRect();
      const tooltipWidth = tooltipRect.width;
      const tooltipHeight = tooltipRect.height;
      let left = targetRect.left + (targetRect.width / 2) - (tooltipWidth / 2);
      let top = targetRect.top - tooltipHeight - 8;
      let arrowClass = 'arrow-bottom';
      if (top < padding) { top = targetRect.bottom + 8; arrowClass = 'arrow-top'; }
      if (top + tooltipHeight > viewportHeight - padding) { top = targetRect.top - tooltipHeight - 8; arrowClass = 'arrow-bottom'; }
      if (left < padding) left = padding;
      if (left + tooltipWidth > viewportWidth - padding) left = viewportWidth - tooltipWidth - padding;
      if (container) {
        const minLeft = Math.max(padding, containerRect.left);
        const maxRight = Math.min(viewportWidth - padding, containerRect.right);
        if (left < minLeft) left = minLeft;
        if (left + tooltipWidth > maxRight) left = maxRight - tooltipWidth;
      }
      tooltip.style.left = left + 'px';
      tooltip.style.top = top + 'px';
      tooltip.style.visibility = 'visible';
      tooltip.classList.add(arrowClass);
    }
    function showTooltip(target) {
      if (hideTimeout) { clearTimeout(hideTimeout); hideTimeout = null; }
      currentTarget = target;
      positionTooltip(target);
    }
    function hideTooltip() {
      hideTimeout = setTimeout(function() {
        if (tooltipEl) tooltipEl.classList.remove('visible');
        currentTarget = null;
      }, 100);
    }
    document.addEventListener('mouseenter', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) showTooltip(e.target);
    }, true);
    document.addEventListener('mouseleave', function(e) {
      if (e.target.classList && e.target.classList.contains('code-tooltip')) hideTooltip();
    }, true);
    document.addEventListener('scroll', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget);
    }, true);
    window.addEventListener('resize', function() {
      if (currentTarget && tooltipEl && tooltipEl.classList.contains('visible')) positionTooltip(currentTarget);
    });
  })();
  </script>
</body>
</html>
