<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>6A Matching | ProTools ER1</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400&family=Fira+Code:wght@400;500&family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/style.css">
  <style>
    .protected-content { -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; }
    .code-tooltip { position: relative; cursor: help; border-bottom: 1px dotted #888; text-decoration: none; }
    .tooltip-popup { position: fixed; background: #1f2937; color: white; padding: 0.5rem 0.75rem; border-radius: 6px; font-size: 0.75rem; white-space: normal; max-width: 300px; opacity: 0; pointer-events: none; transition: opacity 0.15s ease-in-out; z-index: 10000; }
    .tooltip-popup.visible { opacity: 1; }
  </style>
</head>
<body>
  <div id="password-overlay" class="password-overlay">
    <div class="password-modal">
      <h2>ProTools ER1</h2>
      <p>Programming Tools for Empirical Research</p>
      <p style="font-size: 0.9rem; color: #666; margin-bottom: 1.5rem;">Please enter the course password to access the materials.</p>
      <input type="password" id="password-input" placeholder="Enter password" autocomplete="off">
      <button id="password-submit">Access Course</button>
      <p id="password-error" style="color: #e53e3e; font-size: 0.85rem; margin-top: 1rem; display: none;">Incorrect password. Please try again.</p>
    </div>
  </div>

  <div class="page-wrapper protected-content">
    <aside class="sidebar">
      <a href="../index.html" class="sidebar-logo">ProTools ER1</a>
      <span class="sidebar-subtitle">Programming Tools for Empirical Research</span>
      <nav>
        <ul>
          <li><a href="../index.html"><span class="welcome-icon">üè†</span> Welcome</a></li>
          <li class="has-subnav">
            <a href="00-languages-platforms.html"><span class="module-number">0</span> Languages & Platforms</a>
            <ul class="sub-nav">
              <li><a href="00a-rstudio-guide.html">RStudio Guide</a></li>
              <li><a href="00b-stata-guide.html">Stata Guide</a></li>
              <li><a href="00c-vscode-guide.html">VS Code Guide</a></li>
              <li><a href="00d-notebooks-guide.html">Notebooks Guide</a></li>
            </ul>
          </li>
          <li><a href="01-getting-started.html"><span class="module-number">1</span> Getting Started</a></li>
          <li class="has-subnav">
            <a href="02-data-harnessing.html"><span class="module-number">2</span> Data Harnessing</a>
            <ul class="sub-nav">
              <li><a href="02a-file-import.html">File Import</a></li>
              <li><a href="02b-apis.html">APIs</a></li>
              <li><a href="02c-web-scraping.html">Web Scraping</a></li>
            </ul>
          </li>
          <li><a href="03-data-exploration.html"><span class="module-number">3</span> Data Exploration</a></li>
          <li><a href="04-data-cleaning.html"><span class="module-number">4</span> Data Cleaning</a></li>
          <li><a href="05-data-analysis.html"><span class="module-number">5</span> Data Analysis</a></li>
          <li><a href="05a-data-simulation.html"><span class="module-number">5A</span> Data Simulation</a></li>
          <li class="has-subnav">
            <a href="05b-experiments.html"><span class="module-number">5B</span> Experiments</a>
            <ul class="sub-nav">
              <li><a href="05b1-power-analysis.html">Power Analysis</a></li>
              <li><a href="05b2-survey-programming.html">Survey Programming</a></li>
              <li><a href="05b3-randomization.html">Randomization</a></li>
              <li><a href="05b4-balance-checks.html">Balance Checks</a></li>
            </ul>
          </li>
          <li class="has-subnav active">
            <a href="06-causal-inference.html"><span class="module-number">6</span> Causal Inference</a>
            <ul class="sub-nav">
              <li class="active"><a href="06a-matching.html">Matching</a></li>
              <li><a href="06b-did.html">Difference-in-Differences</a></li>
              <li><a href="06c-rdd.html">Regression Discontinuity</a></li>
              <li><a href="06d-iv.html">Instrumental Variables</a></li>
              <li><a href="06e-synthetic-control.html">Synthetic Control</a></li>
            </ul>
          </li>
          <li><a href="07-estimation.html"><span class="module-number">7</span> Estimation Methods</a></li>
          <li><a href="08-replicability.html"><span class="module-number">8</span> Replicability</a></li>
          <li><a href="09-github.html"><span class="module-number">9</span> Git & GitHub</a></li>
          <li><a href="10-nlp-history.html"><span class="module-number">10</span> History of NLP</a></li>
          <li><a href="11-machine-learning.html"><span class="module-number">11</span> Machine Learning</a></li>
          <li><a href="12-llms.html"><span class="module-number">12</span> Large Language Models</a></li>
          <li><a href="../resources.html">Resources</a></li>
          <li><a href="contact.html">Contact & Feedback</a></li>
        </ul>
      </nav>
    </aside>

    <main class="main-content">
      <div class="content">
        <nav class="breadcrumb" style="margin-bottom: 1rem; font-size: 0.9rem; color: #666;">
          <a href="06-causal-inference.html">6 Causal Inference</a> &raquo; Matching
        </nav>

        <div class="module-header">
          <h1>6A &nbsp;Matching</h1>
          <div class="module-meta">
            <span>~2 hours</span>
            <span>PSM, CEM, Nearest Neighbor</span>
          </div>
        </div>

        <div class="toc">
          <h3>Table of Contents</h3>
          <ul>
            <li><a href="#psm">Propensity Score Matching</a></li>
            <li><a href="#cem">Coarsened Exact Matching</a></li>
            <li><a href="#nn">Nearest Neighbor Matching</a></li>
            <li><a href="#diagnostics">Balance Diagnostics</a></li>
            <li><a href="#doubly-robust">Doubly Robust Estimation</a></li>
          </ul>
        </div>

        <h2 id="psm">Propensity Score Matching</h2>

        <p>
          The propensity score is the probability of treatment given observed covariates: <em>e(X) = P(D=1|X)</em>. Units with similar propensity scores are compared, reducing many covariates to a single dimension.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Propensity Score Matching</span>
<span class="code-keyword">import</span> pandas <span class="code-keyword">as</span> pd
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LogisticRegression
<span class="code-keyword">from</span> sklearn.neighbors <span class="code-keyword">import</span> NearestNeighbors

<span class="code-comment"># Step 1: Estimate propensity scores</span>
X = df[[<span class="code-string">'age'</span>, <span class="code-string">'income'</span>, <span class="code-string">'education'</span>]]
y = df[<span class="code-string">'treatment'</span>]

<span class="code-tooltip" data-tip="Logistic regression predicts probability of treatment given covariates">model = LogisticRegression(max_iter=1000)
model.fit(X, y)</span>
df[<span class="code-string">'pscore'</span>] = model.predict_proba(X)[:, 1]

<span class="code-comment"># Step 2: Match treated to nearest control</span>
treated = df[df[<span class="code-string">'treatment'</span>] == 1]
control = df[df[<span class="code-string">'treatment'</span>] == 0]

<span class="code-tooltip" data-tip="Find nearest neighbor in control group for each treated unit">nn = NearestNeighbors(n_neighbors=1, metric=<span class="code-string">'euclidean'</span>)
nn.fit(control[[<span class="code-string">'pscore'</span>]])</span>
distances, indices = nn.kneighbors(treated[[<span class="code-string">'pscore'</span>]])

<span class="code-comment"># Step 3: Create matched sample</span>
matched_control = control.iloc[indices.flatten()]
matched_df = pd.concat([treated.reset_index(drop=True),
                        matched_control.reset_index(drop=True)],
                       keys=[<span class="code-string">'treated'</span>, <span class="code-string">'control'</span>])

<span class="code-comment"># Step 4: Estimate ATT</span>
att = treated[<span class="code-string">'outcome'</span>].mean() - matched_control[<span class="code-string">'outcome'</span>].mean()
print(f<span class="code-string">"ATT: {att:.3f}"</span>)</code></pre>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Propensity Score Matching with teffects</span>

<span class="code-comment">* Method 1: teffects psmatch (built-in, recommended)</span>
<span class="code-tooltip" data-tip="teffects psmatch estimates ATT using propensity score matching. Syntax: outcome (treatment) (covariates)">teffects psmatch (outcome) (treatment age income education), ///
    atet nn(1)</span>

<span class="code-comment">* With caliper (maximum allowed distance)</span>
teffects psmatch (outcome) (treatment age income education), ///
    atet nn(1) caliper(0.1)

<span class="code-comment">* Method 2: psmatch2 (community-contributed, more options)</span>
<span class="code-comment">* ssc install psmatch2</span>
<span class="code-tooltip" data-tip="psmatch2 offers more flexibility: logit/probit models, various matching algorithms">psmatch2 treatment age income education, ///
    outcome(outcome) logit common</span>

<span class="code-comment">* Check balance after matching</span>
pstest age income education, treated(treatment)</code></pre>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Propensity Score Matching with MatchIt</span>
<span class="code-keyword">library</span>(MatchIt)
<span class="code-keyword">library</span>(cobalt)

<span class="code-comment"># Estimate propensity scores and match</span>
<span class="code-tooltip" data-tip="MatchIt performs matching in one step. method='nearest' does 1:1 nearest neighbor matching.">m.out <- matchit(treatment ~ age + income + education,
                 data = df,
                 method = "nearest",
                 distance = "glm",  <span class="code-comment"># logistic regression</span>
                 caliper = 0.1)</span>

<span class="code-comment"># Check match quality</span>
summary(m.out)

<span class="code-comment"># Extract matched data</span>
matched_df <- match.data(m.out)

<span class="code-comment"># Estimate ATT on matched sample</span>
<span class="code-tooltip" data-tip="On matched data, simple difference in means gives the ATT">att_model <- lm(outcome ~ treatment, data = matched_df,
                weights = weights)
summary(att_model)</span></code></pre>
          </div>
        </div>

        <h2 id="cem">Coarsened Exact Matching</h2>

        <p>
          CEM temporarily coarsens covariates into bins, performs exact matching within bins, then uses the original (non-coarsened) data for analysis. This avoids propensity score model dependence.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Coarsened Exact Matching</span>
<span class="code-keyword">import</span> pandas <span class="code-keyword">as</span> pd
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

<span class="code-comment"># Step 1: Coarsen variables into bins</span>
<span class="code-tooltip" data-tip="Create categorical bins for each continuous variable">df[<span class="code-string">'age_bin'</span>] = pd.cut(df[<span class="code-string">'age'</span>], bins=[18, 30, 45, 60, 100])
df[<span class="code-string">'income_bin'</span>] = pd.cut(df[<span class="code-string">'income'</span>], bins=5)
df[<span class="code-string">'edu_bin'</span>] = df[<span class="code-string">'education'</span>]  <span class="code-comment"># already categorical</span></span>

<span class="code-comment"># Step 2: Create strata</span>
df[<span class="code-string">'strata'</span>] = df.groupby([<span class="code-string">'age_bin'</span>, <span class="code-string">'income_bin'</span>, <span class="code-string">'edu_bin'</span>]).ngroup()

<span class="code-comment"># Step 3: Keep only matched strata (both treated and control)</span>
strata_counts = df.groupby([<span class="code-string">'strata'</span>, <span class="code-string">'treatment'</span>]).size().unstack(fill_value=0)
<span class="code-tooltip" data-tip="Keep strata that have at least one treated AND one control unit">matched_strata = strata_counts[(strata_counts[0] > 0) & (strata_counts[1] > 0)].index
df_matched = df[df[<span class="code-string">'strata'</span>].isin(matched_strata)]</span>

<span class="code-comment"># Step 4: Calculate CEM weights</span>
strata_weights = df_matched.groupby([<span class="code-string">'strata'</span>, <span class="code-string">'treatment'</span>]).size().unstack()
df_matched[<span class="code-string">'cem_weight'</span>] = df_matched.apply(
    <span class="code-keyword">lambda</span> row: 1 / strata_weights.loc[row[<span class="code-string">'strata'</span>], row[<span class="code-string">'treatment'</span>]], axis=1
)

print(f<span class="code-string">"Matched: {len(df_matched)} of {len(df)} observations"</span>)</code></pre>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Coarsened Exact Matching</span>
<span class="code-comment">* ssc install cem</span>

<span class="code-comment">* Basic CEM with automatic coarsening</span>
<span class="code-tooltip" data-tip="CEM automatically bins continuous variables. treatment(treatment) specifies treatment indicator.">cem age income education, treatment(treatment)</span>

<span class="code-comment">* Custom cutpoints for coarsening</span>
cem age (#5) income (#4) education, treatment(treatment)
<span class="code-comment">* #5 means 5 equal-sized bins</span>

<span class="code-comment">* Estimate treatment effect with CEM weights</span>
<span class="code-tooltip" data-tip="Use cem_weights from matching as regression weights">reg outcome treatment age income education [pw=cem_weights]</span>

<span class="code-comment">* Check match quality</span>
imb age income education, treatment(treatment)</code></pre>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Coarsened Exact Matching with MatchIt</span>
<span class="code-keyword">library</span>(MatchIt)

<span class="code-comment"># CEM matching</span>
<span class="code-tooltip" data-tip="method='cem' performs coarsened exact matching">m.cem <- matchit(treatment ~ age + income + education,
                 data = df,
                 method = "cem")</span>

<span class="code-comment"># With custom cutpoints</span>
m.cem <- matchit(treatment ~ age + income + education,
                 data = df,
                 method = "cem",
                 cutpoints = list(age = c(30, 45, 60),
                                  income = "q4"))  <span class="code-comment"># quartiles</span>

<span class="code-comment"># Extract matched data with weights</span>
matched_df <- match.data(m.cem)

<span class="code-comment"># Estimate ATT using weights</span>
att_model <- lm(outcome ~ treatment + age + income + education,
                data = matched_df,
                weights = weights)
summary(att_model)</code></pre>
          </div>
        </div>

        <h2 id="nn">Nearest Neighbor Matching</h2>

        <p>
          Match on Mahalanobis distance (accounts for covariance) or Euclidean distance directly on covariates.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Mahalanobis Distance Matching</span>
<span class="code-keyword">from</span> scipy.spatial.distance <span class="code-keyword">import</span> cdist
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

covariates = [<span class="code-string">'age'</span>, <span class="code-string">'income'</span>, <span class="code-string">'education'</span>]
X_treated = df.loc[df[<span class="code-string">'treatment'</span>] == 1, covariates].values
X_control = df.loc[df[<span class="code-string">'treatment'</span>] == 0, covariates].values

<span class="code-comment"># Calculate Mahalanobis distance matrix</span>
<span class="code-tooltip" data-tip="Mahalanobis distance accounts for correlation between variables">cov_matrix = np.cov(df[covariates].values.T)
VI = np.linalg.inv(cov_matrix)
distances = cdist(X_treated, X_control, metric=<span class="code-string">'mahalanobis'</span>, VI=VI)</span>

<span class="code-comment"># Find nearest neighbor for each treated</span>
matches = np.argmin(distances, axis=1)

<span class="code-comment"># Extract matched control outcomes</span>
control_indices = df[df[<span class="code-string">'treatment'</span>] == 0].index
matched_outcomes = df.loc[control_indices[matches], <span class="code-string">'outcome'</span>].values

<span class="code-comment"># ATT</span>
att = df.loc[df[<span class="code-string">'treatment'</span>] == 1, <span class="code-string">'outcome'</span>].mean() - matched_outcomes.mean()</code></pre>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Mahalanobis Distance Matching</span>

<span class="code-comment">* Using teffects nnmatch</span>
<span class="code-tooltip" data-tip="metric(maha) uses Mahalanobis distance instead of propensity scores">teffects nnmatch (outcome age income education) (treatment), ///
    atet metric(maha) nn(1)</span>

<span class="code-comment">* With bias adjustment (Abadie-Imbens)</span>
teffects nnmatch (outcome age income education) (treatment), ///
    atet metric(maha) nn(3) biasadj(age income education)

<span class="code-comment">* Multiple matches with replacement</span>
teffects nnmatch (outcome age income education) (treatment), ///
    atet metric(maha) nn(5)</code></pre>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Mahalanobis Distance Matching</span>
<span class="code-keyword">library</span>(MatchIt)

<span class="code-comment"># Mahalanobis distance matching</span>
<span class="code-tooltip" data-tip="distance='mahalanobis' uses Mahalanobis metric instead of propensity scores">m.maha <- matchit(treatment ~ age + income + education,
                  data = df,
                  method = "nearest",
                  distance = "mahalanobis")</span>

<span class="code-comment"># Multiple matches (k:1)</span>
m.maha <- matchit(treatment ~ age + income + education,
                  data = df,
                  method = "nearest",
                  distance = "mahalanobis",
                  ratio = 3)  <span class="code-comment"># 3 controls per treated</span>

<span class="code-comment"># With caliper on propensity score</span>
m.maha <- matchit(treatment ~ age + income + education,
                  data = df,
                  method = "nearest",
                  distance = "mahalanobis",
                  mahvars = ~ age + income + education,
                  caliper = 0.25,
                  std.caliper = TRUE)</code></pre>
          </div>
        </div>

        <h2 id="diagnostics">Balance Diagnostics</h2>

        <p>
          Always check that matching improved covariate balance. Report standardized mean differences (SMD) and variance ratios.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Balance Diagnostics</span>
<span class="code-keyword">import</span> matplotlib.pyplot <span class="code-keyword">as</span> plt

<span class="code-keyword">def</span> <span class="code-function">calc_smd</span>(df, var, treatment_col):
    <span class="code-string">"""Calculate standardized mean difference."""</span>
    treated = df[df[treatment_col] == 1][var]
    control = df[df[treatment_col] == 0][var]
    pooled_std = np.sqrt((treated.var() + control.var()) / 2)
    <span class="code-keyword">return</span> (treated.mean() - control.mean()) / pooled_std

<span class="code-comment"># Before and after matching</span>
covariates = [<span class="code-string">'age'</span>, <span class="code-string">'income'</span>, <span class="code-string">'education'</span>]
smd_before = [calc_smd(df, v, <span class="code-string">'treatment'</span>) <span class="code-keyword">for</span> v <span class="code-keyword">in</span> covariates]
smd_after = [calc_smd(matched_df, v, <span class="code-string">'treatment'</span>) <span class="code-keyword">for</span> v <span class="code-keyword">in</span> covariates]

<span class="code-comment"># Love plot</span>
<span class="code-tooltip" data-tip="Love plots visualize balance improvement. SMD < 0.1 is generally acceptable.">fig, ax = plt.subplots(figsize=(8, 5))
y_pos = np.arange(len(covariates))
ax.scatter(smd_before, y_pos, marker=<span class="code-string">'o'</span>, label=<span class="code-string">'Before'</span>, s=100)
ax.scatter(smd_after, y_pos, marker=<span class="code-string">'s'</span>, label=<span class="code-string">'After'</span>, s=100)
ax.axvline(x=0.1, color=<span class="code-string">'r'</span>, linestyle=<span class="code-string">'--'</span>, alpha=0.5)
ax.axvline(x=-0.1, color=<span class="code-string">'r'</span>, linestyle=<span class="code-string">'--'</span>, alpha=0.5)
ax.set_yticks(y_pos)
ax.set_yticklabels(covariates)
ax.set_xlabel(<span class="code-string">'Standardized Mean Difference'</span>)
ax.legend()
plt.tight_layout()
plt.show()</span></code></pre>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Balance Diagnostics</span>

<span class="code-comment">* After psmatch2</span>
pstest age income education, treated(treatment) both graph

<span class="code-comment">* Standardized differences table</span>
<span class="code-tooltip" data-tip="tebalance summarize shows SMD and variance ratios for each covariate">tebalance summarize</span>

<span class="code-comment">* Density plots of propensity scores</span>
tebalance density

<span class="code-comment">* Overidentification test for balance</span>
tebalance overid</code></pre>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Balance Diagnostics with cobalt</span>
<span class="code-keyword">library</span>(cobalt)

<span class="code-comment"># Balance table</span>
<span class="code-tooltip" data-tip="bal.tab shows SMD, variance ratios, and other balance statistics">bal.tab(m.out, un = TRUE, thresholds = c(m = 0.1))</span>

<span class="code-comment"># Love plot (visual balance check)</span>
<span class="code-tooltip" data-tip="Love plots show SMD before/after matching. Points inside threshold lines indicate good balance.">love.plot(m.out,
          binary = "std",
          thresholds = c(m = 0.1),
          var.order = "unadjusted")</span>

<span class="code-comment"># Density plots</span>
bal.plot(m.out, var.name = "age", which = "both")

<span class="code-comment"># Full balance summary</span>
summary(m.out)</code></pre>
          </div>
        </div>

        <h2 id="doubly-robust">Doubly Robust Estimation</h2>

        <p>
          Combine propensity score weighting with outcome regression. Consistent if either the propensity score model OR the outcome model is correctly specified.
        </p>

        <div class="code-tabs">
          <div class="tab-buttons">
            <button class="tab-button active" data-lang="python">Python</button>
            <button class="tab-button" data-lang="stata">Stata</button>
            <button class="tab-button" data-lang="r">R</button>
          </div>
          <div class="tab-content active" data-lang="python">
<pre><code><span class="code-comment"># Python: Doubly Robust Estimation (AIPW)</span>
<span class="code-keyword">from</span> sklearn.linear_model <span class="code-keyword">import</span> LogisticRegression, LinearRegression
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np

covariates = [<span class="code-string">'age'</span>, <span class="code-string">'income'</span>, <span class="code-string">'education'</span>]
X = df[covariates].values
D = df[<span class="code-string">'treatment'</span>].values
Y = df[<span class="code-string">'outcome'</span>].values

<span class="code-comment"># Step 1: Estimate propensity scores</span>
ps_model = LogisticRegression(max_iter=1000)
ps_model.fit(X, D)
pscore = ps_model.predict_proba(X)[:, 1]

<span class="code-comment"># Step 2: Estimate outcome models</span>
<span class="code-tooltip" data-tip="Fit separate outcome models for treated and control groups">mu1_model = LinearRegression()
mu0_model = LinearRegression()
mu1_model.fit(X[D == 1], Y[D == 1])
mu0_model.fit(X[D == 0], Y[D == 0])</span>

mu1 = mu1_model.predict(X)  <span class="code-comment"># E[Y(1)|X]</span>
mu0 = mu0_model.predict(X)  <span class="code-comment"># E[Y(0)|X]</span>

<span class="code-comment"># Step 3: AIPW estimator</span>
<span class="code-tooltip" data-tip="AIPW combines IPW and regression: consistent if either model is correct">aipw_1 = mu1 + D * (Y - mu1) / pscore
aipw_0 = mu0 + (1 - D) * (Y - mu0) / (1 - pscore)
ate_aipw = np.mean(aipw_1 - aipw_0)</span>

print(f<span class="code-string">"ATE (AIPW): {ate_aipw:.3f}"</span>)</code></pre>
          </div>
          <div class="tab-content" data-lang="stata">
<pre><code><span class="code-comment">* Stata: Doubly Robust with teffects</span>

<span class="code-comment">* AIPW (Augmented IPW)</span>
<span class="code-tooltip" data-tip="teffects aipw is doubly robust: combines outcome model with propensity weighting">teffects aipw (outcome age income education) ///
    (treatment age income education), atet</span>

<span class="code-comment">* IPWRA (IPW + Regression Adjustment)</span>
teffects ipwra (outcome age income education) ///
    (treatment age income education), atet

<span class="code-comment">* With different link functions</span>
teffects aipw (outcome age income education, poisson) ///
    (treatment age income education, probit), atet</code></pre>
          </div>
          <div class="tab-content" data-lang="r">
<pre><code><span class="code-comment"># R: Doubly Robust with AIPW package</span>
<span class="code-keyword">library</span>(AIPW)

<span class="code-comment"># Basic AIPW</span>
<span class="code-tooltip" data-tip="AIPW package implements augmented inverse probability weighting">aipw_obj <- AIPW$new(
  Y = df$outcome,
  A = df$treatment,
  W = df[, c("age", "income", "education")],
  Q.SL.library = "SL.glm",   <span class="code-comment"># outcome model</span>
  g.SL.library = "SL.glm"    <span class="code-comment"># propensity model</span>
)</span>

<span class="code-comment"># Fit and summarize</span>
aipw_obj$fit()
aipw_obj$summary()

<span class="code-comment"># With SuperLearner (ML ensemble)</span>
<span class="code-keyword">library</span>(SuperLearner)
aipw_ml <- AIPW$new(
  Y = df$outcome,
  A = df$treatment,
  W = df[, c("age", "income", "education")],
  Q.SL.library = c("SL.glm", "SL.ranger", "SL.xgboost"),
  g.SL.library = c("SL.glm", "SL.ranger")
)
aipw_ml$fit()
aipw_ml$summary()</code></pre>
          </div>
        </div>

        <div class="info-box warning">
          <div class="info-box-title">Key Assumption: Selection on Observables</div>
          <p style="margin-bottom: 0;">
            All matching methods assume no unmeasured confounders‚Äîthat once you condition on observed covariates, treatment assignment is as good as random. This is untestable. Use sensitivity analysis (e.g., Rosenbaum bounds) to assess robustness to hidden bias.
          </p>
        </div>

        <div class="citation">
          <div class="citation-title">References</div>
          <ul>
            <li><strong>Iacus, S., King, G., & Porro, G.</strong> (2012). "Causal Inference without Balance Checking: Coarsened Exact Matching." <em>Political Analysis</em>.</li>
            <li><strong>Ho, D., Imai, K., King, G., & Stuart, E.</strong> (2007). "Matching as Nonparametric Preprocessing." <em>Political Analysis</em>.</li>
            <li><strong>Abadie, A. & Imbens, G.</strong> (2006). "Large Sample Properties of Matching Estimators." <em>Econometrica</em>.</li>
          </ul>
        </div>

        <div class="nav-footer">
          <a href="06-causal-inference.html" class="nav-link prev">6: Causal Inference Overview</a>
          <a href="06b-did.html" class="nav-link next">6B: Difference-in-Differences</a>
        </div>
      </div>
    </main>
  </div>

  <button class="mobile-menu-toggle" aria-label="Toggle navigation menu">Menu</button>
  <script src="../js/main.js"></script>
  <script src="../js/password-protection.js"></script>
</body>
</html>
